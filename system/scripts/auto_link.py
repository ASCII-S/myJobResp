#!/usr/bin/env python3
"""
è‡ªåŠ¨é“¾æ¥ç”Ÿæˆå™¨
åŠŸèƒ½ï¼š
1. åŸºäºå†…å®¹ç›¸ä¼¼åº¦è‡ªåŠ¨å‘ç°ç›¸å…³æ–‡æ¡£
2. åŸºäºæ ‡ç­¾åŒ¹é…æ¨èç›¸å…³ç¬”è®°
3. è‡ªåŠ¨æ›´æ–°æ–‡æ¡£åº•éƒ¨çš„"ç›¸å…³ç¬”è®°"åŒºå—
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Tuple, Set
import argparse
from collections import Counter
import jieba
import yaml

# é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).parent.parent.parent  # è„šæœ¬åœ¨ system/scripts/ ä¸­
NOTES_DIR = ROOT_DIR / "notes"

# é…ç½®æ–‡ä»¶ï¼ˆä¼˜å…ˆç”¨æˆ·é…ç½®ï¼Œåå¤‡æ¨¡æ¿é…ç½®ï¼‰
USER_CONFIG = ROOT_DIR / "config" / "kb_config.yaml"
TEMPLATE_CONFIG = ROOT_DIR / "system" / "config" / "kb_config.yaml"
CONFIG_FILE = USER_CONFIG if USER_CONFIG.exists() else TEMPLATE_CONFIG


def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def parse_frontmatter(content: str) -> Tuple[Dict, str]:
    """è§£ææ–‡æ¡£çš„frontmatterå’Œæ­£æ–‡"""
    pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
    match = re.match(pattern, content, re.DOTALL)
    
    if match:
        frontmatter_str = match.group(1)
        body = match.group(2)
        try:
            frontmatter = yaml.safe_load(frontmatter_str)
            return frontmatter or {}, body
        except yaml.YAMLError:
            return {}, content
    return {}, content


def extract_keywords(text: str, top_n: int = 20) -> List[str]:
    """æå–æ–‡æœ¬å…³é”®è¯"""
    # ç§»é™¤Markdownè¯­æ³•
    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
    text = re.sub(r'`.*?`', '', text)
    text = re.sub(r'#+ ', '', text)
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)
    
    # åˆ†è¯
    words = jieba.cut(text)
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'æœ‰', 'ä¸º', 'ç­‰', 'å¦‚', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€ä¹ˆ'}
    filtered_words = [w for w in words if len(w) > 1 and w not in stopwords]
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(filtered_words)
    
    return [word for word, _ in word_freq.most_common(top_n)]


def calculate_similarity(doc1: Dict, doc2: Dict, config: Dict) -> float:
    """è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    keyword_weight = config['auto_linking']['keyword_weight']
    tag_weight = config['auto_linking']['tag_weight']
    
    # æ ‡ç­¾ç›¸ä¼¼åº¦
    tags1 = set(doc1.get('tags', []))
    tags2 = set(doc2.get('tags', []))
    
    if tags1 and tags2:
        tag_similarity = len(tags1 & tags2) / len(tags1 | tags2)
    else:
        tag_similarity = 0
    
    # å…³é”®è¯ç›¸ä¼¼åº¦
    keywords1 = set(doc1.get('keywords', []))
    keywords2 = set(doc2.get('keywords', []))
    
    if keywords1 and keywords2:
        keyword_similarity = len(keywords1 & keywords2) / len(keywords1 | keywords2)
    else:
        keyword_similarity = 0
    
    # åŠ æƒå¹³å‡
    similarity = keyword_weight * keyword_similarity + tag_weight * tag_similarity
    
    return similarity


def find_related_notes(target_file: Path, all_notes: List[Dict], config: Dict) -> List[Tuple[Dict, float]]:
    """æ‰¾åˆ°ä¸ç›®æ ‡æ–‡æ¡£ç›¸å…³çš„ç¬”è®°"""
    # è¯»å–ç›®æ ‡æ–‡æ¡£
    with open(target_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    frontmatter, body = parse_frontmatter(content)
    
    # æå–å…³é”®è¯
    keywords = extract_keywords(body)
    
    target_doc = {
        'filepath': target_file,
        'tags': frontmatter.get('tags', []),
        'keywords': keywords
    }
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = []
    for note in all_notes:
        # è·³è¿‡è‡ªå·±
        if note['filepath'] == target_file:
            continue
        
        similarity = calculate_similarity(target_doc, note, config)
        
        if similarity >= config['auto_linking']['similarity_threshold']:
            similarities.append((note, similarity))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # è¿”å›å‰Nä¸ª
    max_suggestions = config['auto_linking']['max_suggestions']
    return similarities[:max_suggestions]


def update_related_notes_section(filepath: Path, related_notes: List[Tuple[Dict, float]]) -> None:
    """æ›´æ–°æ–‡æ¡£çš„"ç›¸å…³ç¬”è®°"åŒºå—"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾"ç›¸å…³ç¬”è®°"åŒºå—
    pattern = r'(## ç›¸å…³ç¬”è®°\n).*?(?=\n## |$)'
    
    # ç”Ÿæˆæ–°çš„ç›¸å…³ç¬”è®°å†…å®¹
    related_section = "## ç›¸å…³ç¬”è®°\n"
    related_section += "<!-- è‡ªåŠ¨ç”Ÿæˆ -->\n\n"
    
    if related_notes:
        for note, similarity in related_notes:
            rel_path = note['relative_path']
            title = note['title']
            tags = ', '.join(note.get('tags', [])[:3])
            related_section += f"- [{title}]({rel_path}) - ç›¸ä¼¼åº¦: {similarity:.0%}"
            if tags:
                related_section += f" | æ ‡ç­¾: {tags}"
            related_section += "\n"
    else:
        related_section += "æš‚æ— ç›¸å…³ç¬”è®°\n"
    
    related_section += "\n"
    
    # æ›¿æ¢æˆ–æ·»åŠ ç›¸å…³ç¬”è®°åŒºå—
    if re.search(pattern, content, re.DOTALL):
        new_content = re.sub(pattern, related_section, content, flags=re.DOTALL)
    else:
        # å¦‚æœä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°æ–‡æ¡£æœ«å°¾
        if not content.endswith('\n'):
            content += '\n'
        new_content = content + '\n---\n\n' + related_section
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(new_content)


def scan_all_notes(notes_dir: Path) -> List[Dict]:
    """æ‰«ææ‰€æœ‰ç¬”è®°ï¼Œæå–å…ƒæ•°æ®å’Œå…³é”®è¯"""
    notes = []
    
    print("ğŸ” æ‰«æç¬”è®°æ–‡ä»¶...")
    for md_file in notes_dir.rglob("*.md"):
        if md_file.name.startswith('.'):
            continue
        
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        frontmatter, body = parse_frontmatter(content)
        keywords = extract_keywords(body)
        
        note_info = {
            'filepath': md_file,
            'relative_path': md_file.relative_to(ROOT_DIR),
            'title': md_file.stem,
            'tags': frontmatter.get('tags', []),
            'keywords': keywords
        }
        notes.append(note_info)
    
    print(f"ğŸ“š æ‰¾åˆ° {len(notes)} ç¯‡ç¬”è®°")
    return notes


def generate_cross_topic_index(all_notes: List[Dict], config: Dict) -> str:
    """ç”Ÿæˆè·¨ä¸»é¢˜ç´¢å¼•"""
    # ç»Ÿè®¡æ ‡ç­¾
    tag_notes = {}
    for note in all_notes:
        for tag in note.get('tags', []):
            if tag not in tag_notes:
                tag_notes[tag] = []
            tag_notes[tag].append(note)
    
    # æ‰¾å‡ºè·¨ä¸»é¢˜çš„æ ‡ç­¾ï¼ˆå‡ºç°åœ¨å¤šä¸ªä¸»é¢˜çš„ï¼‰
    cross_topic_tags = {tag: notes for tag, notes in tag_notes.items() if len(notes) >= 2}
    
    # ç”ŸæˆMarkdown
    md = f"""# ğŸ”— çŸ¥è¯†ç‚¹è·¨ä¸»é¢˜ç´¢å¼•

**ç”Ÿæˆæ—¶é—´**: {Path.cwd()}

æœ¬ç´¢å¼•åˆ—å‡ºåœ¨å¤šä¸ªä¸»é¢˜ä¸­éƒ½å‡ºç°çš„çŸ¥è¯†ç‚¹ï¼Œå¸®åŠ©å»ºç«‹è·¨é¢†åŸŸè¿æ¥ã€‚

---

"""
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„
    for tag in sorted(cross_topic_tags.keys()):
        notes = cross_topic_tags[tag]
        md += f"## {tag} ({len(notes)}ç¯‡)\n\n"
        
        # æŒ‰ä¸»é¢˜åˆ†ç»„
        by_topic = {}
        for note in notes:
            # ä»è·¯å¾„æå–ä¸»é¢˜ï¼ˆç¬¬ä¸€çº§ç›®å½•ï¼‰
            parts = note['relative_path'].parts
            if len(parts) >= 2 and parts[0] == 'notes':
                topic = parts[1]
            else:
                topic = 'å…¶ä»–'
            
            if topic not in by_topic:
                by_topic[topic] = []
            by_topic[topic].append(note)
        
        md += f"**æ¶‰åŠä¸»é¢˜**: {', '.join(by_topic.keys())}\n\n"
        
        for topic, topic_notes in sorted(by_topic.items()):
            md += f"### {topic}\n\n"
            for note in topic_notes:
                md += f"- [{note['title']}]({note['relative_path']})\n"
            md += "\n"
        
        md += "---\n\n"
    
    return md


def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨é“¾æ¥ç”Ÿæˆå™¨')
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤')
    
    # update å‘½ä»¤ï¼šæ›´æ–°å•ä¸ªæ–‡æ¡£çš„ç›¸å…³ç¬”è®°
    parser_update = subparsers.add_parser('update', help='æ›´æ–°å•ä¸ªæ–‡æ¡£çš„ç›¸å…³ç¬”è®°')
    parser_update.add_argument('filepath', type=str, help='æ–‡ä»¶è·¯å¾„')
    
    # update-all å‘½ä»¤ï¼šæ›´æ–°æ‰€æœ‰æ–‡æ¡£
    parser_update_all = subparsers.add_parser('update-all', help='æ›´æ–°æ‰€æœ‰æ–‡æ¡£çš„ç›¸å…³ç¬”è®°')
    
    # index å‘½ä»¤ï¼šç”Ÿæˆè·¨ä¸»é¢˜ç´¢å¼•
    parser_index = subparsers.add_parser('index', help='ç”Ÿæˆè·¨ä¸»é¢˜ç´¢å¼•')
    
    args = parser.parse_args()
    config = load_config()
    
    if args.command == 'update':
        filepath = ROOT_DIR / args.filepath
        if not filepath.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return
        
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {filepath.name}")
        all_notes = scan_all_notes(NOTES_DIR)
        
        print("ğŸ”— æŸ¥æ‰¾ç›¸å…³ç¬”è®°...")
        related_notes = find_related_notes(filepath, all_notes, config)
        
        if related_notes:
            print(f"âœ… æ‰¾åˆ° {len(related_notes)} ç¯‡ç›¸å…³ç¬”è®°")
            for note, sim in related_notes:
                print(f"   - {note['title']} (ç›¸ä¼¼åº¦: {sim:.0%})")
        else:
            print("â„¹ï¸  æœªæ‰¾åˆ°ç›¸å…³ç¬”è®°")
        
        print("ğŸ’¾ æ›´æ–°æ–‡æ¡£...")
        update_related_notes_section(filepath, related_notes)
        print("âœ… å®Œæˆ")
    
    elif args.command == 'update-all':
        all_notes = scan_all_notes(NOTES_DIR)
        
        print(f"ğŸ”„ å¼€å§‹æ›´æ–°æ‰€æœ‰æ–‡æ¡£...")
        for i, note in enumerate(all_notes, 1):
            print(f"[{i}/{len(all_notes)}] {note['title']}")
            related_notes = find_related_notes(note['filepath'], all_notes, config)
            update_related_notes_section(note['filepath'], related_notes)
        
        print("âœ… å…¨éƒ¨å®Œæˆ")
    
    elif args.command == 'index':
        all_notes = scan_all_notes(NOTES_DIR)
        
        print("ğŸ“Š ç”Ÿæˆè·¨ä¸»é¢˜ç´¢å¼•...")
        index_content = generate_cross_topic_index(all_notes, config)
        
        output_file = ROOT_DIR / "é¢è¯•å¤§çº²" / "_çŸ¥è¯†ç‚¹ç´¢å¼•.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(index_content)
        
        print(f"âœ… ç´¢å¼•å·²ç”Ÿæˆ: {output_file}")
    
    else:
        parser.print_help()


if __name__ == '__main__':
    main()

