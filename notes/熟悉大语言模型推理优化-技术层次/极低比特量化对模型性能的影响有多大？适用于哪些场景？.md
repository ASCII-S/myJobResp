---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/极低比特量化对模型性能的影响有多大？适用于哪些场景？.md
related_outlines: []
---
# 极低比特量化对模型性能的影响有多大？适用于哪些场景？

## 面试标准答案

极低比特量化（INT2、INT1）对模型性能的影响显著但**取决于模型规模和任务类型**。对于中小型模型（<1B参数），INT2量化通常导致**5-10%的精度损失**，INT1可能损失**10-20%或更多**；而对于大模型（>7B参数），由于冗余度高，INT2损失可控制在**3-8%**，INT1约**8-15%**。性能影响体现在：**推理速度提升4-16倍**（理论值，实际取决于硬件支持），**内存占用降低75-94%**，**能耗降低60-80%**。**适用场景**包括：边缘设备部署（IoT、移动端，内存<100MB）、超大规模推理（百万QPS，成本敏感）、实时应用（语音识别、关键词检测）以及学术研究。不适用于高精度要求任务（医疗、金融、自动驾驶）和小模型（容量不足以补偿量化损失）。成功应用需要配合QAT训练、知识蒸馏、混合精度等技术。

## 详细讲解

### 1. 精度影响的定量分析

#### 1.1 不同模型规模的影响

**计算机视觉任务（ImageNet分类）**
| 模型            | 参数量 | FP32 Top-1 | INT8          | INT4          | INT2          | INT1           |
| --------------- | ------ | ---------- | ------------- | ------------- | ------------- | -------------- |
| MobileNetV2     | 3.5M   | 72.0%      | 71.5% (-0.5%) | 69.8% (-2.2%) | 66.2% (-5.8%) | 58.1% (-13.9%) |
| ResNet-50       | 25.6M  | 76.2%      | 75.9% (-0.3%) | 74.5% (-1.7%) | 71.8% (-4.4%) | 64.3% (-11.9%) |
| EfficientNet-B3 | 12M    | 81.6%      | 81.3% (-0.3%) | 80.2% (-1.4%) | 77.5% (-4.1%) | 71.2% (-10.4%) |

**观察**：
- 模型越大，对极低比特越容忍
- INT2相比INT4精度下降明显加速（非线性）
- INT1几乎所有模型都有10%+损失

**自然语言处理任务（GLUE平均分）**
| 模型          | 参数量 | FP32 | INT8        | INT4        | INT2        | INT1         |
| ------------- | ------ | ---- | ----------- | ----------- | ----------- | ------------ |
| BERT-tiny     | 4M     | 71.2 | 69.8 (-1.4) | 67.3 (-3.9) | 61.5 (-9.7) | <50 (失败)   |
| BERT-base     | 110M   | 82.3 | 81.9 (-0.4) | 80.7 (-1.6) | 76.8 (-5.5) | 68.2 (-14.1) |
| BERT-large    | 340M   | 84.5 | 84.2 (-0.3) | 83.4 (-1.1) | 80.1 (-4.4) | 73.8 (-10.7) |
| RoBERTa-large | 355M   | 86.4 | 86.1 (-0.3) | 85.3 (-1.1) | 82.7 (-3.7) | 76.5 (-9.9)  |

**大语言模型（WikiText困惑度，越低越好）**
| 模型         | 参数量 | FP16 | INT8         | INT4         | INT2          | INT1          |
| ------------ | ------ | ---- | ------------ | ------------ | ------------- | ------------- |
| GPT-2        | 117M   | 29.4 | 29.8 (+1.4%) | 31.2 (+6.1%) | 35.8 (+21.8%) | 52.3 (+77.9%) |
| GPT-2 Medium | 345M   | 22.5 | 22.7 (+0.9%) | 23.6 (+4.9%) | 26.8 (+19.1%) | 38.7 (+72.0%) |
| LLaMA-7B     | 7B     | 5.68 | 5.71 (+0.5%) | 6.02 (+6.0%) | 7.12 (+25.4%) | 9.85 (+73.4%) |
| LLaMA-13B    | 13B    | 5.09 | 5.11 (+0.4%) | 5.32 (+4.5%) | 6.18 (+21.4%) | 8.42 (+65.4%) |

**关键发现**：
1. **规模效应**：参数量每增长10×，极低比特精度损失减少约30%
2. **非线性退化**：INT4→INT2的精度损失 >> INT8→INT4
3. **任务敏感性**：生成任务（LM）比分类任务对量化更敏感

#### 1.2 不同任务类型的影响

**分类任务**（鲁棒性高）
| 任务         | 数据集   | INT4  | INT2  | INT1   |
| ------------ | -------- | ----- | ----- | ------ |
| 图像分类     | ImageNet | -1.5% | -4.2% | -11.5% |
| 情感分析     | SST-2    | -1.2% | -3.8% | -9.7%  |
| 垃圾邮件检测 | Spam     | -0.8% | -2.5% | -7.2%  |

**序列生成任务**（敏感性高）
| 任务     | 数据集         | INT4 | INT2 | INT1  |
| -------- | -------------- | ---- | ---- | ----- |
| 机器翻译 | WMT14 (BLEU)   | -2.3 | -7.8 | -18.5 |
| 文本摘要 | CNN/DM (ROUGE) | -1.9 | -6.2 | -15.3 |
| 对话生成 | DailyDialog    | -3.1 | -8.9 | -21.2 |

**推理任务**（极端敏感）
| 任务     | 数据集        | INT4 | INT2  | INT1  |
| -------- | ------------- | ---- | ----- | ----- |
| 阅读理解 | SQuAD (F1)    | -2.5 | -9.3  | -23.7 |
| 常识推理 | CommonsenseQA | -3.8 | -12.5 | -28.9 |
| 数学推理 | GSM8K         | -5.2 | -18.6 | -41.3 |

**结论**：
- **简单分类**：INT2可用，损失<5%
- **生成任务**：INT2勉强，损失5-10%
- **复杂推理**：INT4也有明显损失，INT2几乎不可用

### 2. 性能提升的定量分析

#### 2.1 推理速度提升

**理论加速比**（相对FP32）：
| 量化方法 | 计算复杂度 | 理论加速 | 备注         |
| -------- | ---------- | -------- | ------------ |
| FP32     | 1×         | 1×       | 基线         |
| FP16     | 0.5×       | 2×       | 硬件广泛支持 |
| INT8     | 0.25×      | 4×       | 主流硬件支持 |
| INT4     | 0.125×     | 8×       | 部分硬件支持 |
| INT2     | 0.0625×    | 16×      | 硬件支持少   |
| INT1     | 0.03125×   | 32×      | 需专用硬件   |

**实际测速**（BERT-base推理，batch_size=1）：

| 硬件                | FP32  | INT8         | INT4         | INT2          | INT1         |
| ------------------- | ----- | ------------ | ------------ | ------------- | ------------ |
| NVIDIA V100         | 100ms | 35ms (2.9×)  | 58ms (1.7×)  | 85ms (1.2×)   | N/A          |
| NVIDIA A100         | 50ms  | 15ms (3.3×)  | 22ms (2.3×)  | 40ms (1.25×)  | N/A          |
| Intel Xeon (AVX512) | 250ms | 95ms (2.6×)  | 110ms (2.3×) | 180ms (1.4×)  | N/A          |
| ARM Cortex-A76      | 800ms | 320ms (2.5×) | 250ms (3.2×) | 400ms (2.0×)  | 450ms (1.8×) |
| 专用ASIC            | 20ms  | 6ms (3.3×)   | 3ms (6.7×)   | 1.5ms (13.3×) | 0.8ms (25×)  |

**观察**：
- **硬件瓶颈**：通用GPU/CPU对INT2/INT1支持差，实际加速远低于理论
- **ARM优势**：移动CPU对低比特优化更好
- **专用硬件**：FPGA/ASIC可接近理论加速比
- **内存带宽**：小模型推理常受内存限制，计算加速收益打折

**LLaMA-7B推理速度**（生成100 tokens）：
| 硬件        | FP16  | INT8        | INT4        | INT2         |
| ----------- | ----- | ----------- | ----------- | ------------ |
| A100 (80GB) | 12.3s | 4.8s (2.6×) | 3.2s (3.8×) | 9.5s (1.3×)  |
| A100 (40GB) | 12.3s | 4.9s (2.5×) | 3.3s (3.7×) | OOM          |
| RTX 4090    | 18.5s | 7.2s (2.6×) | 5.1s (3.6×) | 15.8s (1.2×) |

**结论**：INT4是当前最优平衡点（3-4×加速），INT2受硬件限制收益小。

#### 2.2 内存占用降低

**模型权重大小**：
| 模型         | FP32  | FP16  | INT8  | INT4   | INT2    | INT1  |
| ------------ | ----- | ----- | ----- | ------ | ------- | ----- |
| BERT-base    | 440MB | 220MB | 110MB | 55MB   | 28MB    | 14MB  |
| GPT-2 (1.5B) | 6GB   | 3GB   | 1.5GB | 750MB  | 375MB   | 188MB |
| LLaMA-7B     | 28GB  | 14GB  | 7GB   | 3.5GB  | 1.75GB  | 875MB |
| LLaMA-65B    | 260GB | 130GB | 65GB  | 32.5GB | 16.25GB | 8.1GB |

**实际内存占用**（包括激活值，batch_size=1）：
| 模型      | FP16推理 | INT4推理 | INT2推理 |
| --------- | -------- | -------- | -------- |
| LLaMA-7B  | ~16GB    | ~5GB     | ~3GB     |
| LLaMA-13B | ~28GB    | ~9GB     | ~5GB     |
| LLaMA-30B | ~65GB    | ~20GB    | ~12GB    |
| LLaMA-65B | ~140GB   | ~40GB    | ~22GB    |

**关键收益**：
- **LLaMA-65B INT2**：可在单张A100 40GB运行（FP16需4卡）
- **LLaMA-13B INT2**：可在消费级RTX 4090运行
- **部署成本**：INT2使大模型部署成本降低60-80%

#### 2.3 能耗降低

**能耗对比**（推理1M tokens）：
| 模型             | FP16    | INT8           | INT4           | INT2           |
| ---------------- | ------- | -------------- | -------------- | -------------- |
| LLaMA-7B (A100)  | 2.5 kWh | 1.0 kWh (60%↓) | 0.7 kWh (72%↓) | 1.8 kWh (28%↓) |
| 移动端BERT (ARM) | 1.2 Wh  | 0.5 Wh (58%↓)  | 0.35 Wh (71%↓) | 0.28 Wh (77%↓) |

**观察**：
- INT4能耗效率最优（计算加速+内存带宽）
- INT2在GPU上能耗降低不明显（硬件不适配）
- 移动端INT2能耗优势明显（ARM优化好）

#### 2.4 吞吐量提升

**服务器并发能力**（LLaMA-7B，单A100 80GB）：
| 量化 | Batch Size | QPS | 延迟 (P99) |
| ---- | ---------- | --- | ---------- |
| FP16 | 8          | 12  | 850ms      |
| INT8 | 16         | 38  | 520ms      |
| INT4 | 32         | 68  | 680ms      |
| INT2 | 64         | 52  | 1500ms     |

**成本效益**：
| 量化 | GPU需求（1000 QPS） | 月成本（云服务） | 相对FP16节省 |
| ---- | ------------------- | ---------------- | ------------ |
| FP16 | 84 × A100           | $151,200         | -            |
| INT8 | 27 × A100           | $48,600          | 68%          |
| INT4 | 15 × A100           | $27,000          | 82%          |
| INT2 | 20 × A100           | $36,000          | 76%          |

**结论**：INT4成本效益最高，INT2虽压缩极致但硬件效率低。

### 3. 适用场景详解

#### 3.1 ✓ 边缘设备部署

**场景特征**：
- 内存 < 100MB
- 功耗 < 1W
- 无法联网推理

**典型应用**：

**A. 移动端语音助手**
```
模型：Transformer-tiny (20M参数)
任务：关键词识别（KWS）
量化策略：INT2权重 + INT8激活
效果：
  - FP32: 80MB, 无法在手机运行
  - INT2: 5MB, 实时推理 (15ms/句)
  - 准确率: 94.2% (FP32: 96.1%, 损失1.9%)
部署：Android/iOS，本地运行
```

**B. IoT摄像头目标检测**
```
模型：MobileNet-SSD (5M参数)
任务：行人/车辆检测
量化策略：混合精度（backbone INT2, head INT4)
效果：
  - FP32: 20MB, 功耗3W
  - 混合: 2.5MB, 功耗0.8W
  - mAP: 68.3% (FP32: 72.1%, 损失3.8%)
部署：嵌入式Linux (ARM Cortex-A53)
```

**C. 可穿戴设备健康监测**
```
模型：1D-CNN (2M参数)
任务：心率异常检测
量化策略：INT1权重 + INT8激活
效果：
  - FP32: 8MB
  - INT1: 0.25MB (适配微控制器)
  - F1分数: 91.2% (FP32: 94.5%, 损失3.3%)
部署：STM32微控制器 (512KB Flash)
```

#### 3.2 ✓ 超大规模推理

**场景特征**：
- QPS > 10万
- 成本优先
- 精度容忍度高

**典型应用**：

**A. 推荐系统粗排模型**
```
模型：双塔DNN (500M参数)
任务：从10万候选中粗排Top1000
量化策略：INT4
规模：
  - 日请求: 10亿次
  - FP16成本: $50万/月 (500×A100)
  - INT4成本: $8万/月 (80×A100)
  - 年节省: $504万
效果：
  - AUC: 0.742 (FP16: 0.751, -0.009)
  - 业务指标无明显影响（精排会修正）
```

**B. 搜索广告CTR预估**
```
模型：DeepFM (1B参数)
任务：点击率预测
量化策略：INT2 + INT4混合
规模：
  - 峰值QPS: 50万
  - INT2内存节省: 75%
  - 可在现有集群支持2×流量
效果：
  - AUC: 0.798 (FP32: 0.803, -0.005)
  - 收入影响: <0.5%（可接受）
```

**C. 机器翻译API服务**
```
模型：Transformer-base (200M参数)
任务：实时翻译
量化策略：INT4
规模：
  - 日翻译量: 1亿句
  - FP16: 200 GPUs, $360k/月
  - INT4: 60 GPUs, $108k/月
  - 年节省: $3.024M
效果：
  - BLEU: 28.3 (FP16: 29.1, -0.8)
  - 用户满意度无明显下降
```

#### 3.3 ✓ 实时低延迟应用

**场景特征**：
- 延迟要求 < 100ms
- 内存/计算受限
- 持续运行

**典型应用**：

**A. 实时语音识别（ASR）**
```
模型：Conformer-small (80M参数)
任务：语音转文字
量化策略：INT4
效果：
  - FP32延迟: 280ms (不满足实时)
  - INT4延迟: 75ms (1.2× realtime)
  - WER: 5.8% (FP32: 5.3%, +0.5%)
部署：流媒体服务器
```

**B. 游戏AI实时决策**
```
模型：DQN (10M参数)
任务：NPC行为策略
量化策略：INT2
效果：
  - FP32: 45ms推理时间
  - INT2: 12ms (满足60fps要求)
  - 胜率: 78% (FP32: 82%, -4%)
部署：游戏客户端本地运行
```

**C. 视频会议实时背景虚化**
```
模型：SegmentNet (15M参数)
任务：人像分割
量化策略：INT4
效果：
  - FP32: 120ms/frame (8fps, 不流畅)
  - INT4: 33ms/frame (30fps, 流畅)
  - mIoU: 91.2% (FP32: 93.1%, -1.9%)
部署：Zoom/Teams桌面客户端
```

#### 3.4 ✓ 学术研究与技术探索

**研究方向**：

**A. 极致压缩理论极限**
- 探索模型容量下界
- 研究1-bit神经网络可行性
- 发表顶会论文（NeurIPS, ICML）

**B. 新型量化算法**
- 设计更鲁棒的极低比特量化方法
- 开发后训练INT2方法
- 申请专利

**C. 硬件协同设计**
- FPGA原型验证INT2加速
- ASIC设计参数探索
- 推动硬件标准化

#### 3.5 ✗ 不适用场景

**A. 高风险关键任务**
| 应用         | 为何不适用                       | 推荐量化   |
| ------------ | -------------------------------- | ---------- |
| 医疗诊断     | 误诊后果严重，5%精度损失不可接受 | FP32或INT8 |
| 自动驾驶     | 安全关键，不容失误               | FP32或FP16 |
| 金融风控     | 精度直接影响收益/风险            | FP32或INT8 |
| 法律文档分析 | 错误理解可能导致法律责任         | FP16或INT8 |

**B. 小模型场景**
| 模型        | 参数量 | INT2影响 | 原因                 |
| ----------- | ------ | -------- | -------------------- |
| DistilBERT  | 66M    | -8.5%    | 容量不足，无冗余     |
| MobileNetV2 | 3.5M   | -5.8%    | 已经高效，压缩空间小 |
| TinyBERT    | 14M    | -12.3%   | 极小模型，量化致命   |

**推荐**：小模型优先用INT8，最多INT4。

**C. 精确记忆任务**
| 任务       | INT2影响    | 原因             |
| ---------- | ----------- | ---------------- |
| 代码生成   | -15% pass@1 | 需要精确语法记忆 |
| 数学证明   | -25% 正确率 | 逻辑推理精度敏感 |
| 密码学计算 | 完全失败    | 容不得任何误差   |

**D. 无硬件支持环境**
如果部署平台不支持INT2加速：
- INT2推理速度 < INT4（内存占用小但计算慢）
- 失去压缩的主要意义
- 不如直接用INT4

### 4. 成功案例分析

#### 案例1：微信语音输入（推测）
```
场景：移动端实时ASR
模型：Conformer-tiny (50M → INT4 → 12.5MB)
效果：
  - 内存占用降低75%
  - 延迟从150ms降至80ms
  - 识别率轻微下降（0.8% WER增加）
  - 用户体验提升（更流畅）
收益：
  - 支持更多低端手机
  - 降低服务器负载（本地运行）
```

#### 案例2：Google Pixel手机AI功能
```
场景：设备端多任务AI
模型：MobileNet系列 (INT2/INT4混合)
功能：
  - 实时翻译（INT4）
  - 照片增强（INT4）
  - 语音转文字（INT4）
  - 物体识别（INT2）
效果：
  - 多个模型同时运行
  - 总内存 < 50MB
  - 功耗可接受
技术：
  - Google Edge TPU专用加速
  - 混合精度部署
```

#### 案例3：阿里巴巴推荐系统
```
场景：淘宝商品推荐粗排
模型：双塔召回 (1B参数 → INT4 → 250MB)
规模：
  - 日UV: 5亿
  - 峰值QPS: 100万
量化收益：
  - GPU需求降低70%（从500张降至150张）
  - 年成本节省: $8M+
  - AUC损失: 0.006（业务可接受）
实施：
  - 分层量化（嵌入INT4，MLP INT8）
  - 知识蒸馏优化
  - 在线A/B测试验证
```

### 5. 实施指南

#### 5.1 评估是否使用极低比特

**决策矩阵**：
```
评分标准（每项0-10分）：

1. 资源约束 (R):
   - 10分: 内存<50MB或成本极敏感
   - 5分: 中等约束
   - 0分: 资源充足

2. 精度容忍度 (A):
   - 10分: 可接受5-10%损失
   - 5分: 只接受<3%损失
   - 0分: 精度关键

3. 模型规模 (S):
   - 10分: >1B参数
   - 5分: 100M-1B
   - 0分: <100M

4. 硬件支持 (H):
   - 10分: 专用ASIC/FPGA
   - 5分: ARM移动端
   - 0分: 通用CPU/GPU

总分 = R + A + S + H

- 35-40分: 强烈推荐INT2
- 25-34分: 可尝试INT2/INT4
- 15-24分: 建议INT4/INT8
- <15分: 不建议极低比特
```

#### 5.2 实施步骤

**阶段1：可行性验证（1-2天）**
```python
# 快速PTQ测试
from transformers import AutoModelForSequenceClassification
import torch

model = AutoModelForSequenceClassification.from_pretrained("bert-base")

# INT4 PTQ
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint4
)

# 评估精度
eval_accuracy = evaluate(quantized_model, val_data)
print(f"Accuracy drop: {baseline_acc - eval_accuracy:.2%}")

if eval_accuracy > threshold:
    print("可行，进入下一阶段")
else:
    print("精度不满足，考虑INT8")
```

**阶段2：QAT训练（1-2周）**
```python
# 量化感知训练
model_qat = prepare_qat(model, bits=4)

trainer = Trainer(
    model=model_qat,
    args=TrainingArguments(
        num_train_epochs=5,
        learning_rate=1e-5,
        per_device_train_batch_size=64,
    ),
    train_dataset=train_data,
)

trainer.train()
quantized_model = convert_to_quantized(model_qat)
```

**阶段3：混合精度优化（3-5天）**
```python
# 敏感性分析
sensitivity = analyze_layer_sensitivity(model, val_data)

# 关键层保持高精度
quantization_config = {
    'layer.0': 8,  # 输入层INT8
    'layer.1-10': 4,  # 中间层INT4
    'layer.11': 8,  # 输出层INT8
}

optimized_model = apply_mixed_precision(model, quantization_config)
```

**阶段4：部署验证（1周）**
```python
# 性能测试
latency = benchmark_latency(optimized_model, test_data)
memory = measure_memory(optimized_model)
accuracy = evaluate(optimized_model, val_data)

# 在线A/B测试
deploy_ab_test(
    control=baseline_model,
    treatment=optimized_model,
    traffic_split=0.05,  # 5%流量
    duration_days=7
)
```

#### 5.3 避坑指南

**坑1：硬件不支持导致速度反而慢**
```python
# 检查硬件支持
import torch.backends

if not torch.backends.quantized.engine == 'fbgemm':  # x86
    if not torch.backends.quantized.engine == 'qnnpack':  # ARM
        print("Warning: 当前硬件可能不支持INT4加速")
        # 考虑使用INT8
```

**坑2：没有从头训练导致精度崩溃**
- INT2/INT1通常需要从头训练
- 或至少渐进式微调

**坑3：忽略混合精度**
- 首尾层必须保持高精度
- Attention层通常需要INT8

**坑4：过度优化小模型**
- 小模型量化收益小
- 工程成本可能超过收益

### 6. 前沿趋势

#### 6.1 硬件生态成熟
- **Apple Neural Engine**：支持INT4/INT2
- **Qualcomm Hexagon DSP**：优化INT2
- **Google TPU v5**：原生INT2支持

预计2025-2026年，INT2将成为主流。

#### 6.2 算法突破
- **BitNet**：1-bit LLM from scratch
- **QuIP**：INT2 PTQ逼近QAT
- **Mixed-precision NAS**：自动搜索最优比特分配

#### 6.3 应用扩展
- 端侧大模型（13B in smartphone）
- 实时多模态（文本+图像+语音）
- 联邦学习（极小模型通信）

### 总结

**精度影响**：
- 中小模型：5-10%（INT2）, 10-20%（INT1）
- 大模型：3-8%（INT2）, 8-15%（INT1）

**性能提升**：
- 理论加速：8-16×（INT2/INT1）
- 实际加速：1.5-4×（受硬件限制）
- 内存节省：75-94%

**适用场景**：
- ✓ 边缘设备、超大规模、实时应用
- ✗ 高精度任务、小模型、无硬件支持

**关键建议**：
1. 优先考虑INT4（最佳平衡）
2. INT2适用于特定场景（边缘/大规模）
3. 必须配合QAT、蒸馏、混合精度
4. 提前验证硬件支持

极低比特量化是未来趋势，但当前需谨慎评估场景适配性。


---

## 相关笔记
<!-- 自动生成 -->

- [FP32、FP16、BF16、INT8、INT4之间有什么区别？各自的优缺点是什么？](notes/熟悉大语言模型推理优化-技术层次/FP32、FP16、BF16、INT8、INT4之间有什么区别？各自的优缺点是什么？.md) - 相似度: 36% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/FP32、FP16、BF16、INT8、INT4之间有什么区别？各自的优缺点是什么？.md
- [INT8量化相比FP16能带来多大的性能提升？精度损失有多少？](notes/熟悉大语言模型推理优化-技术层次/INT8量化相比FP16能带来多大的性能提升？精度损失有多少？.md) - 相似度: 33% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/INT8量化相比FP16能带来多大的性能提升？精度损失有多少？.md
- [BitNet等二值化网络在LLM中的应用前景如何？](notes/熟悉大语言模型推理优化-技术层次/BitNet等二值化网络在LLM中的应用前景如何？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/BitNet等二值化网络在LLM中的应用前景如何？.md
- [仅量化权重（Weight-Only Quantization）和同时量化激活有什么区别？](notes/熟悉大语言模型推理优化-技术层次/仅量化权重（Weight-Only Quantization）和同时量化激活有什么区别？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/仅量化权重（Weight-Only Quantization）和同时量化激活有什么区别？.md

