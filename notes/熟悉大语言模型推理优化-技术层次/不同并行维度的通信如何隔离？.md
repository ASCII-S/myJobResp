# 不同并行维度的通信如何隔离？

## 面试标准答案

不同并行维度通过独立的通信组(process group)隔离，每个GPU同时属于多个组但通信互不干扰。PyTorch中使用`dist.new_group()`创建隔离的通信组，NCCL为每个组维护独立的通信上下文。TP组在节点内用NVLink，PP组跨stage点对点通信，DP组用All-Reduce。通信可并行执行或按优先级调度，使用不同communicator确保不会相互阻塞。关键是合理规划GPU rank到各维度的映射。

---

## 详细讲解

### 通信组创建

```python
import torch.distributed as dist

# 总GPU数: 64 = TP(8) × PP(4) × DP(2)
world_size = 64
rank = dist.get_rank()

# 计算各维度rank
tp_rank = rank % 8
pp_rank = (rank // 8) % 4
dp_rank = rank // 32

# 创建TP组
tp_ranks = [rank - tp_rank + i for i in range(8)]
tp_group = dist.new_group(tp_ranks)

# 创建PP组
pp_ranks = [rank + i * 8 for i in range(4)]
pp_group = dist.new_group(pp_ranks)

# 创建DP组
dp_ranks = [rank + i * 32 for i in range(2)]
dp_group = dist.new_group(dp_ranks)
```

### 使用隔离的通信组

```python
# TP通信
dist.all_reduce(tensor, group=tp_group)

# PP通信
dist.send(tensor, dst=next_pp_rank, group=pp_group)
dist.recv(tensor, src=prev_pp_rank, group=pp_group)

# DP通信
dist.all_reduce(gradients, group=dp_group)

# 各组通信互不影响
```

### 通信优先级

```python
# NCCL支持并发通信
# 可以同时执行不同组的通信

async def concurrent_communication():
    # TP和PP通信可overlap
    tp_handle = dist.all_reduce(
        tp_tensor, group=tp_group, async_op=True
    )
    pp_handle = dist.send(
        pp_tensor, dst=next_stage, async_op=True
    )
    
    # 等待两个完成
    tp_handle.wait()
    pp_handle.wait()
```

### 通信调度

```python
# 优先级: TP > PP > DP (推理)
# 因为TP延迟敏感，DP不存在(推理)

class CommunicationScheduler:
    def schedule_communications(self):
        # 高优先级: TP (频繁)
        if tp_comm_ready:
            execute_tp_communication()
        
        # 中优先级: PP (中频)
        elif pp_comm_ready:
            execute_pp_communication()
        
        # 低优先级: DP (低频，仅训练)
        elif dp_comm_ready:
            execute_dp_communication()
```

通信组隔离确保各并行维度独立且高效。

