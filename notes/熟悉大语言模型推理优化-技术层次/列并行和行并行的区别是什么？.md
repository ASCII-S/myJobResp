# 列并行和行并行的区别是什么？

## 面试标准答案

列并行将权重矩阵按列切分，输入在所有设备复制，输出在特征维度分片，不需要立即通信。行并行将权重矩阵按行切分，输入在特征维度分片，输出需要通过All-Reduce汇总。两者的主要区别在于：切分维度不同、通信时机不同、适用场景不同。在Transformer中，通常将QKV投影和FFN第一层用列并行,输出投影和FFN第二层用行并行，这样可以最小化通信次数。

---

## 详细讲解

### 1. 列并行（Column Parallelism）

#### 1.1 基本原理

将权重矩阵 $W \in \mathbb{R}^{d_{in} \times d_{out}}$ 沿着输出维度（列）切分：

$$W = [W_1 | W_2 | ... | W_N]$$

其中每个 $W_i \in \mathbb{R}^{d_{in} \times (d_{out}/N)}$。

#### 1.2 计算过程

对于 $Y = XW$：
```
输入: X ∈ ℝ^(B×S×d_in)  (在所有设备复制)
权重: W_i ∈ ℝ^(d_in×d_out/N)  (每设备一个分片)

设备 i 计算:
Y_i = X · W_i ∈ ℝ^(B×S×d_out/N)

最终输出: Y = [Y_1, Y_2, ..., Y_N] ∈ ℝ^(B×S×d_out)
```

#### 1.3 特点

**优势**：
- 不需要立即通信，输出可保持分片状态
- 如果后续层使用行并行，可以无缝衔接
- 计算完全独立，无需同步

**通信需求**：
- 前向：无（如果输入已复制）
- 如需合并输出：All-Gather

**适用层**：
- Attention 的 QKV 投影：$Q, K, V = X W_q, X W_k, X W_v$
- FFN 第一层：$H = \text{GeLU}(X W_1)$

### 2. 行并行（Row Parallelism）

#### 2.1 基本原理

将权重矩阵 $W \in \mathbb{R}^{d_{in} \times d_{out}}$ 沿着输入维度（行）切分：

$$W = \begin{bmatrix} W_1 \\ W_2 \\ ... \\ W_N \end{bmatrix}$$

其中每个 $W_i \in \mathbb{R}^{(d_{in}/N) \times d_{out}}$。

#### 2.2 计算过程

对于 $Y = XW$：
```
输入: X_i ∈ ℝ^(B×S×d_in/N)  (每设备一个分片)
权重: W_i ∈ ℝ^(d_in/N×d_out)  (每设备一个分片)

设备 i 计算:
Y_i = X_i · W_i ∈ ℝ^(B×S×d_out)

All-Reduce 求和:
Y = Σ Y_i ∈ ℝ^(B×S×d_out)
```

#### 2.3 特点

**优势**：
- 输出完整，不需要后续分片
- 自然接续列并行层
- 可以与 dropout、残差连接等操作融合

**通信需求**：
- 前向：All-Reduce（求和）
- 必须同步才能得到正确结果

**适用层**：
- Attention 输出投影：$O = \text{Concat}(heads) W_o$
- FFN 第二层：$Y = H W_2$

### 3. 详细对比

| 特性           | 列并行             | 行并行                   |
| -------------- | ------------------ | ------------------------ |
| **切分维度**   | 沿输出维度（列）   | 沿输入维度（行）         |
| **输入要求**   | 完整输入（复制）   | 分片输入                 |
| **输出状态**   | 分片输出           | 完整输出（需All-Reduce） |
| **通信时机**   | 延迟到需要时       | 计算后立即通信           |
| **通信类型**   | All-Gather（可选） | All-Reduce（必须）       |
| **计算独立性** | 完全独立           | 独立计算，需同步求和     |
| **适用层**     | QKV投影、FFN第一层 | 输出投影、FFN第二层      |

### 4. 在Transformer中的组合使用

#### 4.1 Attention层

```python
# 输入: X (完整)

# QKV投影 - 列并行
Q_i, K_i, V_i = X @ W_q_i, X @ W_k_i, X @ W_v_i  # 每设备独立

# Attention计算 - 在分片上独立进行
Attn_i = Softmax(Q_i K_i^T / √d) V_i  # 每个设备计算一部分头

# 输出投影 - 行并行
O_i = Attn_i @ W_o_i  # 局部计算
O = All-Reduce(O_i)    # 汇总结果
```

**通信次数**：每个Attention层只需1次All-Reduce

#### 4.2 FFN层

```python
# 输入: X (完整，来自上一层的All-Reduce)

# 第一层 - 列并行
H_i = GeLU(X @ W_1_i)  # 输出分片

# 第二层 - 行并行
Y_i = H_i @ W_2_i      # 局部计算
Y = All-Reduce(Y_i)     # 汇总结果
```

**通信次数**：每个FFN只需1次All-Reduce

### 5. 数学推导

#### 5.1 列并行正确性

$$
\begin{align}
Y &= X \cdot [W_1 | W_2 | ... | W_N] \\
  &= [X \cdot W_1 | X \cdot W_2 | ... | X \cdot W_N] \\
  &= [Y_1 | Y_2 | ... | Y_N]
\end{align}
$$

#### 5.2 行并行正确性

$$
\begin{align}
Y &= [X_1 | X_2 | ... | X_N] \cdot \begin{bmatrix} W_1 \\ W_2 \\ ... \\ W_N \end{bmatrix} \\
  &= X_1 W_1 + X_2 W_2 + ... + X_N W_N \\
  &= \sum_{i=1}^N Y_i
\end{align}
$$

### 6. 通信开销分析

假设：
- Batch size: B
- Sequence length: S  
- Hidden dimension: H
- 并行度: N
- 设备间带宽: BW

#### 6.1 列并行

**All-Gather 通信量**（如果需要合并）：
$$\text{Data} = B \times S \times H \times \text{sizeof}(\text{dtype})$$

**时间**：
$$T_{all-gather} = \frac{(N-1)}{N} \times \frac{B \times S \times H}{BW}$$

#### 6.2 行并行

**All-Reduce 通信量**：
$$\text{Data} = B \times S \times H \times \text{sizeof}(\text{dtype})$$

**时间**（Ring All-Reduce）：
$$T_{all-reduce} = 2 \times \frac{(N-1)}{N} \times \frac{B \times S \times H}{BW}$$

### 7. 实现示例

#### 7.1 列并行伪代码

```python
class ColumnParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, parallel_size):
        self.out_features_per_partition = out_features // parallel_size
        self.weight = nn.Parameter(
            torch.empty(in_features, self.out_features_per_partition)
        )
    
    def forward(self, input):
        # input: [B, S, in_features] - 在所有设备复制
        # output: [B, S, out_features/N] - 分片输出
        output = torch.matmul(input, self.weight)
        return output  # 无需通信
```

#### 7.2 行并行伪代码

```python
class RowParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, parallel_size):
        self.in_features_per_partition = in_features // parallel_size
        self.weight = nn.Parameter(
            torch.empty(self.in_features_per_partition, out_features)
        )
    
    def forward(self, input):
        # input: [B, S, in_features/N] - 分片输入
        output_partial = torch.matmul(input, self.weight)
        # All-Reduce 汇总
        output = all_reduce(output_partial, op=ReduceOp.SUM)
        return output  # [B, S, out_features] - 完整输出
```

### 8. 选择策略

**使用列并行的情况**：
- 层的输出需要保持分片（后续层是行并行）
- 需要在多个独立分支上并行（如多头注意力）
- 后续有逐元素操作（激活函数）

**使用行并行的情况**：
- 需要完整的输出（如最后的输出层）
- 输入已经是分片的（来自列并行层）
- 需要与残差连接、LayerNorm等操作结合

**最优组合**：
交替使用列并行和行并行，使得每两层只需一次通信，最大化计算通信比。

### 9. 性能考量

**计算效率**：
- 两种方式的计算量相同
- 都能达到理想的并行加速

**通信效率**：
- 列并行：延迟通信，灵活性更高
- 行并行：立即通信，但可与计算重叠

**显存占用**：
- 列并行：激活显存减少 1/N
- 行并行：激活显存完整，但可以立即释放输入

理解列并行和行并行的区别及其组合使用，是实现高效张量并行的关键。

