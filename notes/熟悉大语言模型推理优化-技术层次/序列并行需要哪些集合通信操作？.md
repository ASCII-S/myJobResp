---
created: '2025-10-19'
last_reviewed: '2025-10-19'
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/序列并行需要哪些集合通信操作？.md
related_outlines: []
---

# 序列并行需要哪些集合通信操作？

## 面试标准答案

序列并行主要使用All-to-All通信操作在特征维度分片和序列维度分片之间转换。具体包括：1)特征→序列的All-to-All，将[B,S,H/N]转为[B,S/N,H]；2)序列→特征的All-to-All，将[B,S/N,H]转回[B,S,H/N]。每个Transformer层需要4次All-to-All(Attention前后各2次)。通信量约为激活大小，比All-Reduce少但比张量并行的通信次数多。

---

## 详细讲解

### 1. 核心通信：All-to-All

```python
def all_to_all_sp(tensor, scatter_dim, gather_dim):
    """
    在分片维度间转换
    scatter_dim: 要分散的维度
    gather_dim: 要聚集的维度
    """
    return torch.distributed.all_to_all(
        tensor, scatter_dim, gather_dim
    )
```

### 2. 通信模式

```python
# 一个Transformer层的通信
# Attention前
x_seq → All-to-All → x_feat  # 序列→特征

# Attention中 (张量并行通信)
# 2× All-Reduce

# Attention后  
x_feat → All-to-All → x_seq  # 特征→序列

# FFN前
x_seq → All-to-All → x_feat

# FFN中 (张量并行通信)
# 2× All-Reduce

# FFN后
x_feat → All-to-All → x_seq

# 总计: 4× All-to-All + 4× All-Reduce
```

### 3. 通信量

```python
# All-to-All通信量 (Ring算法)
通信量 = (N-1)/N × Data_Size ≈ Data_Size

# vs All-Reduce
All-Reduce: 2 × Data_Size

# All-to-All更轻量
```

### 4. 性能影响

```
纯张量并行: 4× All-Reduce/层
张量+序列并行: 4× All-Reduce + 4× All-to-All/层

通信量增加，但显存节省巨大，整体收益为正
```

