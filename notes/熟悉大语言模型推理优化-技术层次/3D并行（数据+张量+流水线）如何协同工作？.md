# 3D并行（数据+张量+流水线）如何协同工作？

## 面试标准答案

3D并行组合DP、TP、PP三种策略：1)TP在节点内切分层内权重，利用NVLink高速互连；2)PP跨节点切分层间，利用粗粒度通信；3)DP在多个TPP组间复制模型处理不同数据。通信组织：每个GPU同时属于3个通信组(TP组、PP组、DP组)，通信彼此独立。例如512 GPUs可配置为TP=8×PP=8×DP=8，每组8卡TP、8组流水线、8份数据副本。关键是合理分配各维度度数，优化通信计算比。

---

## 详细讲解

### 3D并行配置

```python
# 512 GPUs总量
TP_size = 8   # 张量并行
PP_size = 8   # 流水线并行  
DP_size = 8   # 数据并行

assert TP_size * PP_size * DP_size == 512

# 每个GPU的坐标
gpu_rank = 0-511
tp_rank = gpu_rank % TP_size
pp_rank = (gpu_rank // TP_size) % PP_size
dp_rank = gpu_rank // (TP_size * PP_size)
```

### 通信组划分

```python
# TP组 (节点内)
tp_group = [gpu_rank - tp_rank + i for i in range(TP_size)]
# 例如 GPU 0: [0,1,2,3,4,5,6,7]

# PP组 (跨节点)
pp_group = [gpu_rank + i * TP_size for i in range(PP_size)]
# 例如 GPU 0: [0,8,16,24,32,40,48,56]

# DP组 (跨TPP组)
dp_group = [gpu_rank + i * (TP_size * PP_size) for i in range(DP_size)]
# 例如 GPU 0: [0,64,128,192,256,320,384,448]
```

### 协同工作流程

```python
# 前向传播
for micro_batch in micro_batches:
    # 1. DP: 不同micro_batch分配到不同DP组
    
    # 2. PP: micro_batch在stage间流水
    for stage in pipeline_stages:
        
        # 3. TP: 每个stage内张量并行
        local_output = compute_with_tensor_parallel(
            input, stage_layers
        )
        # TP通信: All-Reduce
        
        # PP通信: 发送到下一stage
        send_to_next_stage(local_output)
```

### 通信模式

```
TP通信 (高频，节点内):
- 每层2次All-Reduce
- NVLink: 600 GB/s
- 延迟: ~5ms/layer

PP通信 (中频，跨节点):
- 每个micro-batch 1次传输
- IB: 25 GB/s
- 延迟: ~50ms/batch

DP通信 (低频，训练时):
- 每个iteration 1次梯度同步
- IB: 25 GB/s
- 延迟: ~秒级

推理: 只有TP+PP通信，无DP
```

### 实际案例

```
GPT-3 (175B) 推理:
512 GPUs配置:

TP=8 (节点内)
PP=64 (8节点 × 8 stages/节点)
DP=1 (推理不需要)

性能:
- TP通信占比: 10%
- PP气泡: 15%
- 整体效率: ~75%
```

3D并行是大规模模型部署的标准方案。

