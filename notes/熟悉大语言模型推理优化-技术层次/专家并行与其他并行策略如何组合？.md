# 专家并行与其他并行策略如何组合？

## 面试标准答案

专家并行(EP)通常与数据并行(DP)和张量并行(TP)组合使用。常见配置：1)EP+DP-多个数据并行组，每组内做专家并行，适合专家数<GPU数；2)EP+TP-专家内部用张量并行，适合单专家很大的场景；3)EP+TP+DP-三维组合，大规模部署。关键是合理划分通信组，专家并行通信组内做All-to-All，不同组间独立。例如64 GPU可配置为EP=8,TP=4,DP=2。

---

## 详细讲解

### 组合模式

**EP + DP**:
```python
# 64 GPUs, 128专家
EP = 8   # 8 GPU一组做专家并行
DP = 8   # 8个这样的组(数据并行)

# 每组持有全部128专家
# 组间处理不同数据
```

**EP + TP**:
```python
# 专家内部张量并行
每个专家由4个GPU共同计算(TP=4)
专家间分布(EP=16)

# 适合: 单专家参数量大
```

**EP + TP + DP**:
```python
# 128 GPUs
TP = 4   # 专家内张量并行
EP = 8   # 32专家分布
DP = 4   # 数据并行

# 通信组划分
tp_group: [0,1,2,3], [4,5,6,7], ...
ep_group: [0,4,8,...], [1,5,9,...], ...
dp_group: [0-31], [32-63], [64-95], [96-127]
```

### 通信隔离

```python
# 不同并行维度的通信独立
EP: All-to-All (专家路由)
TP: All-Reduce (层内)
DP: 无通信(推理) / All-Reduce(训练梯度)
```

### 实际案例

```
Switch Transformer (1.6T参数):
- 2048专家
- EP=256 (跨节点)
- TP=8 (节点内)
- DP=多副本服务
```

组合并行策略实现超大MoE模型的高效部署。

