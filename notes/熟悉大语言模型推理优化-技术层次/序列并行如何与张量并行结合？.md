---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 熟悉大语言模型推理优化-技术层次
- 熟悉大语言模型推理优化-技术层次/序列并行如何与张量并行结合？.md
related_outlines: []
---
# 序列并行如何与张量并行结合？

## 面试标准答案

序列并行与张量并行结合，在特征维度做张量并行切分的同时，在序列维度做序列并行切分。具体方法是：在列并行层输出后，通过All-to-All操作将数据从特征维度分片转换为序列维度分片，然后在序列维度上独立执行LayerNorm和Dropout等操作，最后再通过All-to-All转换回特征维度分片。这样可以节省LayerNorm、Dropout的激活显存（减少1/N），特别适用于长序列场景。Megatron-LM默认启用此优化。

---

## 详细讲解

### 1. 序列并行的动机

#### 1.1 张量并行的显存问题

标准张量并行中：

```python
# 列并行层的输出
attn_out: [B, S, H/N]  # 特征维度分片，节省显存 ✓

# 但LayerNorm需要完整的特征维度
# 方案1: All-Gather（浪费显存）
attn_out_full = all_gather(attn_out)  # [B, S, H] ✗
ln_out = layer_norm(attn_out_full)

# 方案2: 序列并行（节省显存）
# 转换为序列维度分片
attn_out_seq: [B, S/N, H]  # ✓
ln_out = layer_norm(attn_out_seq)  # 在分片上独立计算
```

#### 1.2 哪些操作需要完整数据

**特征维度完整**的操作：
- 矩阵乘法（GEMM）的某些模式
- Self-Attention（在头维度分片即可）

**序列维度完整**的操作：
- Attention的QK^T（需要完整序列）

**都可以分片**的操作：
- LayerNorm（在特征维度归一化）
- Dropout（逐元素）
- 激活函数（逐元素）

### 2. 序列并行的基本原理

#### 2.1 数据重排

通过All-to-All通信在两种分片方式间转换：

**特征维度分片** ↔ **序列维度分片**

```
特征分片: [B, S, H/N] × N个GPU
转换为
序列分片: [B, S/N, H] × N个GPU

通过 All-to-All 通信实现
```

#### 2.2 All-to-All操作

```python
def all_to_all_transpose(tensor, scatter_dim, gather_dim, group):
    """
    tensor: [B, S, H/N] - 在 dim=2 分片
    scatter_dim: 要分散的维度（2，特征维度）
    gather_dim: 要聚集的维度（1，序列维度）
    
    返回: [B, S/N, H] - 在 dim=1 分片
    """
    world_size = dist.get_world_size(group)
    
    # 步骤1: 重塑tensor为 [B, S, N, H/N]
    shape = list(tensor.shape)
    shape.insert(scatter_dim, world_size)
    shape[scatter_dim+1] = shape[scatter_dim+1] // world_size
    tensor = tensor.view(shape)
    
    # 步骤2: 转置使scatter_dim靠前
    # [B, S, N, H/N] -> [B, N, S, H/N]
    tensor = tensor.transpose(scatter_dim, gather_dim)
    
    # 步骤3: All-to-All通信
    output_tensor_list = [torch.empty_like(tensor) 
                          for _ in range(world_size)]
    dist.all_to_all(output_tensor_list, 
                    list(tensor.chunk(world_size, dim=gather_dim)),
                    group=group)
    
    # 步骤4: 拼接结果
    output = torch.cat(output_tensor_list, dim=scatter_dim)
    
    # 步骤5: 重塑回正常形状
    # [B, N, S/N, H/N] -> [B, S/N, H]
    output = output.reshape([...])
    
    return output
```

### 3. 与张量并行的结合

#### 3.1 完整的Transformer Block流程

```python
def transformer_block_with_sequence_parallel(x, rank, world_size):
    """
    x: [B, S/N, H] - 初始输入（序列维度分片）
    """
    
    # === Input: 序列维度分片 [B, S/N, H] ===
    
    # LayerNorm 1 - 在序列分片上独立执行
    x_ln = layer_norm_seq_parallel(x)  # [B, S/N, H]
    
    # === 转换为特征维度分片（用于张量并行） ===
    # All-to-All: [B, S/N, H] -> [B, S, H/N]
    x_feat_shard = all_to_all_transpose(
        x_ln, 
        scatter_dim=2,  # 在特征维度分散
        gather_dim=1    # 在序列维度聚集
    )  # [B, S, H/N]
    
    # === Self-Attention（张量并行） ===
    # QKV投影 - 列并行
    q = x_feat_shard @ W_q_local  # [B, S, (H/N)] @ [H/N, H/N] -> [B, S, H/N]
    k = x_feat_shard @ W_k_local
    v = x_feat_shard @ W_v_local
    
    # Attention计算（每个GPU计算一部分头）
    attn = attention(q, k, v)  # [B, S, H/N]
    
    # 输出投影 - 行并行
    attn_out_local = attn @ W_o_local  # [B, S, H]
    attn_out = all_reduce(attn_out_local)  # All-Reduce聚合
    
    # === 转换回序列维度分片 ===
    # All-to-All: [B, S, H/N] -> [B, S/N, H]
    attn_out_seq = all_to_all_transpose(
        attn_out,
        scatter_dim=1,  # 在序列维度分散  
        gather_dim=2    # 在特征维度聚集
    )  # [B, S/N, H]
    
    # Dropout（序列分片上独立）
    attn_out_seq = dropout(attn_out_seq)
    
    # 残差（输入也是序列分片）
    x = x + attn_out_seq  # [B, S/N, H]
    
    # LayerNorm 2
    x_ln2 = layer_norm_seq_parallel(x)
    
    # === 再次转换为特征维度分片（FFN张量并行） ===
    x_feat_shard = all_to_all_transpose(x_ln2, 2, 1)
    
    # === FFN（张量并行） ===
    ffn_hidden = gelu(x_feat_shard @ W_1_local)  # [B, S, 4H/N]
    ffn_out_local = ffn_hidden @ W_2_local  # [B, S, H]
    ffn_out = all_reduce(ffn_out_local)
    
    # === 转换回序列维度分片 ===
    ffn_out_seq = all_to_all_transpose(ffn_out, 1, 2)
    
    # Dropout + 残差
    x = x + dropout(ffn_out_seq)
    
    # 输出: [B, S/N, H] - 序列维度分片
    return x
```

#### 3.2 通信次数统计

**标准张量并行**（无序列并行）：
- Attention输出: 1× All-Reduce
- FFN输出: 1× All-Reduce
- 总计: 2× All-Reduce

**张量并行+序列并行**：
- Attention前: 1× All-to-All（序列→特征）
- Attention输出: 1× All-Reduce
- Attention后: 1× All-to-All（特征→序列）
- FFN前: 1× All-to-All
- FFN输出: 1× All-Reduce
- FFN后: 1× All-to-All
- 总计: 2× All-Reduce + 4× All-to-All

### 4. 通信量分析

#### 4.1 All-to-All通信量

Ring All-to-All算法：

```python
# N个GPU，每个持有数据 D/N
# 总通信量: (N-1)/N × D ≈ D

# 与All-Reduce的通信量比较:
# All-Reduce: 2 × (N-1)/N × D ≈ 2D
# All-to-All: (N-1)/N × D ≈ D

# All-to-All通信量是All-Reduce的一半
```

#### 4.2 总通信量对比

**假设数据量** D = B × S × H：

**无序列并行**:
```
2 × All-Reduce = 2 × 2D = 4D
```

**有序列并行**:
```
2 × All-Reduce + 4 × All-to-All
= 2 × 2D + 4 × D
= 4D + 4D = 8D
```

**看似通信量翻倍？实际情况**：

- All-to-All通常比All-Reduce快（单次传输）
- All-to-All可以与某些计算重叠
- 节省的显存允许更大batch，抵消通信开销

### 5. 显存节省分析

#### 5.1 激活显存对比

**无序列并行**：
```python
# LayerNorm输入需要All-Gather
attn_out_gathered: [B, S, H]  # 完整特征维度

# Dropout激活
dropout_activation: [B, S, H]

# 总激活: ~2 × B × S × H
```

**有序列并行**：
```python
# LayerNorm输入（序列分片）
attn_out_seq: [B, S/N, H]  # 节省 (N-1)/N

# Dropout激活
dropout_activation: [B, S/N, H]  # 节省 (N-1)/N

# 总激活: ~2 × B × S/N × H
# 节省: (N-1)/N ≈ 87.5% (N=8)
```

#### 5.2 具体示例

**GPT-3 配置** (8路并行):
```
B = 32, S = 2048, H = 12288
数据类型: FP16 (2 bytes)

无序列并行 - 每层激活:
= 32 × 2048 × 12288 × 2 × 2
= 3.2 GB

有序列并行 - 每层激活:
= 32 × (2048/8) × 12288 × 2 × 2
= 0.4 GB

节省: 2.8 GB/层
96层总节省: ~270 GB
```

### 6. 优化的序列并行实现

#### 6.1 g和f操作符的扩展

Megatron-LM定义特殊操作符简化实现：

```python
class g_seq:
    """前向: All-to-All (特征→序列), 反向: All-to-All (序列→特征)"""
    @staticmethod
    def forward(ctx, input):
        return all_to_all_transpose(input, 1, 2)  # 特征→序列
    
    @staticmethod
    def backward(ctx, grad_output):
        return all_to_all_transpose(grad_output, 2, 1)  # 序列→特征

class f_seq:
    """前向: All-to-All (序列→特征), 反向: All-to-All (特征→序列)"""
    @staticmethod
    def forward(ctx, input):
        return all_to_all_transpose(input, 2, 1)  # 序列→特征
    
    @staticmethod
    def backward(ctx, grad_output):
        return all_to_all_transpose(grad_output, 1, 2)  # 特征→序列
```

#### 6.2 简化的Block实现

```python
def transformer_block_simplified(x):
    """
    x: [B, S/N, H] - 序列分片
    """
    # LayerNorm（序列分片上）
    x_ln = layer_norm(x)
    
    # 转换为特征分片
    x_feat = f_seq(x_ln)  # [B, S, H/N]
    
    # Attention（张量并行）
    attn = self_attention_tp(x_feat)
    attn_out = all_reduce(attn)
    
    # 转换回序列分片
    attn_seq = g_seq(attn_out)  # [B, S/N, H]
    
    # Dropout + 残差（序列分片）
    x = x + dropout(attn_seq)
    
    # 类似流程用于FFN
    x_ln2 = layer_norm(x)
    x_feat = f_seq(x_ln2)
    ffn_out = ffn_tp(x_feat)
    ffn_out = all_reduce(ffn_out)
    ffn_seq = g_seq(ffn_out)
    x = x + dropout(ffn_seq)
    
    return x
```

### 7. 通信优化

#### 7.1 All-to-All与计算重叠

```python
# 异步All-to-All
handle = async_all_to_all(tensor)

# 进行不依赖结果的计算
other_work()

# 等待完成
result = handle.wait()
```

#### 7.2 融合All-to-All

```python
# 如果有多个张量需要转换，可以融合
tensors = [tensor1, tensor2, tensor3]
fused_tensor = concat(tensors)
fused_result = all_to_all(fused_tensor)
results = split(fused_result)
```

### 8. 适用场景

#### 8.1 何时启用序列并行

**推荐**：
- ✅ 长序列（S > 2048）
- ✅ 显存受限
- ✅ 大batch size
- ✅ 多层模型

**不推荐**：
- ❌ 短序列（S < 512）
- ❌ 显存充足
- ❌ 通信带宽受限

#### 8.2 实际效果

**Megatron-LM论文数据**：
```
GPT-3 175B, 8×A100:
序列长度: 2048

无序列并行:
- 最大batch size: 32
- 激活显存: ~260 GB

有序列并行:
- 最大batch size: 48 (+50%)
- 激活显存: ~180 GB (-30%)
- 吞吐量提升: ~35%
```

### 9. 实现示例

```python
class SequenceParallelTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.tp_group = get_tensor_parallel_group()
        self.world_size = dist.get_world_size(self.tp_group)
        
    def forward(self, input_ids):
        # 嵌入层输出: [B, S, H]
        x = self.embedding(input_ids)
        
        # 转换为序列分片: [B, S/N, H]
        x = self.scatter_to_sequence_parallel(x)
        
        # Transformer layers
        for layer in self.layers:
            x = layer(x)  # 保持序列分片
        
        # 最后一层LayerNorm
        x = self.final_ln(x)  # 在序列分片上
        
        # 转换回完整（如果需要）
        x = self.gather_from_sequence_parallel(x)  # [B, S, H]
        
        # LM head
        logits = self.lm_head(x)
        
        return logits
    
    def scatter_to_sequence_parallel(self, x):
        # [B, S, H] -> [B, S/N, H]
        return all_to_all_transpose(x, scatter_dim=1, gather_dim=2,
                                     group=self.tp_group)
    
    def gather_from_sequence_parallel(self, x):
        # [B, S/N, H] -> [B, S, H]
        return all_to_all_transpose(x, scatter_dim=2, gather_dim=1,
                                     group=self.tp_group)
```

### 10. 总结

**序列并行的核心优势**：
1. 大幅减少激活显存（~87.5% for 8-way）
2. 允许更大batch size
3. 提高整体吞吐量

**与张量并行的协同**：
- 张量并行：在特征维度分片，用于GEMM
- 序列并行：在序列维度分片，用于LayerNorm等
- All-to-All：在两种分片间转换

**权衡**：
- 增加通信次数（4× All-to-All）
- 但节省显存收益更大
- 特别适合长序列场景

序列并行是现代大模型训练和推理的标准配置，与张量并行配合使用可以突破显存限制。


---

## 相关笔记
<!-- 自动生成 -->

- [序列并行如何与张量并行结合使用？](notes/熟悉大语言模型推理优化-技术层次/序列并行如何与张量并行结合使用？.md) - 相似度: 39% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/序列并行如何与张量并行结合使用？.md
- [序列并行需要哪些集合通信操作？](notes/熟悉大语言模型推理优化-技术层次/序列并行需要哪些集合通信操作？.md) - 相似度: 33% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/序列并行需要哪些集合通信操作？.md
- [张量并行中的通信模式是什么（All-Reduce、All-Gather）？](notes/熟悉大语言模型推理优化-技术层次/张量并行中的通信模式是什么（All-Reduce、All-Gather）？.md) - 相似度: 33% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/张量并行中的通信模式是什么（All-Reduce、All-Gather）？.md
- [序列并行如何沿序列维度切分？](notes/熟悉大语言模型推理优化-技术层次/序列并行如何沿序列维度切分？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/序列并行如何沿序列维度切分？.md
- [序列并行适用于哪些算子（LayerNorm、Dropout）？](notes/熟悉大语言模型推理优化-技术层次/序列并行适用于哪些算子（LayerNorm、Dropout）？.md) - 相似度: 31% | 标签: 熟悉大语言模型推理优化-技术层次, 熟悉大语言模型推理优化-技术层次/序列并行适用于哪些算子（LayerNorm、Dropout）？.md

