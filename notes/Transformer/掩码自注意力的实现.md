---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- Transformer
- Transformer/掩码自注意力的实现.md
related_outlines: []
---
# 掩码自注意力的实现

## 面试标准答案

掩码自注意力是在标准自注意力基础上添加掩码矩阵的技术。在解码器中，我们使用因果掩码（下三角掩码）确保位置i只能关注到位置i及之前的token，防止信息泄露。实现时，在softmax之前将掩码位置设为负无穷，softmax后这些位置的注意力权重变为0。

## 详细解析

### 1. 掩码自注意力的核心概念

掩码自注意力（Masked Self-Attention）是Transformer架构中的核心组件，特别是在解码器中起到关键作用。它在标准自注意力机制的基础上引入了掩码机制，用于控制模型在生成过程中只能关注到已生成的token，而不能"偷看"未来的token。

### 2. 数学公式与实现

#### 标准自注意力公式：
```
Attention(Q,K,V) = softmax(QK^T/√d_k)V
```

#### 掩码自注意力公式：
```
Attention(Q,K,V) = softmax((QK^T + M)/√d_k)V
```

其中M是掩码矩阵，掩码位置为-∞，非掩码位置为0。

### 3. 实现细节

#### 3.1 因果掩码的构造

```python
import torch
import torch.nn as nn
import numpy as np

def create_causal_mask(seq_len):
    """创建因果掩码矩阵"""
    # 创建下三角矩阵，上三角部分为True（需要掩码）
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    return mask

# 示例：4个token的因果掩码
seq_len = 4
causal_mask = create_causal_mask(seq_len)
print("因果掩码矩阵:")
print(causal_mask.int())
```

输出：
```
因果掩码矩阵:
tensor([[0, 1, 1, 1],
        [0, 0, 1, 1],
        [0, 0, 0, 1],
        [0, 0, 0, 0]])
```

#### 3.2 掩码应用的完整实现

```python
class MaskedSelfAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # 计算Q, K, V
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # 计算注意力分数
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        # 应用掩码
        if mask is not None:
            # 将掩码位置设为负无穷
            scores = scores.masked_fill(mask == 1, -1e9)
        
        # Softmax归一化
        attention_weights = torch.softmax(scores, dim=-1)
        
        # 应用注意力权重
        context = torch.matmul(attention_weights, V)
        
        # 重塑输出
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        # 输出投影
        output = self.W_o(context)
        
        return output, attention_weights
```

### 4. 掩码的作用机制

#### 4.1 防止信息泄露

在解码过程中，模型生成第i个token时，只应该基于前i-1个token的信息，而不能看到第i+1个及之后的token。掩码确保了这种因果关系：

```python
# 示例：注意力分数矩阵
scores_before_mask = torch.tensor([
    [0.5, 0.3, 0.2, 0.1],
    [0.4, 0.4, 0.1, 0.1],
    [0.2, 0.3, 0.3, 0.2],
    [0.1, 0.2, 0.3, 0.4]
])

# 应用因果掩码
mask = create_causal_mask(4)
scores_after_mask = scores_before_mask.masked_fill(mask, -1e9)
attention_weights = torch.softmax(scores_after_mask, dim=-1)

print("掩码后的注意力权重:")
print(attention_weights)
```

输出：
```
掩码后的注意力权重:
tensor([[1.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000],
        [0.2689, 0.3679, 0.3679, 0.0000],
        [0.1003, 0.1652, 0.2237, 0.5108]])
```

#### 4.2 维护自回归特性

掩码确保模型在训练和推理时保持一致的自回归特性：
- **训练时**：虽然可以并行计算所有位置，但掩码确保每个位置只看到之前的信息
- **推理时**：逐个生成token，自然满足因果关系

### 5. 不同类型的掩码

#### 5.1 Padding掩码

```python
def create_padding_mask(seq, pad_token_id=0):
    """创建padding掩码"""
    return (seq == pad_token_id).unsqueeze(1).unsqueeze(2)
```

#### 5.2 组合掩码

```python
def combine_masks(causal_mask, padding_mask):
    """组合因果掩码和padding掩码"""
    if padding_mask is not None:
        # 逻辑或：任一掩码为True则最终掩码为True
        combined_mask = causal_mask | padding_mask
    else:
        combined_mask = causal_mask
    return combined_mask
```

### 6. 性能优化技巧

#### 6.1 预计算掩码

```python
class OptimizedMaskedAttention(nn.Module):
    def __init__(self, d_model, n_heads, max_seq_len=512):
        super().__init__()
        # 其他初始化...
        
        # 预计算并注册因果掩码
        causal_mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1)
        self.register_buffer('causal_mask', causal_mask)
    
    def forward(self, x):
        seq_len = x.size(1)
        # 使用预计算的掩码
        mask = self.causal_mask[:seq_len, :seq_len]
        # 其余计算...
```

#### 6.2 Flash Attention中的掩码实现

现代实现中，掩码通常在更底层的内核中处理：

```python
# 使用Flash Attention的掩码实现示例
def flash_attention_with_mask(q, k, v, causal=True):
    """
    Flash Attention风格的掩码实现
    在计算过程中动态应用掩码，而不是预先计算整个注意力矩阵
    """
    # 这是简化的概念代码
    # 实际Flash Attention在CUDA内核中实现掩码
    pass
```

### 7. 常见错误和注意事项

#### 7.1 掩码值设置

- **错误**：将掩码位置设为0
- **正确**：将掩码位置设为-∞（实际实现中用-1e9）

#### 7.2 掩码形状匹配

```python
# 确保掩码形状与注意力分数矩阵匹配
batch_size, n_heads, seq_len, seq_len = scores.shape
mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, n_heads, -1, -1)
```

#### 7.3 梯度计算

掩码操作不会阻断梯度流，因为被掩码的位置在softmax后权重为0，对应的梯度自然为0。

### 8. 应用场景总结

1. **GPT系列模型**：解码器中的掩码自注意力
2. **BERT的MLM任务**：掩码特定token的注意力
3. **机器翻译**：目标序列的解码过程
4. **文本生成**：确保生成的因果一致性

掩码自注意力是实现高效并行训练的关键技术，它允许模型在训练时并行计算所有位置的注意力，同时保持推理时的自回归特性。

---

## 相关笔记
<!-- 自动生成 -->

- [因果掩码（Causal_Mask）的作用](notes/Transformer/因果掩码（Causal_Mask）的作用.md) - 相似度: 39% | 标签: Transformer, Transformer/因果掩码（Causal_Mask）的作用.md
- [解码过程的自回归性质](notes/Transformer/解码过程的自回归性质.md) - 相似度: 36% | 标签: Transformer, Transformer/解码过程的自回归性质.md

