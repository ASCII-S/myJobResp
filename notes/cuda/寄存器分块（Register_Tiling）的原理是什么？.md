---
created: '2025-10-19'
last_reviewed: '2025-10-19'
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- cuda
- cuda/寄存器分块（Register_Tiling）的原理是什么？.md
related_outlines: []
---

# 寄存器分块（Register Tiling）的原理是什么？

## 面试标准答案

寄存器分块（Register Tiling）是在共享内存分块的基础上，让**每个线程负责计算输出矩阵的一个小块（如8×8）而非单个元素**，将这些中间结果存储在寄存器中复用。其原理是：每个线程从共享内存加载一块A的行和B的列到寄存器，然后计算这个小块的所有输出元素，从而将数据复用率从线程块级别提升到线程级别。这样可以减少共享内存访问次数，提高算术强度，典型的实现是每个线程计算8×8=64个输出元素，可以带来2-4倍的性能提升。

---

## 详细讲解

### 1. 寄存器分块的动机

#### 1.1 共享内存分块的局限

**回顾共享内存Tiling**：
```cuda
// 每个线程计算C的1个元素
for (int k = 0; k < TILE_SIZE; k++) {
    sum += As[ty][k] * Bs[k][tx];
    //     ^^^^^^^^^^   ^^^^^^^^^^
    //     读共享内存    读共享内存
}
// 每个元素需要 2×TILE_SIZE 次共享内存访问
```

**问题**：
- 每次迭代访问共享内存2次
- 对于TILE_SIZE=32：每个元素需要64次共享内存访问
- 共享内存延迟虽然低（~20 cycles），但仍有优化空间

#### 1.2 数据复用的层次

```
全局内存 
   ↓ (复用: TILE_SIZE倍)
共享内存 
   ↓ (复用: THREAD_TILE倍)
寄存器
   ↓
计算单元
```

**寄存器的优势**：
- 延迟：1 cycle（共享内存的1/20）
- 带宽：理论上无限（寄存器文件带宽极高）
- 容量：每个SM 64KB（每线程约256个寄存器）

### 2. 寄存器分块的基本原理

#### 2.1 核心思想

**传统方式**：1个线程 → 1个输出元素
```
线程(0,0) 计算 C[0][0]
线程(0,1) 计算 C[0][1]
...
```

**寄存器分块**：1个线程 → M×N个输出元素
```
线程(0,0) 计算 C[0:M][0:N]  // 一个小块
线程(0,1) 计算 C[0:M][N:2N]
...
```

#### 2.2 数据流

```cuda
// 每个线程维护一个小块的结果
float results[THREAD_TILE_M][THREAD_TILE_N];

// 加载A的一部分到寄存器
float reg_a[THREAD_TILE_M];
for (int i = 0; i < THREAD_TILE_M; i++) {
    reg_a[i] = As[ty * THREAD_TILE_M + i][k];
}

// 加载B的一部分到寄存器
float reg_b[THREAD_TILE_N];
for (int j = 0; j < THREAD_TILE_N; j++) {
    reg_b[j] = Bs[k][tx * THREAD_TILE_N + j];
}

// 外积计算（所有数据在寄存器中）
for (int i = 0; i < THREAD_TILE_M; i++) {
    for (int j = 0; j < THREAD_TILE_N; j++) {
        results[i][j] += reg_a[i] * reg_b[j];
    }
}
```

**关键点**：
- `reg_a[i]` 被复用 THREAD_TILE_N 次
- `reg_b[j]` 被复用 THREAD_TILE_M 次
- 减少共享内存访问从 2×TILE_SIZE 到 2×TILE_SIZE/THREAD_TILE

### 3. 代码实现

#### 3.1 基础实现

```cuda
#define TILE_SIZE 32
#define THREAD_TILE_M 8
#define THREAD_TILE_N 8

__global__ void gemm_register_tiling(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    int M, int N, int K
) {
    __shared__ float As[TILE_SIZE][TILE_SIZE + 1];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE + 1];
    
    // 每个线程块处理的输出块大小
    int block_tile_m = TILE_SIZE;
    int block_tile_n = TILE_SIZE;
    
    // 线程块内的线程数需要调整
    // 原来32×32个线程，现在只需要(32/8)×(32/8)=4×4=16个线程
    int tx = threadIdx.x;  // 0-3
    int ty = threadIdx.y;  // 0-3
    
    // 每个线程负责的输出块的起始位置
    int thread_row = ty * THREAD_TILE_M;
    int thread_col = tx * THREAD_TILE_N;
    
    // 输出块在全局矩阵中的位置
    int row_base = blockIdx.y * TILE_SIZE;
    int col_base = blockIdx.x * TILE_SIZE;
    
    // 寄存器中存储结果
    float results[THREAD_TILE_M][THREAD_TILE_N] = {0.0f};
    
    // 遍历K维度
    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {
        // 协作加载A和B到共享内存
        // 注意：现在只有16个线程，需要每个线程加载更多数据
        int load_count = (TILE_SIZE * TILE_SIZE) / (blockDim.x * blockDim.y);
        
        for (int i = 0; i < load_count; i++) {
            int linear_idx = (ty * blockDim.x + tx) * load_count + i;
            int load_y = linear_idx / TILE_SIZE;
            int load_x = linear_idx % TILE_SIZE;
            
            int a_row = row_base + load_y;
            int a_col = t * TILE_SIZE + load_x;
            As[load_y][load_x] = (a_row < M && a_col < K) ? 
                                 A[a_row * K + a_col] : 0.0f;
            
            int b_row = t * TILE_SIZE + load_y;
            int b_col = col_base + load_x;
            Bs[load_y][load_x] = (b_row < K && b_col < N) ? 
                                 B[b_row * N + b_col] : 0.0f;
        }
        
        __syncthreads();
        
        // 寄存器分块计算
        for (int k = 0; k < TILE_SIZE; k++) {
            // 从共享内存加载到寄存器
            float reg_a[THREAD_TILE_M];
            float reg_b[THREAD_TILE_N];
            
            #pragma unroll
            for (int i = 0; i < THREAD_TILE_M; i++) {
                reg_a[i] = As[thread_row + i][k];
            }
            
            #pragma unroll
            for (int j = 0; j < THREAD_TILE_N; j++) {
                reg_b[j] = Bs[k][thread_col + j];
            }
            
            // 外积计算（纯寄存器操作）
            #pragma unroll
            for (int i = 0; i < THREAD_TILE_M; i++) {
                #pragma unroll
                for (int j = 0; j < THREAD_TILE_N; j++) {
                    results[i][j] += reg_a[i] * reg_b[j];
                }
            }
        }
        
        __syncthreads();
    }
    
    // 写回结果
    #pragma unroll
    for (int i = 0; i < THREAD_TILE_M; i++) {
        #pragma unroll
        for (int j = 0; j < THREAD_TILE_N; j++) {
            int row = row_base + thread_row + i;
            int col = col_base + thread_col + j;
            if (row < M && col < N) {
                C[row * N + col] = results[i][j];
            }
        }
    }
}

// 启动配置
dim3 blockDim(TILE_SIZE / THREAD_TILE_N, TILE_SIZE / THREAD_TILE_M);  // 4×4
dim3 gridDim((N + TILE_SIZE - 1) / TILE_SIZE, 
             (M + TILE_SIZE - 1) / TILE_SIZE);
```

#### 3.2 优化版本：向量化+寄存器分块

```cuda
#define TILE_SIZE 64
#define THREAD_TILE_M 8
#define THREAD_TILE_N 8
#define BLOCK_THREADS 256  // 64×64 / (8×8) = 64，实际使用256个线程

__global__ void gemm_register_tiling_optimized(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    int M, int N, int K
) {
    __shared__ float As[TILE_SIZE][TILE_SIZE + 1];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE + 1];
    
    int tid = threadIdx.x;
    
    // 计算此线程负责的输出块位置
    int warp_id = tid / 32;
    int lane_id = tid % 32;
    
    // 更复杂的线程到输出块的映射（此处简化）
    int thread_tile_id = tid / (32 / THREAD_TILE_N);
    int thread_row = (thread_tile_id / (TILE_SIZE / THREAD_TILE_N)) * THREAD_TILE_M;
    int thread_col = (thread_tile_id % (TILE_SIZE / THREAD_TILE_N)) * THREAD_TILE_N;
    
    int row_base = blockIdx.y * TILE_SIZE;
    int col_base = blockIdx.x * TILE_SIZE;
    
    // 寄存器存储结果
    float results[THREAD_TILE_M][THREAD_TILE_N];
    #pragma unroll
    for (int i = 0; i < THREAD_TILE_M; i++) {
        #pragma unroll
        for (int j = 0; j < THREAD_TILE_N; j++) {
            results[i][j] = 0.0f;
        }
    }
    
    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {
        // 向量化加载到共享内存
        int num_loads = (TILE_SIZE * TILE_SIZE) / BLOCK_THREADS;
        
        #pragma unroll
        for (int i = 0; i < num_loads / 4; i++) {
            int idx = tid * num_loads + i * 4;
            int y = idx / TILE_SIZE;
            int x = idx % TILE_SIZE;
            
            // 向量化加载
            float4 a_vec = *((float4*)&A[(row_base + y) * K + t * TILE_SIZE + x]);
            *((float4*)&As[y][x]) = a_vec;
            
            float4 b_vec = *((float4*)&B[(t * TILE_SIZE + y) * N + col_base + x]);
            *((float4*)&Bs[y][x]) = b_vec;
        }
        
        __syncthreads();
        
        // 寄存器分块计算
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; k++) {
            float reg_a[THREAD_TILE_M];
            float reg_b[THREAD_TILE_N];
            
            // 加载到寄存器
            #pragma unroll
            for (int i = 0; i < THREAD_TILE_M; i++) {
                reg_a[i] = As[thread_row + i][k];
            }
            
            #pragma unroll
            for (int j = 0; j < THREAD_TILE_N; j++) {
                reg_b[j] = Bs[k][thread_col + j];
            }
            
            // 外积（完全展开）
            #pragma unroll
            for (int i = 0; i < THREAD_TILE_M; i++) {
                #pragma unroll
                for (int j = 0; j < THREAD_TILE_N; j++) {
                    results[i][j] += reg_a[i] * reg_b[j];
                }
            }
        }
        
        __syncthreads();
    }
    
    // 向量化写回
    #pragma unroll
    for (int i = 0; i < THREAD_TILE_M; i++) {
        int row = row_base + thread_row + i;
        if (row < M && col_base + thread_col < N) {
            float4 result_vec;
            result_vec.x = results[i][0];
            result_vec.y = results[i][1];
            result_vec.z = results[i][2];
            result_vec.w = results[i][3];
            *((float4*)&C[row * N + col_base + thread_col]) = result_vec;
            
            // 处理剩余的元素
            for (int j = 4; j < THREAD_TILE_N; j++) {
                int col = col_base + thread_col + j;
                if (col < N) {
                    C[row * N + col] = results[i][j];
                }
            }
        }
    }
}
```

### 4. 性能分析

#### 4.1 访存次数对比

假设 TILE_SIZE=32, THREAD_TILE=8:

**无寄存器分块**：
```
每个元素的计算：
- 共享内存读取：2 × TILE_SIZE = 64次
- 浮点运算：2 × TILE_SIZE = 64次
- 算术强度（相对共享内存）：1 FLOP/load
```

**有寄存器分块**：
```
每个THREAD_TILE块的计算：
- 共享内存读取：2 × TILE_SIZE × THREAD_TILE = 512次
- 浮点运算：2 × TILE_SIZE × THREAD_TILE² = 4096次
- 算术强度：8 FLOP/load

提升：8倍
```

#### 4.2 寄存器使用

```cuda
// 每个线程的寄存器使用
float results[8][8];     // 64个寄存器
float reg_a[8];          // 8个寄存器
float reg_b[8];          // 8个寄存器
// 总计：~80个寄存器

// 每个SM的寄存器：65536个
// 每个线程80个 → 最多819个线程/SM
// 占用率：(819/2048) × 100% ≈ 40%
```

**需要权衡**：
- THREAD_TILE越大 → 寄存器使用越多 → 占用率可能下降
- 典型选择：8×8 或 4×8

#### 4.3 实测性能

在 NVIDIA A100 上测试 4096×4096 GEMM：

| 实现              | 时间(ms) | GFLOPS | 加速比 |
| ----------------- | -------- | ------ | ------ |
| 共享内存Tiling    | 95       | 1440   | 1.0x   |
| + 寄存器分块(4×4) | 58       | 2360   | 1.64x  |
| + 寄存器分块(8×8) | 42       | 3260   | 2.26x  |
| + 向量化          | 28       | 4890   | 3.39x  |

### 5. 线程数量调整

#### 5.1 线程块配置

```cuda
// 无寄存器分块：32×32 = 1024个线程
dim3 blockDim_basic(32, 32);

// 寄存器分块(8×8)：只需要 (32/8)×(32/8) = 16个线程
dim3 blockDim_reg(4, 4);

// 实际通常使用更多线程以提高占用率
// 例如64×64的Tile，8×8的寄存器分块
dim3 blockDim_opt(32, 8);  // 256个线程
```

#### 5.2 占用率考虑

```
目标：最大化SM占用率

限制因素：
1. 寄存器使用
2. 共享内存使用
3. 线程数

优化策略：
- 选择合适的THREAD_TILE大小
- 调整线程块大小
- 使用 __launch_bounds__ 限制寄存器
```

### 6. 高级优化技巧

#### 6.1 双缓冲寄存器

```cuda
// 流水线化：预取下一次迭代的数据
float reg_a[2][THREAD_TILE_M];  // 双缓冲
float reg_b[2][THREAD_TILE_N];

for (int k = 0; k < TILE_SIZE; k++) {
    int cur = k % 2;
    int next = 1 - cur;
    
    // 预取下一次迭代的数据
    if (k + 1 < TILE_SIZE) {
        for (int i = 0; i < THREAD_TILE_M; i++) {
            reg_a[next][i] = As[thread_row + i][k + 1];
        }
    }
    
    // 使用当前数据计算
    for (int i = 0; i < THREAD_TILE_M; i++) {
        for (int j = 0; j < THREAD_TILE_N; j++) {
            results[i][j] += reg_a[cur][i] * reg_b[cur][j];
        }
    }
}
```

#### 6.2 不同的Tile形状

```cuda
// 正方形
#define THREAD_TILE_M 8
#define THREAD_TILE_N 8

// 矩形（适应不同的矩阵形状）
#define THREAD_TILE_M 4
#define THREAD_TILE_N 16
```

### 7. 与Tensor Core对比

| 特性         | 寄存器分块   | Tensor Core         |
| ------------ | ------------ | ------------------- |
| 适用数据类型 | 任意         | FP16/BF16/INT8/TF32 |
| 性能提升     | 2-4x         | 10-20x              |
| 编程复杂度   | 中等         | 较高                |
| 硬件要求     | 任意GPU      | Volta+              |
| 适用场景     | FP32精度要求 | 可接受混合精度      |

## 总结

**寄存器分块的核心**：
- 每个线程计算多个输出元素（8×8）
- 将中间数据缓存在寄存器中
- 提高数据复用率和算术强度

**关键优势**：
1. **减少共享内存访问**：8倍减少（8×8分块）
2. **提高算术强度**：从1提升到8 FLOP/load
3. **充分利用寄存器**：最快的存储层次

**实现要点**：
- 调整线程数量（减少到1/64）
- 每个线程加载更多数据
- 外积计算模式
- 完全展开循环（`#pragma unroll`）

**性能提升**：
- 单独使用：2-3倍加速
- 结合向量化：3-4倍加速
- 是接近cuBLAS性能的关键技术

**权衡考虑**：
- 寄存器使用 vs 占用率
- THREAD_TILE大小选择
- 与其他优化技术的组合

寄存器分块是CPU/GPU GEMM优化的**经典技术**，在现代GPU上仍然是达到高性能的必要手段。

