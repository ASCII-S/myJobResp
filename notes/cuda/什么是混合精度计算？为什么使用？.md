# 什么是混合精度计算？为什么使用？

## 面试标准答案

混合精度计算是指在神经网络训练/推理中**同时使用多种数值精度**（主要是FP16和FP32）的技术。核心思想是：**大部分计算用FP16利用Tensor Core加速，关键操作用FP32保证数值稳定性**。使用原因：**1) 加速计算2-3倍；2) 减少显存占用50%；3) 提高吞吐量**。实现通过自动混合精度（AMP）框架，配合loss scaling解决FP16梯度下溢问题。现代深度学习训练的标准做法。

---

## 详细讲解

### 1. 混合精度的基本概念

#### 1.1 什么是混合精度？

不同于纯FP32或纯FP16训练，混合精度训练：
- **前向传播**：主要用FP16
- **权重存储**：FP32主副本
- **梯度计算**：FP16
- **权重更新**：FP32
- **Loss计算**：FP32

#### 1.2 数值精度对比

| 类型 | 位数 | 指数位 | 尾数位 | 范围      | 精度    |
| ---- | ---- | ------ | ------ | --------- | ------- |
| FP32 | 32   | 8      | 23     | ±3.4×10³⁸ | 7位小数 |
| FP16 | 16   | 5      | 10     | ±65504    | 3位小数 |
| BF16 | 16   | 8      | 7      | ±3.4×10³⁸ | 2位小数 |
| TF32 | 19   | 8      | 10     | ±3.4×10³⁸ | 3位小数 |

### 2. 为什么使用混合精度？

#### 2.1 三大核心优势

**1. 计算加速（2-3倍）**

| 模型      | FP32时间 | 混合精度时间 | 加速比 |
| --------- | -------- | ------------ | ------ |
| ResNet-50 | 100s     | 35s          | 2.9×   |
| BERT-Base | 180s     | 65s          | 2.8×   |
| GPT-2     | 500s     | 170s         | 2.9×   |

**2. 显存节省（50%）**

```
模型参数和激活值占用减半：
- FP32: 4 bytes/param
- FP16: 2 bytes/param

示例（BERT-Large，340M参数）：
- FP32: 1.36 GB 参数 + 8 GB 激活 = 9.36 GB
- 混合精度: 0.68 GB (FP16) + 1.36 GB (FP32副本) + 4 GB 激活 = 6.04 GB
节省: 35%
```

**3. 吞吐量提升**

```
V100 GPU性能：
- FP32: 15 TFLOPS
- FP16 (Tensor Core): 125 TFLOPS (8.3×)

实际训练吞吐量：
- ResNet-50: 400 → 1200 images/s (3×)
- BERT: 50 → 140 samples/s (2.8×)
```

#### 2.2 为什么不全用FP16？

**FP16的三大问题：**

1. **下溢（Underflow）**
   - 梯度太小（<6×10⁻⁸）变为0
   - 导致权重不更新

2. **上溢（Overflow）**
   - 数值超过65504
   - 变成NaN/Inf

3. **舍入误差**
   - 累加多个小数时精度损失
   - 影响收敛

**混合精度的解决方案：**
- Loss Scaling：放大梯度防止下溢
- FP32权重副本：保证更新精度
- 选择性FP32：关键操作用高精度

### 3. 混合精度训练流程

#### 3.1 标准流程图

```
初始化：
├─ 权重（FP32主副本）
└─ 模型（FP16计算副本）

训练迭代：
1. 前向传播（FP16）
   └─ 自动转换输入为FP16
2. Loss计算（FP32）
   └─ 避免数值问题
3. Loss Scaling
   └─ loss *= scale_factor
4. 反向传播（FP16）
   └─ 计算梯度
5. Unscale梯度
   └─ grad /= scale_factor
6. 梯度检查
   └─ 检测Inf/NaN
7. 权重更新（FP32）
   └─ 用FP32副本更新
8. 更新FP16副本
   └─ 同步权重
```

#### 3.2 PyTorch实现

```python
from torch.cuda.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()
    
    # 前向传播（自动FP16）
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    # 反向传播 + scaling
    scaler.scale(loss).backward()
    
    # 更新权重（自动unscale + FP32更新）
    scaler.step(optimizer)
    scaler.update()
```

### 4. Loss Scaling详解

#### 4.1 为什么需要？

```
FP16最小正数：6.1×10⁻⁵
典型梯度范围：10⁻⁷ ~ 10⁻³

问题：小梯度会underflow → 0

解决：放大梯度
scaled_loss = loss × 2¹⁶
scaled_grad = ∂(scaled_loss)/∂w = grad × 2¹⁶
真实梯度 = scaled_grad / 2¹⁶
```

#### 4.2 动态Loss Scaling

```python
# PyTorch GradScaler自动管理
scaler = GradScaler(
    init_scale=2.**16,      # 初始缩放因子
    growth_factor=2.0,      # 增长系数
    backoff_factor=0.5,     # 缩小系数
    growth_interval=2000    # 增长间隔
)

# 自动调整逻辑：
if 检测到Inf/NaN:
    scale *= backoff_factor  # 缩小
    跳过本次更新
else:
    连续N次成功后:
        scale *= growth_factor  # 增大
```

### 5. 不同精度类型的选择

#### 5.1 FP16 vs BF16 vs TF32

| 特性        | FP16         | BF16           | TF32       |
| ----------- | ------------ | -------------- | ---------- |
| GPU支持     | V100+        | A100+          | A100+      |
| 数值范围    | 小（易溢出） | 大（与FP32同） | 大         |
| 需要scaling | 是           | 否             | 否         |
| 训练稳定性  | 中           | 高             | 高         |
| 性能        | 最快         | 与FP16相当     | 略慢于FP16 |
| 推荐场景    | V100推理     | A100训练       | A100默认   |

#### 5.2 实际选择建议

```python
# V100: FP16 + Loss Scaling
with autocast(dtype=torch.float16):
    output = model(input)
scaler.scale(loss).backward()

# A100: BF16（推荐）
with autocast(dtype=torch.bfloat16):
    output = model(input)
# 不需要scaler

# A100: TF32（自动启用，最简单）
# 无需任何代码修改，PyTorch自动使用
```

### 6. 常见问题和解决方案

#### 6.1 训练发散/不收敛

**原因：** Loss scaling不当或数值溢出

**解决：**
```python
# 1. 调整初始scale
scaler = GradScaler(init_scale=2.**12)  # 降低初始值

# 2. 手动clip梯度
scaler.unscale_(optimizer)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
scaler.step(optimizer)

# 3. 某些层用FP32
class Model(nn.Module):
    def __init__(self):
        self.conv = nn.Conv2d(...)  # FP16
        self.bn = nn.BatchNorm2d(...).float()  # 强制FP32
```

#### 6.2 精度损失

**检查方法：**
```python
# 对比FP32和混合精度结果
model_fp32 = Model().cuda()
model_amp = Model().cuda()

# FP32
with torch.no_grad():
    out_fp32 = model_fp32(input)

# 混合精度
with torch.no_grad(), autocast():
    out_amp = model_amp(input)

# 计算差异
diff = (out_fp32 - out_amp).abs().mean()
print(f"差异: {diff:.6f}")  # 应该 < 1e-3
```

#### 6.3 性能提升不明显

**可能原因：**
1. 矩阵维度未对齐（需要8/16倍数）
2. 模型太小（通信开销大于计算）
3. 瓶颈在数据加载

**优化：**
```python
# 确保维度对齐
def align_dim(dim, divisor=8):
    return ((dim + divisor - 1) // divisor) * divisor

# 增大batch size（利用显存节省）
old_batch = 32
new_batch = 64  # 混合精度显存减半，可以加倍batch

# 使用多个worker加载数据
dataloader = DataLoader(..., num_workers=4, pin_memory=True)
```

### 7. 不同框架的使用

#### 7.1 TensorFlow

```python
from tensorflow.keras import mixed_precision

# 启用混合精度
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# 模型自动使用FP16
model = tf.keras.Sequential([...])

# Loss scaling自动处理
optimizer = tf.keras.optimizers.Adam()
optimizer = mixed_precision.LossScaleOptimizer(optimizer)
```

#### 7.2 JAX

```python
from jax import numpy as jnp

# 定义混合精度策略
def mixed_precision_apply(fn):
    def wrapper(x):
        x_fp16 = x.astype(jnp.float16)
        out = fn(x_fp16)
        return out.astype(jnp.float32)
    return wrapper

# 应用
model = mixed_precision_apply(model)
```

### 8. 性能评估

**评估指标：**

| 指标              | 计算方法                             | 目标  |
| ----------------- | ------------------------------------ | ----- |
| 加速比            | FP32时间 / 混合精度时间              | >2×   |
| 显存节省          | (FP32显存 - 混合精度显存) / FP32显存 | >30%  |
| 精度损失          | \|FP32 acc - 混合精度 acc\|          | <0.5% |
| Tensor Core利用率 | Profiler测量                         | >80%  |

### 9. 最佳实践

| 建议                       | 说明                   |
| -------------------------- | ---------------------- |
| ✅ 默认启用混合精度         | 现代GPU训练必备        |
| ✅ V100用FP16，A100用BF16   | 针对硬件优化           |
| ✅ 使用框架AMP              | 自动处理复杂细节       |
| ✅ 增大batch size           | 利用节省的显存         |
| ✅ 检查精度损失             | 确保模型质量           |
| ✅ Profiling验证Tensor Core | 确保真正加速           |
| ❌ 不要手动管理scaling      | 容易出错，用GradScaler |
| ❌ 小模型不一定需要         | 计算占比小时收益有限   |

### 10. 快速启用清单

```
□ 环境检查
  □ GPU: Volta/Turing/Ampere/Hopper
  □ PyTorch >= 1.6 或 TF >= 2.4
  □ CUDA >= 11.0

□ 代码修改（3行）
  from torch.cuda.amp import autocast, GradScaler
  scaler = GradScaler()
  
  with autocast():
      output = model(input)
      
  scaler.scale(loss).backward()
  scaler.step(optimizer)
  scaler.update()

□ 验证
  □ 性能提升 >2×
  □ 精度损失 <0.5%
  □ 无NaN/Inf
```

### 11. 记忆口诀

**"混合精度训练快，FP16计算FP32存；Loss Scaling防下溢，GradScaler自动稳；显存减半速度翻，深度学习新标准；V100用FP16，A100推荐BF16；三行代码即可用，性能提升看得见"**

