# cuBLAS提供了哪些功能？如何使用？

## 面试标准答案

cuBLAS是NVIDIA提供的CUDA加速线性代数库，实现了标准BLAS（Basic Linear Algebra Subprograms）的GPU版本。主要提供三个级别的功能：**Level 1（向量-向量操作）、Level 2（矩阵-向量操作）、Level 3（矩阵-矩阵操作）**。使用流程包括：创建句柄、设置数据、调用库函数、销毁句柄。常用于深度学习中的全连接层、矩阵乘法等场景，相比手写CUDA性能更优且代码更简洁。

---

## 详细讲解

### 1. cuBLAS功能分级

#### 1.1 BLAS三个级别

cuBLAS遵循标准BLAS规范，分为三个级别：

| 级别    | 操作类型  | 计算复杂度 | 典型函数              | 应用场景           |
| ------- | --------- | ---------- | --------------------- | ------------------ |
| Level 1 | 向量-向量 | O(n)       | dot, axpy, scal, norm | 向量归一化、点积   |
| Level 2 | 矩阵-向量 | O(n²)      | gemv, ger, trsv       | 线性方程组、变换   |
| Level 3 | 矩阵-矩阵 | O(n³)      | gemm, trsm, syrk      | 深度学习、科学计算 |

**性能特点：** Level 3性能最优（计算密集），Level 1性能最低（内存密集）

### 2. 核心功能详解

#### 2.1 Level 1：向量操作

```cuda
// 点积：result = x · y
cublasSdot(handle, n, x, 1, y, 1, &result);

// AXPY：y = alpha * x + y
cublasSaxpy(handle, n, &alpha, x, 1, y, 1);

// 缩放：x = alpha * x
cublasSscal(handle, n, &alpha, x, 1);

// L2范数：result = ||x||₂
cublasSnrm2(handle, n, x, 1, &result);

// 索引最大值：找到|x[i]|最大的索引
cublasIsamax(handle, n, x, 1, &result);
```

**实际应用示例：向量归一化**

```cuda
float norm;
cublasSnrm2(handle, n, x, 1, &norm);  // 计算||x||
float alpha = 1.0f / norm;
cublasSscal(handle, n, &alpha, x, 1);  // x = x / ||x||
```

#### 2.2 Level 2：矩阵-向量操作

```cuda
// GEMV：y = alpha * A * x + beta * y
// A: m×n矩阵，x: n维向量，y: m维向量
cublasSgemv(handle, 
            CUBLAS_OP_N,     // 不转置A
            m, n,            // 矩阵维度
            &alpha,          // 缩放因子
            A, lda,          // 矩阵A及其leading dimension
            x, 1,            // 向量x及其步长
            &beta,           // y的缩放因子
            y, 1);           // 向量y及其步长

// GER：A = alpha * x * yᵀ + A（秩1更新）
cublasSger(handle, m, n, &alpha, x, 1, y, 1, A, lda);
```

**应用场景：** 神经网络中的线性变换、矩阵向量乘法

#### 2.3 Level 3：矩阵-矩阵操作（最重要）

**GEMM（通用矩阵乘法）**

```cuda
// C = alpha * op(A) * op(B) + beta * C
// op可以是转置(T)或不转置(N)

cublasSgemm(handle,
            CUBLAS_OP_N,    // A不转置
            CUBLAS_OP_N,    // B不转置
            m, n, k,        // 矩阵维度
            &alpha,         // 缩放因子
            A, lda,         // A: m×k
            B, ldb,         // B: k×n
            &beta,          // C的缩放因子
            C, ldc);        // C: m×n
```

**数据类型变体：**

| 函数前缀 | 数据类型        | 用途                    |
| -------- | --------------- | ----------------------- |
| `S`      | float           | 单精度浮点              |
| `D`      | double          | 双精度浮点              |
| `C`      | cuComplex       | 单精度复数              |
| `Z`      | cuDoubleComplex | 双精度复数              |
| `H`      | half (\_\_half) | 半精度浮点（FP16）      |
| 特殊     | mixed precision | 混合精度（Tensor Core） |

### 3. 使用流程完整示例

#### 3.1 基本使用模板

```cuda
#include <cublas_v2.h>

// 步骤1：创建句柄
cublasHandle_t handle;
cublasCreate(&handle);

// 步骤2：分配和初始化数据
float *d_A, *d_B, *d_C;
int m = 1024, n = 1024, k = 1024;

cudaMalloc(&d_A, m * k * sizeof(float));
cudaMalloc(&d_B, k * n * sizeof(float));
cudaMalloc(&d_C, m * n * sizeof(float));

// 将host数据拷贝到device（假设已有h_A, h_B）
cudaMemcpy(d_A, h_A, m * k * sizeof(float), cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, k * n * sizeof(float), cudaMemcpyHostToDevice);

// 步骤3：设置参数并调用
float alpha = 1.0f, beta = 0.0f;
cublasSgemm(handle,
            CUBLAS_OP_N, CUBLAS_OP_N,
            m, n, k,
            &alpha,
            d_A, m,  // leading dimension = m (列优先)
            d_B, k,
            &beta,
            d_C, m);

// 步骤4：拷贝结果回host
cudaMemcpy(h_C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);

// 步骤5：清理资源
cublasDestroy(handle);
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
```

#### 3.2 关键注意事项

**1. 列优先存储（Column-Major）**

```cuda
// cuBLAS使用Fortran风格的列优先存储
// C/C++是行优先，需要转换思维

// 在C中：A[i][j] = A[i * n + j]（行优先）
// 在cuBLAS中：A按列存储，相当于A的转置

// 解决方案：交换操作数
// 计算C = A * B，在cuBLAS中写成：
cublasSgemm(handle,
            CUBLAS_OP_N, CUBLAS_OP_N,
            n, m, k,     // 注意维度顺序
            &alpha,
            d_B, n,      // B和A交换位置
            d_A, k,
            &beta,
            d_C, n);
```

**2. Leading Dimension（LD）**

```cuda
// LD表示矩阵在内存中一列的步长
// 对于m×n矩阵，通常LD = m（列优先）

// 如果使用子矩阵：
// 原矩阵100×100，使用左上角50×50的子矩阵
// LD仍然是100（原矩阵的行数）
cublasSgemm(handle, ..., d_A, 100, ...);  // LD = 100
```

### 4. 高级功能

#### 4.1 批量矩阵乘法（Batched GEMM）

```cuda
// 一次性计算多个独立的矩阵乘法
// 方法1：Batched（矩阵大小相同）
cublasSgemmBatched(handle,
                   CUBLAS_OP_N, CUBLAS_OP_N,
                   m, n, k,
                   &alpha,
                   d_A_array, lda,  // 指针数组
                   d_B_array, ldb,
                   &beta,
                   d_C_array, ldc,
                   batchCount);      // 批次数量

// 方法2：Strided Batched（矩阵连续存储）
cublasSgemmStridedBatched(handle,
                          CUBLAS_OP_N, CUBLAS_OP_N,
                          m, n, k,
                          &alpha,
                          d_A, lda, strideA,  // stride = m*k
                          d_B, ldb, strideB,  // stride = k*n
                          &beta,
                          d_C, ldc, strideC,  // stride = m*n
                          batchCount);
```

**应用场景：** 
- 卷积神经网络中的批量全连接层
- 多样本的矩阵运算
- Transformer中的注意力机制

#### 4.2 Tensor Core加速（混合精度）

```cuda
// 使用FP16输入，FP32累加
cublasGemmEx(handle,
             CUBLAS_OP_N, CUBLAS_OP_N,
             m, n, k,
             &alpha,
             d_A, CUDA_R_16F, lda,     // FP16输入
             d_B, CUDA_R_16F, ldb,     // FP16输入
             &beta,
             d_C, CUDA_R_32F, ldc,     // FP32输出
             CUDA_R_32F,               // 计算类型
             CUBLAS_GEMM_DEFAULT_TENSOR_OP);  // 启用Tensor Core

// 或使用更简单的接口
cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH);
```

**性能提升：** Tensor Core可提供8-10倍的GEMM性能

#### 4.3 Stream和异步执行

```cuda
cudaStream_t stream;
cudaStreamCreate(&stream);

// 将cuBLAS操作绑定到stream
cublasSetStream(handle, stream);

// 之后的所有cuBLAS调用都在该stream中异步执行
cublasSgemm(handle, ...);  // 异步执行

// 等待stream完成
cudaStreamSynchronize(stream);
```

### 5. 性能优化技巧

#### 5.1 选择合适的函数级别

```cuda
// ❌ 错误：用循环调用Level 1函数做矩阵乘法
for (int i = 0; i < m; i++) {
    cublasSdot(handle, n, &A[i*n], 1, B, 1, &C[i]);
}

// ✅ 正确：直接使用Level 3函数
cublasSgemm(handle, ...);  // 性能提升10-100倍
```

#### 5.2 批量操作减少kernel启动开销

```cuda
// ❌ 低效：多次调用
for (int i = 0; i < 100; i++) {
    cublasSgemm(handle, ..., &A[i*m*k], &B[i*k*n], &C[i*m*n]);
}

// ✅ 高效：使用Batched版本
cublasSgemmStridedBatched(handle, ..., batchCount=100);
```

#### 5.3 预先分配句柄

```cuda
// ❌ 在循环中重复创建
for (int i = 0; i < 1000; i++) {
    cublasCreate(&handle);
    cublasSgemm(...);
    cublasDestroy(handle);
}

// ✅ 复用句柄
cublasCreate(&handle);
for (int i = 0; i < 1000; i++) {
    cublasSgemm(...);
}
cublasDestroy(handle);
```

### 6. 实际应用场景

#### 6.1 深度学习：全连接层

```cuda
// Forward: output = input * weight^T + bias
// input: batch × in_features
// weight: out_features × in_features
// output: batch × out_features

cublasSgemm(handle,
            CUBLAS_OP_T,    // weight转置
            CUBLAS_OP_N,
            out_features, batch, in_features,
            &alpha,
            d_weight, in_features,
            d_input, in_features,
            &beta,
            d_output, out_features);

// 加bias（使用gemv或广播）
cublasSgemm(handle,
            CUBLAS_OP_N, CUBLAS_OP_N,
            out_features, batch, 1,
            &one,
            d_bias, out_features,
            d_ones, 1,  // 全1向量
            &one,
            d_output, out_features);
```

#### 6.2 Transformer：注意力机制

```cuda
// Q, K, V: (batch, seq_len, d_model)
// Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V

// 步骤1：QK^T
cublasSgemmStridedBatched(handle,
                          CUBLAS_OP_T, CUBLAS_OP_N,
                          seq_len, seq_len, d_k,
                          &scale,  // scale = 1/sqrt(d_k)
                          d_K, d_k, seq_len * d_k,
                          d_Q, d_k, seq_len * d_k,
                          &zero,
                          d_scores, seq_len, seq_len * seq_len,
                          batch);

// 步骤2：softmax(scores)（使用自定义kernel）

// 步骤3：scores * V
cublasSgemmStridedBatched(handle,
                          CUBLAS_OP_N, CUBLAS_OP_N,
                          d_v, seq_len, seq_len,
                          &one,
                          d_V, d_v, seq_len * d_v,
                          d_scores, seq_len, seq_len * seq_len,
                          &zero,
                          d_output, d_v, seq_len * d_v,
                          batch);
```

### 7. 常见错误和调试

#### 7.1 错误处理

```cuda
cublasStatus_t status;
status = cublasSgemm(handle, ...);

if (status != CUBLAS_STATUS_SUCCESS) {
    switch(status) {
        case CUBLAS_STATUS_NOT_INITIALIZED:
            printf("cuBLAS未初始化\n");
            break;
        case CUBLAS_STATUS_INVALID_VALUE:
            printf("参数无效\n");
            break;
        case CUBLAS_STATUS_EXECUTION_FAILED:
            printf("执行失败\n");
            break;
        default:
            printf("未知错误\n");
    }
}
```

#### 7.2 常见问题

| 问题                 | 原因                     | 解决方案                 |
| -------------------- | ------------------------ | ------------------------ |
| 结果不正确           | 列优先vs行优先混淆       | 理解LD参数，或转置矩阵   |
| 性能不佳             | 使用了低级别函数         | 改用Level 3函数          |
| 内存访问错误         | LD设置错误               | 检查leading dimension    |
| alpha/beta在device？ | cuBLAS要求在host或device | 确保alpha/beta在host内存 |

### 8. 性能对比

```
实测性能（1024×1024矩阵乘法，V100 GPU）：

手写naive CUDA kernel:     ~50 GFLOPS
手写优化CUDA kernel:       ~500 GFLOPS
cuBLAS（FP32）:            ~7000 GFLOPS
cuBLAS（FP16 Tensor Core）: ~15000 GFLOPS

结论：使用cuBLAS是明智选择
```

### 9. 最佳实践总结

| 建议                        | 说明                      |
| --------------------------- | ------------------------- |
| ✅ 优先使用Level 3函数       | 计算密集，性能最优        |
| ✅ 复用cuBLAS句柄            | 减少创建销毁开销          |
| ✅ 使用Batched版本           | 批量操作提升效率          |
| ✅ 启用Tensor Core           | FP16/混合精度大幅提升性能 |
| ✅ 绑定Stream实现并发        | 多stream并发执行          |
| ✅ 理解列优先存储            | 避免因内存布局导致的错误  |
| ❌ 避免频繁的host-device传输 | 尽量在GPU上完成所有计算   |
| ❌ 避免在循环中创建句柄      | 性能损失严重              |

### 10. 快速参考

```cuda
// 常用函数速查
cublasSgemm    // 单精度矩阵乘法
cublasDgemm    // 双精度矩阵乘法
cublasHgemm    // 半精度矩阵乘法
cublasGemmEx   // 混合精度矩阵乘法（Tensor Core）

cublasSgemv    // 矩阵-向量乘法
cublasSdot     // 向量点积
cublasSaxpy    // y = ax + y
cublasSscal    // x = ax

// Batched版本
cublasSgemmBatched
cublasSgemmStridedBatched
```

### 11. 记忆口诀

**"Level三最强性能优，列优先存储要记牢；句柄复用减开销，Batched操作效率高；Tensor Core混合精度好，深度学习离不了"**

