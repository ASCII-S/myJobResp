---
created: '2025-10-19'
last_reviewed: '2025-10-19'
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 精通大模型压缩技术
- 精通大模型压缩技术/困惑度（Perplexity）、准确率、F1分数等任务指标的变化.md
related_outlines: []
---

# 困惑度（Perplexity）、准确率、F1分数等任务指标的变化

## 面试标准答案

模型压缩后的性能评估需要**根据任务类型选择合适的指标**。**分类任务**关注准确率（Accuracy）、精确率/召回率、F1分数、AUC等；**生成任务**使用困惑度（Perplexity）、BLEU、ROUGE等；**排序/检索任务**采用NDCG、MRR、MAP等。压缩后这些指标通常会轻微下降：量化（INT8）一般下降0.5-2%，剪枝根据比例下降1-5%，知识蒸馏可能下降3-10%。评估时需要**多指标综合考量**，避免单一指标的误导，并在真实业务数据上验证而非仅依赖标准数据集。

---

## 详细讲解

### 1. 核心评估指标详解

#### （1）分类任务指标

**准确率（Accuracy）**
```
定义：预测正确的样本数 / 总样本数
公式：Accuracy = (TP + TN) / (TP + TN + FP + FN)

优点：直观易懂，适合类别平衡数据
缺点：类别不平衡时会误导（如99%负样本时，全预测负类也能达到99%）

压缩后典型变化：
- INT8量化：-0.3% ~ -1.5%
- INT4量化：-1% ~ -3%
- 剪枝30%：-1% ~ -2%
- 剪枝50%：-2% ~ -4%
```

**精确率（Precision）与召回率（Recall）**
```
精确率 = TP / (TP + FP)  # 预测为正的样本中真正为正的比例
召回率 = TP / (TP + FN)  # 真实为正的样本中被正确预测的比例

应用场景：
- 精确率优先：垃圾邮件检测（宁可漏检，不误杀正常邮件）
- 召回率优先：癌症筛查（宁可误诊，不漏诊）

压缩影响：
- 通常精确率和召回率不均衡下降
- 剪枝可能更多影响召回率（模型容量降低）
- 量化通常影响相对均衡
```

**F1分数（F1-Score）**
```
定义：精确率和召回率的调和平均数
公式：F1 = 2 × (Precision × Recall) / (Precision + Recall)

变体：
- F0.5：更重视精确率
- F2：更重视召回率
- Macro-F1：各类别F1的平均（类别平等）
- Micro-F1：全局计算（样本平等）

压缩后变化：
- INT8量化：-0.5% ~ -2%
- 激进压缩（INT4+剪枝）：-3% ~ -6%

示例：
原始模型：F1 = 0.924
INT8量化后：F1 = 0.912 (-1.3%)
评估：下降在可接受范围 ✓
```

**AUC-ROC（曲线下面积）**
```
定义：ROC曲线下的面积，衡量分类器在不同阈值下的整体表现
取值：0.5-1.0（0.5为随机猜测，1.0为完美分类）

优点：
- 对类别不平衡不敏感
- 考虑所有可能的分类阈值
- 更稳定，不易受压缩影响

压缩后变化：
- AUC通常比Accuracy更稳定
- INT8量化：-0.005 ~ -0.02
- 剪枝40%：-0.01 ~ -0.03

推荐场景：
- 评估压缩对模型判别能力的整体影响
- 对比不同压缩方法的优劣
```

#### （2）生成任务指标

**困惑度（Perplexity, PPL）**
```
定义：语言模型对测试数据的预测不确定性
公式：PPL = exp(-1/N × Σ log P(wi|context))

解释：
- 越低越好（表示模型越"确定"）
- 可理解为"模型平均有多少个等可能的下一个词选择"
- PPL=50 表示平均面临50个等概率选择

压缩后变化：
- INT8量化：+5% ~ +15%
- INT4量化：+10% ~ +25%
- 剪枝30%：+8% ~ +20%
- 知识蒸馏：+15% ~ +40%（取决于学生模型大小）

示例：
原始模型：PPL = 12.3
INT8量化：PPL = 13.8 (+12.2%)
INT4量化：PPL = 15.6 (+26.8%)

评估标准：
- PPL增加 < 20%：通常可接受
- PPL增加 > 30%：需要人工验证生成质量
```

**BLEU（机器翻译评估）**
```
定义：N-gram精确匹配度，考虑1到4-gram
取值：0-100分（或0-1）

计算：BLEU = BP × exp(Σ wn log pn)
- pn：n-gram精确率
- BP：短句惩罚因子

压缩影响：
- INT8量化：-0.5 ~ -2 BLEU points
- 知识蒸馏：-2 ~ -5 BLEU points

示例（WMT翻译任务）：
原始：BLEU = 34.2
INT8：BLEU = 33.1 (-1.1) ✓ 可接受
INT4：BLEU = 31.5 (-2.7) ⚠️ 需人工评估

注意：
- BLEU只看表面匹配，不考虑语义
- 需配合人工评估
```

**ROUGE（文本摘要评估）**
```
指标族：
- ROUGE-N：N-gram召回率（ROUGE-1, ROUGE-2常用）
- ROUGE-L：最长公共子序列
- ROUGE-S：跳跃二元组

压缩影响：
- ROUGE通常比BLEU对压缩更鲁棒
- INT8量化：-1% ~ -3%
- 剪枝：-2% ~ -5%

示例（新闻摘要）：
原始：ROUGE-1 = 42.5, ROUGE-L = 38.2
压缩后：ROUGE-1 = 41.1, ROUGE-L = 37.0
评估：下降约3%，在合理范围 ✓
```

**人工评估（Human Evaluation）**
```
维度：
- 流畅性（Fluency）：语言是否自然
- 相关性（Relevance）：是否回答问题
- 一致性（Consistency）：前后是否矛盾
- 信息量（Informativeness）：内容是否丰富

评分方式：
- Likert量表（1-5分）
- 成对比较（A vs B哪个更好）
- 绝对评分 + 相对排序

压缩影响案例：
原始模型：流畅性 4.3, 相关性 4.1
INT8量化：流畅性 4.2, 相关性 4.0
INT4量化：流畅性 3.9, 相关性 3.8

关键：
- 人工评估是最终标准
- 自动指标仅供参考
- 压缩后必须抽样人工验证
```

#### （3）排序/检索任务指标

**NDCG（归一化折损累积增益）**
```
定义：考虑位置权重的排序质量指标
公式：NDCG@k = DCG@k / IDCG@k

特点：
- 排名越靠前的相关文档权重越大
- 考虑相关性等级（不只是二元）
- 归一化后可跨查询比较

压缩影响：
- INT8量化：-1% ~ -3%
- 向量量化（检索系统）：-2% ~ -5%

示例（搜索引擎）：
原始：NDCG@10 = 0.742
压缩后：NDCG@10 = 0.721 (-2.8%)
业务影响：轻微，但需监控点击率
```

**MRR（平均倒数排名）**
```
定义：首个相关结果排名倒数的平均值
公式：MRR = 1/|Q| × Σ 1/rank_i

解释：
- 只关注第一个相关结果
- 适合"寻找答案"类任务
- MRR=0.5 表示平均第2位出现相关结果

压缩影响：
- 通常比NDCG下降更明显（首位更敏感）
- INT8：-2% ~ -4%
```

### 2. 不同压缩技术的指标变化模式

#### 量化技术

| 量化方法      | Accuracy      | F1-Score      | Perplexity  | BLEU        | 备注     |
| ------------- | ------------- | ------------- | ----------- | ----------- | -------- |
| **FP16**      | -0.1% ~ -0.5% | -0.2% ~ -0.6% | +2% ~ +5%   | -0.3 ~ -0.8 | 几乎无损 |
| **INT8 PTQ**  | -0.5% ~ -2%   | -0.8% ~ -2.5% | +5% ~ +15%  | -0.8 ~ -2   | 最常用   |
| **INT8 QAT**  | -0.2% ~ -1%   | -0.3% ~ -1.5% | +3% ~ -10%  | -0.5 ~ -1.5 | 质量更好 |
| **INT4 GPTQ** | -1% ~ -3%     | -1.5% ~ -4%   | +10% ~ +25% | -1.5 ~ -3   | 激进压缩 |
| **INT4 AWQ**  | -0.8% ~ -2.5% | -1.2% ~ -3.5% | +8% ~ +20%  | -1.2 ~ -2.5 | 优于GPTQ |

**关键观察**：
- 量化对生成任务（Perplexity）影响大于分类任务
- QAT（量化感知训练）显著好于PTQ（训练后量化）
- INT4需要高级算法（GPTQ/AWQ）才能保持性能

#### 剪枝技术

| 剪枝类型         | Accuracy      | F1-Score    | Perplexity  | 特点               |
| ---------------- | ------------- | ----------- | ----------- | ------------------ |
| **非结构化 30%** | -0.5% ~ -1.5% | -0.8% ~ -2% | +5% ~ +12%  | 压缩比高但不加速   |
| **非结构化 50%** | -1.5% ~ -3%   | -2% ~ -4%   | +10% ~ +20% | 需要Fine-tuning    |
| **非结构化 70%** | -3% ~ -6%     | -4% ~ -8%   | +20% ~ +40% | 大幅质量下降       |
| **结构化 30%**   | -1% ~ -2.5%   | -1.5% ~ -3% | +8% ~ -18%  | 硬件友好，实际加速 |
| **结构化 50%**   | -2.5% ~ -5%   | -3.5% ~ -6% | +18% ~ -35% | 激进但可用         |

**关键观察**：
- 剪枝比例 < 40% 通常可接受
- 结构化剪枝牺牲更多精度但能实际加速
- 迭代式剪枝（逐步提高比例）效果好于一次性剪枝

#### 知识蒸馏

| 学生/教师比例 | Accuracy    | F1-Score    | Perplexity    | 示例     |
| ------------- | ----------- | ----------- | ------------- | -------- |
| **50%参数量** | -2% ~ -4%   | -3% ~ -5%   | +15% ~ +30%   | 70B→35B  |
| **20%参数量** | -4% ~ -8%   | -5% ~ -10%  | +25% ~ +50%   | 70B→14B  |
| **10%参数量** | -6% ~ -12%  | -8% ~ -15%  | +40% ~ +80%   | 70B→7B   |
| **1%参数量**  | -10% ~ -20% | -15% ~ -25% | +100% ~ +200% | 70B→700M |

**关键观察**：
- 蒸馏的性能损失与压缩比直接相关
- 小学生模型需要优秀架构（如MobileBERT、DistilBERT）
- 多教师蒸馏可以缓解性能下降

### 3. 评估最佳实践

#### 实践1：建立完整的评估流程

```python
class CompressionEvaluator:
    """模型压缩完整评估框架"""
    
    def evaluate_all_metrics(self, original_model, compressed_model):
        results = {}
        
        # 1. 标准数据集评估
        results['benchmark'] = {
            'accuracy': self.eval_accuracy(compressed_model),
            'f1_score': self.eval_f1(compressed_model),
            'perplexity': self.eval_perplexity(compressed_model)
        }
        
        # 2. 任务特定指标
        if self.task_type == 'classification':
            results['task_metrics'] = {
                'precision': self.eval_precision(compressed_model),
                'recall': self.eval_recall(compressed_model),
                'auc_roc': self.eval_auc(compressed_model)
            }
        elif self.task_type == 'generation':
            results['task_metrics'] = {
                'bleu': self.eval_bleu(compressed_model),
                'rouge': self.eval_rouge(compressed_model),
                'meteor': self.eval_meteor(compressed_model)
            }
        
        # 3. 真实业务数据评估（关键！）
        results['production'] = {
            'user_data_accuracy': self.eval_on_user_data(compressed_model),
            'edge_case_performance': self.eval_edge_cases(compressed_model)
        }
        
        # 4. 人工评估（生成任务必须）
        if self.task_type == 'generation':
            results['human_eval'] = self.human_evaluation(compressed_model)
        
        # 5. 对比原始模型
        results['degradation'] = self.calculate_degradation(
            results, original_model
        )
        
        return results
```

#### 实践2：多维度综合评估

**避免单一指标误导**：
```
案例：聊天机器人压缩
┌────────────────────────────────────────┐
│ 自动指标：                              │
│  - Perplexity: 15.2 → 18.9 (+24%) ⚠️   │
│  - BLEU: 21.3 → 19.8 (-7%) ⚠️          │
│  结论：看起来下降明显                    │
├────────────────────────────────────────┤
│ 人工评估：                              │
│  - 流畅性: 4.2 → 4.1 (-2%) ✓           │
│  - 相关性: 4.0 → 3.9 (-2.5%) ✓         │
│  - 用户满意度: 4.1 → 4.0 (-2.4%) ✓     │
│  结论：实际体验下降很小                  │
├────────────────────────────────────────┤
│ 业务指标：                              │
│  - 平均对话轮次: 3.2 → 3.3 (+3%)       │
│  - 单轮解决率: 72% → 70% (-2.8%)       │
│  - 用户流失率: 5.2% → 5.4% (+3.8%)     │
│  结论：业务影响可接受                    │
└────────────────────────────────────────┘

最终决策：综合判断可上线
关键：不能只看自动指标！
```

#### 实践3：分层级评估

```
评估优先级：
┌────────────────────────────────────┐
│ 第一优先级：业务核心指标           │
│ - 电商：GMV、转化率、用户留存        │
│ - 搜索：点击率、用户满意度           │
│ - 内容：阅读时长、分享率             │
└────────────────────────────────────┘
         ↓ 如果通过
┌────────────────────────────────────┐
│ 第二优先级：用户体验指标           │
│ - 准确率、F1-Score                   │
│ - 响应延迟、成功率                   │
│ - 人工评分                          │
└────────────────────────────────────┘
         ↓ 如果通过
┌────────────────────────────────────┐
│ 第三优先级：技术指标               │
│ - Perplexity、BLEU、ROUGE           │
│ - 模型大小、FLOPs                   │
│ - 吞吐量、资源占用                   │
└────────────────────────────────────┘
```

### 4. 实际案例分析

#### 案例1：BERT分类任务压缩

```
任务：情感分类（3分类：正面/中性/负面）
原始模型：BERT-base（110M参数）

压缩方案对比：
┌──────────────────────────────────────────────────┐
│ 方案A：INT8量化                                   │
│ - Accuracy: 89.2% → 88.3% (-0.9%) ✓              │
│ - Macro-F1: 0.876 → 0.865 (-1.3%) ✓              │
│ - Per-class F1:                                  │
│   正面: 0.91 → 0.90 (-1.1%)                      │
│   中性: 0.82 → 0.80 (-2.4%) ⚠️ 中性类下降明显    │
│   负面: 0.90 → 0.89 (-1.1%)                      │
│ - 推理速度: 2.3倍加速                             │
│ - 模型大小: 440MB → 110MB                        │
│ 评估：中性类下降稍多，需要关注                     │
├──────────────────────────────────────────────────┤
│ 方案B：知识蒸馏（DistilBERT）                     │
│ - Accuracy: 89.2% → 86.5% (-2.7%) ⚠️             │
│ - Macro-F1: 0.876 → 0.851 (-2.9%) ⚠️             │
│ - Per-class F1: 各类均衡下降 2-3%                 │
│ - 推理速度: 6.2倍加速 ✓✓                         │
│ - 模型大小: 440MB → 255MB                        │
│ 评估：性能下降更多但速度优势大                     │
└──────────────────────────────────────────────────┘

最终选择：方案A（INT8量化）
理由：性能损失小，加速比已足够
```

#### 案例2：GPT生成任务压缩

```
任务：对话生成
原始模型：GPT-7B

指标变化全景：
┌──────────────────────────────────────────────────┐
│ INT8量化（GPTQ）                                  │
├──────────────────────────────────────────────────┤
│ 自动指标：                                        │
│ - Perplexity: 16.8 → 18.7 (+11.3%)               │
│ - BLEU: 18.2 → 17.3 (-4.9%)                      │
│ - ROUGE-L: 32.1 → 30.8 (-4.0%)                   │
│ 看起来：下降明显 ⚠️                               │
├──────────────────────────────────────────────────┤
│ 人工评估（100个样本）：                            │
│ - 流畅性: 4.1/5 → 4.0/5 (-2.4%)                  │
│ - 连贯性: 3.9/5 → 3.8/5 (-2.6%)                  │
│ - 信息量: 3.8/5 → 3.7/5 (-2.6%)                  │
│ - 总体质量: 4.0/5 → 3.9/5 (-2.5%)                │
│ 实际上：用户几乎无感 ✓                            │
├──────────────────────────────────────────────────┤
│ A/B测试（10万用户）：                             │
│ - 用户满意度: 3.95 → 3.92 (-0.8%)                │
│ - 对话完成率: 78.2% → 77.5% (-0.9%)              │
│ - 平均对话轮次: 4.3 → 4.4 (+2.3%)                │
│ 业务指标：无显著影响 ✓                            │
└──────────────────────────────────────────────────┘

结论：
- 自动指标（PPL）与用户体验不完全一致
- 生成任务必须做人工评估和A/B测试
- 最终以业务指标为准
```

### 5. 关键要点总结

**指标选择原则**：
- 分类任务：Accuracy + F1 + AUC（必须看类别细分）
- 生成任务：Perplexity + BLEU/ROUGE + 人工评估（人工最重要）
- 排序任务：NDCG + MRR + 业务点击率（业务指标优先）

**评估误区**：
- ❌ 只看单一指标（如只看Accuracy）
- ❌ 只在标准数据集评估（必须用真实业务数据）
- ❌ 生成任务只看自动指标（必须人工评估）
- ❌ 忽视不同类别/长尾case的表现

**最佳实践**：
- ✓ 建立多维度评估体系
- ✓ 标准数据集 + 业务数据双重验证
- ✓ 自动指标 + 人工评估 + A/B测试三管齐下
- ✓ 持续监控，发现问题快速回滚


---

## 相关笔记
<!-- 自动生成 -->

暂无相关笔记

