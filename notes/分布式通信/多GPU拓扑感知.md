# 多GPU拓扑感知

## 拓扑结构类型

### 单机多GPU拓扑
- **全连接拓扑**: 每个GPU都有NVLink连接
- **环形拓扑**: GPU按环形排列连接
- **星形拓扑**: 通过NVSwitch中心连接
- **混合拓扑**: NVLink + PCIe混合连接

### 多机GPU拓扑
- **扁平拓扑**: 所有节点在同一网络层次
- **层次拓扑**: 机架内高速，机架间相对较慢
- **胖树拓扑**: 上层带宽逐级增加
- **Dragonfly拓扑**: 分组内全连接，组间部分连接

## 拓扑发现机制

### 自动拓扑检测
```python
import pynvml
import numpy as np

def detect_gpu_topology():
    """自动检测GPU拓扑结构"""
    pynvml.nvmlInit()
    device_count = pynvml.nvmlDeviceGetCount()
    
    # 创建连接矩阵
    topology_matrix = np.zeros((device_count, device_count))
    
    for i in range(device_count):
        handle_i = pynvml.nvmlDeviceGetHandleByIndex(i)
        for j in range(device_count):
            if i != j:
                handle_j = pynvml.nvmlDeviceGetHandleByIndex(j)
                try:
                    # 检测P2P连接
                    link_type = pynvml.nvmlDeviceGetTopologyCommonAncestor(
                        handle_i, handle_j)
                    if link_type == pynvml.NVML_TOPOLOGY_NVLINK:
                        topology_matrix[i][j] = 3  # NVLink
                    elif link_type == pynvml.NVML_TOPOLOGY_PCI:
                        topology_matrix[i][j] = 2  # PCIe
                    else:
                        topology_matrix[i][j] = 1  # CPU
                except:
                    topology_matrix[i][j] = 0  # 无连接
    
    return topology_matrix
```

### 带宽测量
```python
def measure_bandwidth_matrix():
    """测量GPU间实际带宽"""
    import torch
    device_count = torch.cuda.device_count()
    bandwidth_matrix = np.zeros((device_count, device_count))
    
    test_size = 256 * 1024 * 1024  # 256MB测试数据
    
    for i in range(device_count):
        for j in range(device_count):
            if i != j:
                # 创建测试数据
                data = torch.randn(test_size // 4, device=f'cuda:{i}')
                
                # 预热
                for _ in range(10):
                    data_copy = data.to(f'cuda:{j}')
                
                # 测量带宽
                torch.cuda.synchronize()
                start_time = time.time()
                
                for _ in range(100):
                    data_copy = data.to(f'cuda:{j}')
                    torch.cuda.synchronize()
                
                end_time = time.time()
                avg_time = (end_time - start_time) / 100
                bandwidth = test_size / avg_time / (1024**3)  # GB/s
                bandwidth_matrix[i][j] = bandwidth
    
    return bandwidth_matrix
```

## 拓扑感知算法选择

### 通信算法映射
```python
class TopologyAwareNCCL:
    def __init__(self, topology_matrix):
        self.topology = topology_matrix
        self.device_count = len(topology_matrix)
    
    def select_allreduce_algorithm(self, message_size):
        """根据拓扑选择All-Reduce算法"""
        if self.is_fully_connected():
            if message_size < 32 * 1024:  # 32KB
                return "TREE"  # 小消息用树算法
            else:
                return "RING"  # 大消息用环算法
        elif self.is_hierarchical():
            return "HIERARCHICAL"  # 层次化算法
        else:
            return "ADAPTIVE"  # 自适应算法
    
    def is_fully_connected(self):
        """检查是否为全连接拓扑"""
        nvlink_count = np.sum(self.topology == 3)
        expected_links = self.device_count * (self.device_count - 1)
        return nvlink_count == expected_links
    
    def is_hierarchical(self):
        """检查是否为层次化拓扑"""
        # 简化检查：是否存在不同类型的连接
        unique_types = np.unique(self.topology)
        return len(unique_types) > 2
```

### 通信路径优化
```python
def find_optimal_communication_path(topology, src, dst):
    """寻找最优通信路径"""
    import networkx as nx
    
    # 构建图
    G = nx.DiGraph()
    for i in range(len(topology)):
        for j in range(len(topology)):
            if topology[i][j] > 0:
                # 权重：连接类型的倒数（NVLink权重最小）
                weight = 4 - topology[i][j]
                G.add_edge(i, j, weight=weight)
    
    try:
        # 寻找最短路径
        path = nx.shortest_path(G, src, dst, weight='weight')
        return path
    except nx.NetworkXNoPath:
        return None
```

## 层次化通信优化

### 节点内优化
```python
class IntraNodeOptimizer:
    def __init__(self, local_topology):
        self.local_topology = local_topology
        self.local_ranks = self.get_local_ranks()
    
    def optimize_local_allreduce(self, data):
        """节点内All-Reduce优化"""
        if self.has_nvlink():
            # 使用NVLink的高带宽优势
            return self.nvlink_allreduce(data)
        else:
            # 使用PCIe的树状聚合
            return self.pcie_tree_allreduce(data)
    
    def has_nvlink(self):
        """检查是否有NVLink连接"""
        return np.any(self.local_topology == 3)
```

### 节点间协调
```python
class InterNodeCoordinator:
    def __init__(self, global_topology):
        self.global_topology = global_topology
        self.node_representatives = self.select_representatives()
    
    def select_representatives(self):
        """为每个节点选择代表GPU"""
        # 选择拓扑中心度最高的GPU作为代表
        representatives = []
        for node_gpus in self.get_node_groups():
            centrality = []
            for gpu in node_gpus:
                # 计算该GPU的连接度
                connections = np.sum(self.global_topology[gpu] > 0)
                centrality.append((gpu, connections))
            
            # 选择连接度最高的GPU
            rep_gpu = max(centrality, key=lambda x: x[1])[0]
            representatives.append(rep_gpu)
        
        return representatives
```

## 性能建模与预测

### 通信时间建模
```python
class CommunicationModel:
    def __init__(self, topology, bandwidth_matrix, latency_matrix):
        self.topology = topology
        self.bandwidth = bandwidth_matrix
        self.latency = latency_matrix
    
    def predict_allreduce_time(self, message_size, algorithm="RING"):
        """预测All-Reduce通信时间"""
        if algorithm == "RING":
            return self.model_ring_allreduce(message_size)
        elif algorithm == "TREE":
            return self.model_tree_allreduce(message_size)
        else:
            return self.model_adaptive_allreduce(message_size)
    
    def model_ring_allreduce(self, message_size):
        """Ring All-Reduce时间建模"""
        n = len(self.topology)
        
        # Ring中的最慢链路决定整体性能
        min_bandwidth = float('inf')
        max_latency = 0
        
        for i in range(n):
            next_gpu = (i + 1) % n
            min_bandwidth = min(min_bandwidth, 
                              self.bandwidth[i][next_gpu])
            max_latency = max(max_latency, 
                            self.latency[i][next_gpu])
        
        # 时间 = 延迟 + 传输时间
        chunk_size = message_size // n
        transfer_time = chunk_size * (n - 1) / min_bandwidth
        total_time = max_latency + transfer_time
        
        return total_time
```

## 动态拓扑适应

### 故障检测与重配置
```python
class DynamicTopologyManager:
    def __init__(self):
        self.current_topology = None
        self.backup_paths = {}
    
    def monitor_topology_health(self):
        """监控拓扑健康状态"""
        while True:
            try:
                new_topology = detect_gpu_topology()
                if not np.array_equal(new_topology, self.current_topology):
                    self.handle_topology_change(new_topology)
                    self.current_topology = new_topology
            except Exception as e:
                self.handle_detection_failure(e)
            
            time.sleep(1)  # 每秒检测一次
    
    def handle_topology_change(self, new_topology):
        """处理拓扑变化"""
        # 重新计算通信路径
        self.recalculate_paths(new_topology)
        
        # 通知NCCL重新初始化
        self.reinitialize_nccl(new_topology)
        
        # 更新性能模型
        self.update_performance_model(new_topology)
```

## 实际应用案例

### DGX系统拓扑优化
```
DGX-A100拓扑特点：
- 8个A100 GPU
- 每个GPU有12个NVLink 3.0连接
- 6个NVSwitch提供全连接
- 总聚合带宽：600GB/s

优化策略：
1. 利用NVSwitch的无阻塞特性
2. 平衡8个GPU的通信负载
3. 针对不同workload调整算法
```

### 多机训练拓扑优化
```
多机环境挑战：
- 节点间带宽相对较低（InfiniBand 200Gbps）
- 节点内带宽很高（NVLink 600GB/s）
- 延迟差异大（节点内<1μs，节点间~5μs）

解决方案：
1. 层次化All-Reduce
2. 节点内先聚合，节点间再通信
3. 重叠计算与通信
4. 梯度压缩减少跨节点传输
```
