---
created: '2025-10-19'
last_reviewed: null
next_review: '2025-10-19'
review_count: 0
difficulty: medium
mastery_level: 0.0
tags:
- 深度学习
- 深度学习/梯度消失爆炸中权重初始化策略的影响.md
related_outlines: []
---
# 梯度消失爆炸中权重初始化策略的影响


### 1. 为什么初始化会影响梯度？

* 在前向传播时，每层输出大致是 $z^{(l)} = W^{(l)} a^{(l-1)}$。
* 在反向传播时，梯度是各层导数的连乘，$\delta^{(l)} \sim (W^{(l+1)})^\top \delta^{(l+1)} \odot f'(z^{(l)})$。
* 如果权重初始化得过大或过小，就会让激活值或梯度在层数增加时呈指数式放大（爆炸）或缩小（消失）。

---

### 2. 常见初始化策略及作用

* **随机小数（不合理）**
  如果随便设得很小 → 前向传播时激活趋近 0，反向传播时梯度快速衰减 → **梯度消失**。
  如果设得太大 → 激活值过大进入饱和区，或梯度相乘放大 → **梯度爆炸**。

* **Xavier 初始化（Glorot Initialization）**
  针对 Sigmoid/Tanh 设计。设权重满足

  $$
  Var(W) = \frac{2}{n_{in}+n_{out}}
  $$

  保证前向信号和反向梯度在层间方差保持一致，减少消失/爆炸。

* **He 初始化**
  针对 ReLU 设计。设权重满足

  $$
  Var(W) = \frac{2}{n_{in}}
  $$

  因为 ReLU 一半输入会被截断为 0，需要更大的方差来保持激活分布的稳定。

---

### 3. 总结一句话（口语版）

权重初始化太小会让梯度在层间逐渐衰减导致消失，太大会让梯度逐层放大导致爆炸。像 Xavier 和 He 这样的初始化方法，通过控制权重方差，让前向信号和反向梯度在层间既不缩小也不放大，从而缓解梯度消失和爆炸问题。

---

## 相关笔记
<!-- 自动生成 -->

暂无相关笔记

