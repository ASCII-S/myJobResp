# 反向传播算法

## 面试重点问题

### 1. 为什么需要反向传播？
- **问题**：神经网络本质上是一个多层复合函数，直接计算梯度需要对每个参数单独求偏导，复杂度为 $O(参数个数 \times 前向传播复杂度)$
- **解决**：反向传播利用链式法则，一次前向+一次反向就能计算所有梯度
- **效率提升**：从 $O(W \times E)$ 降低到 $O(E)$

### 2. 链式法则的直观理解
- **物理意义**：每层的梯度=后一层传回的梯度×本层对输入的敏感性
- **信息传播**：误差信号从输出层逐层向前传播
- **梯度累积**：每个参数的梯度是所有影响路径的梯度之和

### 3. 梯度消失和梯度爆炸

#### 梯度消失
**原因**：
- 激活函数导数小（如sigmoid在饱和区）
- 权重过小
- 网络层数过深

**数学表现**：
$$\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial z^{(L)}} \prod_{l=2}^L W^{(l)} \sigma'(z^{(l-1)})$$

当 $|W^{(l)} \sigma'(z^{(l-1)})| < 1$ 时，连乘导致梯度指数衰减。

**解决方案**：
- 使用ReLU等激活函数
- 批归一化（Batch Normalization）
- 残差连接（ResNet）
- LSTM/GRU（RNN中）

#### 梯度爆炸
**原因**：权重过大或网络不稳定
**解决方案**：
- 梯度裁剪（Gradient Clipping）
- 权重正则化
- 合适的权重初始化

### 4. 不同激活函数的梯度特性

#### Sigmoid
- 前向：$\sigma(x) = \frac{1}{1+e^{-x}}$
- 导数：$\sigma'(x) = \sigma(x)(1-\sigma(x))$
- 问题：饱和区导数接近0，容易梯度消失

#### ReLU
- 前向：$\text{ReLU}(x) = \max(0, x)$
- 导数：$\text{ReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}$
- 优点：正区间梯度为1，缓解梯度消失

#### Tanh
- 前向：$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
- 导数：$\tanh'(x) = 1 - \tanh^2(x)$
- 特点：零中心化，但仍有饱和问题

### 5. 批量反向传播的实现细节

#### 向量化计算
```python
# 批量前向传播
Z = W @ A_prev + b  # (n_l, m)
A = activation(Z)   # (n_l, m)

# 批量反向传播  
dZ = dA * activation_derivative(Z)  # (n_l, m)
dW = (1/m) * dZ @ A_prev.T         # (n_l, n_l-1)
db = (1/m) * np.sum(dZ, axis=1, keepdims=True)  # (n_l, 1)
dA_prev = W.T @ dZ                 # (n_l-1, m)
```

#### 内存优化技巧
- **检查点技术**：只保存部分中间结果，需要时重新计算
- **梯度累积**：小批量多次累积等效大批量
- **混合精度训练**：使用FP16减少内存占用

### 6. 自动微分vs手动推导

#### 自动微分的优势
- **正确性**：避免手动推导错误
- **通用性**：适用于任意计算图
- **效率**：编译器优化

#### 两种模式
- **前向模式**：适合输入维度小的情况
- **反向模式**：适合输出维度小的情况（深度学习常用）

## 核心概念

### 什么是反向传播算法
反向传播（Backpropagation, BP）算法是训练神经网络的核心算法，用于计算损失函数相对于网络参数的梯度，从而实现参数的优化更新。

**核心思想**：
- 前向传播计算预测输出和损失
- 反向传播计算梯度，从输出层向输入层逐层传播
- 利用链式法则高效计算所有参数的梯度

## 数学原理

### 链式法则在神经网络中的应用

#### 1. 基本链式法则
对于复合函数 $f(g(x))$，有：
$$\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}$$

#### 2. 多层网络的链式法则
考虑三层网络：$x \rightarrow z_1 \rightarrow z_2 \rightarrow L$

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial z_1} \cdot \frac{\partial z_1}{\partial x}$$

#### 3. 矩阵形式的链式法则
对于权重矩阵 $W^{(l)}$：
$$\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}}$$

### 梯度计算的详细推导

#### 前向传播公式
- 线性变换：$z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$
- 激活函数：$a^{(l)} = \sigma(z^{(l)})$
- 损失函数：$L = \frac{1}{2}||y - a^{(L)}||^2$（以均方误差为例）

#### 反向传播递推公式

**1. 输出层梯度**
$$\delta^{(L)} = \frac{\partial L}{\partial z^{(L)}} = (a^{(L)} - y) \odot \sigma'(z^{(L)})$$

**2. 隐藏层梯度递推**
$$\delta^{(l)} = \frac{\partial L}{\partial z^{(l)}} = ((W^{(l+1)})^T \delta^{(l+1)}) \odot \sigma'(z^{(l)})$$

**3. 权重和偏置梯度**
$$\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T$$
$$\frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}$$

其中 $\odot$ 表示逐元素相乘（Hadamard积）

## 计算图与梯度流

### 计算图的构建

#### 前向计算图
```
输入 x → 线性层 → 激活函数 → 线性层 → 激活函数 → ... → 损失函数 L
```

#### 反向计算图
```
∂L/∂L=1 ← 线性层梯度 ← 激活函数梯度 ← 线性层梯度 ← 激活函数梯度 ← ...
```

### 梯度流动规律

**1. 线性层的梯度流**
- 前向：$y = Wx + b$
- 反向：$\frac{\partial L}{\partial x} = W^T \frac{\partial L}{\partial y}$
- 反向：$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y} x^T$

**2. 激活函数的梯度流**
- 前向：$y = \sigma(x)$
- 反向：$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \odot \sigma'(x)$

**3. 损失函数的梯度流**
- MSE：$\frac{\partial L}{\partial y} = y - t$
- 交叉熵：$\frac{\partial L}{\partial y} = y - t$（softmax+交叉熵）

## 算法实现步骤

### 标准反向传播算法流程

```python
def backpropagation(X, y, weights, biases):
    # 1. 前向传播
    activations = forward_pass(X, weights, biases)
    
    # 2. 计算输出层误差
    delta = compute_output_error(activations[-1], y)
    
    # 3. 反向传播误差
    deltas = [delta]
    for l in range(len(weights)-1, 0, -1):
        delta = weights[l].T @ delta * activation_derivative(activations[l-1])
        deltas.append(delta)
    
    # 4. 计算梯度
    gradients_w = []
    gradients_b = []
    for l in range(len(weights)):
        grad_w = deltas[-(l+1)] @ activations[l].T
        grad_b = deltas[-(l+1)]
        gradients_w.append(grad_w)
        gradients_b.append(grad_b)
    
    return gradients_w, gradients_b
```

### 批量处理的反向传播

对于批量大小为 $m$ 的数据：
$$\frac{\partial L}{\partial W^{(l)}} = \frac{1}{m} \sum_{i=1}^m \delta^{(l)(i)} (a^{(l-1)(i)})^T$$

## 时间复杂度分析

### 计算复杂度

**前向传播**：
- 每层计算：$O(n_{l-1} \times n_l)$
- 总复杂度：$O(\sum_{l=1}^L n_{l-1} \times n_l)$

**反向传播**：
- 梯度计算的复杂度与前向传播相同
- 总复杂度：$O(\sum_{l=1}^L n_{l-1} \times n_l)$

**关键结论**：反向传播的时间复杂度与前向传播相同，都是 $O(E)$，其中 $E$ 是网络边数。

### 空间复杂度
- 需要存储所有中间激活值：$O(\sum_{l=1}^L n_l)$
- 需要存储所有梯度：$O(\sum_{l=1}^L n_{l-1} \times n_l)$

## 代码实现示例

### 简单的两层网络反向传播

```python
import numpy as np

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
    
    def forward(self, X):
        # 前向传播
        self.z1 = X @ self.W1 + self.b1          # (m, hidden_size)
        self.a1 = np.maximum(0, self.z1)         # ReLU
        self.z2 = self.a1 @ self.W2 + self.b2    # (m, output_size)
        return self.z2
    
    def backward(self, X, y, output):
        m = X.shape[0]
        
        # 输出层梯度
        dz2 = output - y                         # (m, output_size)
        dW2 = (1/m) * self.a1.T @ dz2           # (hidden_size, output_size)
        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)
        
        # 隐藏层梯度
        da1 = dz2 @ self.W2.T                   # (m, hidden_size)
        dz1 = da1 * (self.z1 > 0)              # ReLU导数
        dW1 = (1/m) * X.T @ dz1                 # (input_size, hidden_size)
        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)
        
        return {
            'dW1': dW1, 'db1': db1,
            'dW2': dW2, 'db2': db2
        }
```

## 调试技巧

### 1. 梯度检查（Gradient Check）
```python
def gradient_check(f, x, analytic_grad, h=1e-5):
    numeric_grad = np.zeros_like(x)
    it = np.nditer(x, flags=['multi_index'])
    while not it.finished:
        ix = it.multi_index
        
        # f(x+h)
        x[ix] += h
        fxh_pos = f(x)
        
        # f(x-h)  
        x[ix] -= 2*h
        fxh_neg = f(x)
        
        # 数值梯度
        numeric_grad[ix] = (fxh_pos - fxh_neg) / (2*h)
        x[ix] += h  # 恢复
        it.iternext()
    
    # 比较解析梯度和数值梯度
    rel_error = np.abs(analytic_grad - numeric_grad) / np.maximum(1e-8, 
                    np.abs(analytic_grad) + np.abs(numeric_grad))
    return rel_error
```

### 2. 常见调试问题
- **梯度为0**：检查激活函数死区、权重初始化
- **梯度过大**：检查学习率、权重初始化、数据归一化
- **损失不下降**：检查梯度方向、学习率大小
- **过拟合**：添加正则化、Dropout

## 面试常考编程题

### 1. 手写反向传播算法
重点考察：
- 梯度计算公式的正确性
- 矩阵维度的正确处理
- 批量处理的实现

### 2. 实现自定义层
```python
class LinearLayer:
    def __init__(self, in_features, out_features):
        self.weight = np.random.randn(in_features, out_features) * 0.01
        self.bias = np.zeros(out_features)
        
    def forward(self, x):
        self.input = x  # 保存用于反向传播
        return x @ self.weight + self.bias
        
    def backward(self, grad_output):
        grad_input = grad_output @ self.weight.T
        self.grad_weight = self.input.T @ grad_output
        self.grad_bias = np.sum(grad_output, axis=0)
        return grad_input
```

### 3. 计算图实现
考察对计算图和自动微分的理解

## 总结

反向传播算法是深度学习的核心，面试中需要掌握：

1. **数学原理**：链式法则、梯度计算公式
2. **算法流程**：前向传播→计算损失→反向传播→更新参数  
3. **实现细节**：批量处理、向量化计算、内存优化
4. **问题解决**：梯度消失/爆炸的原因和解决方案
5. **调试技能**：梯度检查、常见问题诊断

**关键要点**：
- 理解为什么反向传播比直接求导高效
- 掌握不同激活函数对梯度传播的影响
- 能够手写简单网络的反向传播代码
- 理解现代深度学习框架中自动微分的原理