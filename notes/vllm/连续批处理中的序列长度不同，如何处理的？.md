# 连续批处理中的序列长度不同，如何处理的？

## 面试标准答案（精简版）

vLLM处理不同序列长度分为两个阶段：**Prefill阶段**对输入长度不同的请求进行分批处理，优先调度长序列以提高GPU利用率；**Decode阶段**通过PagedAttention支持变长attention，无需padding到统一长度。同时采用**动态批次调整**，序列完成后立即移除并释放KV Cache，新请求可立即加入。这避免了传统padding带来的计算浪费和内存浪费。

---

## 详细解析

### 1. 问题背景

在实际推理场景中，批次内序列长度差异巨大：
- **输入长度**：可能从几个token到几千个token不等
- **输出长度**：取决于任务类型（问答、代码生成、摘要等）
- **总长度**：输入+输出，动态增长

**传统方法的问题**：
```
传统Padding方案：
Request A: [tokens: 10]  → pad到100 → 90%计算浪费
Request B: [tokens: 50]  → pad到100 → 50%计算浪费  
Request C: [tokens: 100] → pad到100 → 0%浪费
Request D: [tokens: 80]  → pad到100 → 20%浪费
```

### 2. vLLM的分阶段处理策略

#### 2.1 Prefill阶段（处理输入长度差异）

**核心策略：分离调度 + 智能分批**

```
输入序列：
- Req A: 10 tokens
- Req B: 500 tokens
- Req C: 2000 tokens
- Req D: 50 tokens

调度策略：
1. 长序列优先：先处理C (2000)
2. 相似长度分组：A(10) + D(50) 一起处理
3. 避免混合：不将2000和10的请求放同一批
```

**原因**：
- **计算效率**：长短序列混合会导致短序列等待长序列计算完成
- **内存分配**：长序列一次性分配足够的KV Cache页
- **GPU利用率**：相似长度的序列可以更好地利用CUDA核心

**实现机制**：
```python
# 简化的调度逻辑
def schedule_prefill(waiting_requests):
    # 按输入长度排序
    sorted_reqs = sorted(waiting_requests, 
                        key=lambda r: r.input_length, 
                        reverse=True)
    
    batches = []
    current_batch = []
    
    for req in sorted_reqs:
        # 检查是否可以加入当前批次
        if can_fit_in_batch(current_batch, req):
            current_batch.append(req)
        else:
            batches.append(current_batch)
            current_batch = [req]
    
    return batches
```

#### 2.2 Decode阶段（处理输出长度差异）

**核心技术：变长Attention + 动态批次**

```
Iteration视角：
Iter 1: [A:11, B:501, C:2001, D:51]  # 数字表示当前长度
Iter 2: [A:12, B:502, C:2002, D:52]
Iter 3: [A:13, B:503, C:2003, D:53]  # A完成
Iter 4: [B:504, C:2004, D:54, E:10]  # 移除A，加入E
```

**关键特性**：
- 每个序列独立追踪自己的长度
- Attention计算时无需padding
- 完成即移除，不等待其他序列

### 3. PagedAttention的核心作用

#### 3.1 变长序列支持

**传统Attention**（需要padding）：
```python
# 传统方式
batch_tokens = pad_sequences([seq1, seq2, seq3], max_len)
# Shape: [batch_size, max_len, hidden_dim]
attention_output = attention(batch_tokens)
```

**PagedAttention**（无需padding）：
```python
# vLLM方式
# 每个序列独立存储KV Cache
seq1_kv: [len1, num_heads, head_dim]  # 存储在页表中
seq2_kv: [len2, num_heads, head_dim]  # len1 ≠ len2 ≠ len3
seq3_kv: [len3, num_heads, head_dim]

# 通过页表索引计算attention，无浪费
for seq, kv_pages in zip(sequences, kv_page_tables):
    attention_output = paged_attention(seq, kv_pages)
```

#### 3.2 内存管理优势

**页表映射示例**：
```
Sequence A (length=10):  Pages [1, 2]      # 2页
Sequence B (length=500): Pages [3-62]      # 60页
Sequence C (length=50):  Pages [63-67]     # 5页

# A完成后，页1-2立即可分配给新请求
# 无需等待B或C完成
```

### 4. 具体处理机制

#### 4.1 输入长度不同

**分批策略**：
```
场景1：内存充足
→ 长序列单独批次，短序列合并批次

场景2：内存紧张  
→ 限制批次大小，长序列可能独占一个批次

场景3：混合负载
→ 动态调整，平衡延迟和吞吐量
```

**优化技术**：
- **Chunked Prefill**：将长输入分块处理，避免OOM
- **优先级调度**：可根据SLA要求调整顺序

#### 4.2 输出长度不同

**动态管理**：
```python
class SequenceGroup:
    def __init__(self):
        self.sequences = []  # 可能有多个候选序列（beam search）
        self.current_length = input_length
        self.max_length = max_tokens
    
    def is_finished(self):
        return (
            self.current_length >= self.max_length or
            all(seq.is_finished() for seq in self.sequences)
        )

# 每次迭代
for seq_group in running_batches:
    if seq_group.is_finished():
        # 释放KV Cache
        block_manager.free(seq_group)
        running_batches.remove(seq_group)
```

**完成条件**：
- 生成EOS token
- 达到max_tokens
- 满足自定义停止条件

### 5. 性能对比

| 方案           | 输入长度差异   | 输出长度差异   | 内存利用率 | 计算效率 |
| -------------- | -------------- | -------------- | ---------- | -------- |
| 传统Padding    | 需padding到max | 需padding到max | ~50-70%    | ~50-70%  |
| vLLM连续批处理 | 无需padding    | 无需padding    | ~90-95%    | ~85-95%  |

**实测效果**：
- **内存节省**：2-4倍（取决于长度分布）
- **吞吐量提升**：2-24倍
- **延迟降低**：特别是短序列请求

### 6. 实现细节

#### 6.1 调度器核心逻辑

```python
class Scheduler:
    def schedule(self):
        # 1. 处理运行中的序列
        self._update_running()
        
        # 2. 移除已完成的序列
        finished = [s for s in self.running if s.is_finished()]
        for seq in finished:
            self.running.remove(seq)
            self.block_manager.free(seq)
        
        # 3. 计算剩余资源
        num_free_blocks = self.block_manager.get_num_free_blocks()
        
        # 4. 从等待队列中选择新请求
        while self.waiting and self._can_append(num_free_blocks):
            seq = self.waiting.pop(0)
            self._allocate_and_append(seq)
        
        return self.running
    
    def _can_append(self, num_free_blocks):
        # 检查是否有足够内存
        # 考虑序列长度和KV Cache需求
        if not self.waiting:
            return False
        
        seq = self.waiting[0]
        required_blocks = self._get_required_blocks(seq)
        
        return num_free_blocks >= required_blocks
```

#### 6.2 注意力计算

```python
def paged_attention(
    query,           # [num_seqs, num_heads, head_dim]
    kv_page_tables,  # [num_seqs, max_num_pages] 页表
    block_size,      # 每页大小
    seq_lengths      # [num_seqs] 每个序列的实际长度
):
    # 每个序列的attention计算是独立的
    # 不需要统一长度
    for i in range(num_seqs):
        pages = kv_page_tables[i]
        length = seq_lengths[i]
        
        # 只计算实际长度，无padding
        kv = gather_from_pages(pages, length)
        output[i] = attention(query[i], kv)
    
    return output
```

### 7. 总结

vLLM通过以下机制优雅处理序列长度差异：

1. **Prefill阶段**：智能分批，长度相似的序列优先组batch
2. **Decode阶段**：PagedAttention支持变长，无需padding
3. **动态调度**：完成即移除，持续保持批次饱和
4. **内存管理**：页级别分配和回收，避免碎片和浪费

这套机制是vLLM高性能的基础，相比传统方法可节省2-4倍内存，提升2-24倍吞吐量。

