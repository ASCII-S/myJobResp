# 大语言模型推理优化面试大纲

## 主题：大语言模型推理优化

本大纲按照技术层次从上至下组织：模型层 → 算子层 → 系统层 → 硬件层

---

## 第一章：模型层优化

### 1.1 模型压缩技术

#### 1.1.1 模型剪枝（Pruning）

**非结构化剪枝**
- 非结构化剪枝的基本原理是什么？
- 如何判断哪些权重应该被剪除？
- 非结构化剪枝后的稀疏模型如何在硬件上高效执行？面临哪些挑战？
- 剪枝率与模型性能之间的权衡关系如何？如何确定最优剪枝率？

**结构化剪枝**
- 结构化剪枝相比非结构化剪枝有什么优势？常见的结构化剪枝粒度有哪些？
- 如何对Attention头、FFN层进行结构化剪枝？剪枝后需要重新训练吗？
- 结构化剪枝如何保证剪枝后的模型在推理时真正减少计算量？

**动态剪枝与稀疏激活**
- 动态剪枝在推理时如何工作？与静态剪枝有何区别？
- LLM中的稀疏激活（如MoE）如何减少推理计算量？
- Top-K路由机制在MoE模型中的作用是什么？如何影响推理性能？

#### 1.1.2 低秩分解（Low-Rank Decomposition）

**矩阵分解方法**
- SVD、Tucker分解在模型压缩中如何应用？
- LoRA（Low-Rank Adaptation）的原理是什么？如何用于推理优化？
- 低秩分解后模型的参数量和计算量分别减少了多少？

**分解粒度选择**
- 应该对模型的哪些层进行低秩分解？Q、K、V矩阵都需要分解吗？
- 分解秩（rank）如何选择？过低的秩会带来什么问题？
- 低秩分解与量化可以同时使用吗？如何协同优化？

**张量分解**
- CP分解和Tucker分解的区别是什么？各自适用于什么场景？
- 张量分解如何应用于Transformer的多头注意力机制？
- 分解后的张量在推理时如何重组？是否会增加额外开销？

### 1.2 模型量化

#### 1.2.1 量化基础

**量化数据类型**
- [FP32、FP16、BF16、INT8、INT4之间有什么区别？各自的优缺点是什么？](../notes/熟悉大语言模型推理优化-技术层次/FP32、FP16、BF16、INT8、INT4之间有什么区别？各自的优缺点是什么？.md)
- [为什么BF16在大模型训练和推理中被广泛使用？](../notes/熟悉大语言模型推理优化-技术层次/为什么BF16在大模型训练和推理中被广泛使用？.md)
- [INT8量化相比FP16能带来多大的性能提升？精度损失有多少？](../notes/熟悉大语言模型推理优化-技术层次/INT8量化相比FP16能带来多大的性能提升？精度损失有多少？.md)

**量化方式**
- [对称量化和非对称量化的区别是什么？各自适用什么场景？](../notes/熟悉大语言模型推理优化-技术层次/对称量化和非对称量化的区别是什么？各自适用什么场景？.md)
- [Per-tensor量化和Per-channel量化有何不同？哪种精度更高？](../notes/熟悉大语言模型推理优化-技术层次/Per-tensor量化和Per-channel量化有何不同？哪种精度更高？.md)
- [动态量化和静态量化的区别是什么？推理时应该选择哪种？](../notes/熟悉大语言模型推理优化-技术层次/动态量化和静态量化的区别是什么？推理时应该选择哪种？.md)

**量化粒度**
- [逐层量化、逐通道量化、分组量化（Group Quantization）的优缺点是什么？](../notes/熟悉大语言模型推理优化-技术层次/逐层量化、逐通道量化、分组量化（Group_Quantization）的优缺点是什么？.md)
- [如何确定量化的scale和zero-point参数？](../notes/熟悉大语言模型推理优化-技术层次/如何确定量化的scale和zero-point参数？.md)
- [混合精度量化如何工作？如何选择哪些层用低精度？](../notes/熟悉大语言模型推理优化-技术层次/混合精度量化如何工作？如何选择哪些层用低精度？.md)

#### 1.2.2 训练后量化（PTQ）

**PTQ基本流程**
- [训练后量化的基本步骤是什么？需要哪些校准数据？](../notes/熟悉大语言模型推理优化-技术层次/训练后量化的基本步骤是什么？需要哪些校准数据？.md)
- [校准数据集的大小和质量如何影响量化效果？](../notes/熟悉大语言模型推理优化-技术层次/校准数据集的大小和质量如何影响量化效果？.md)
- [MinMax、KL散度、百分位数等校准方法的区别是什么？](../notes/熟悉大语言模型推理优化-技术层次/MinMax、KL散度、百分位数等校准方法的区别是什么？.md)

**权重和激活量化**
- [仅量化权重（Weight-Only Quantization）和同时量化激活有什么区别？](../notes/熟悉大语言模型推理优化-技术层次/仅量化权重（Weight-Only%20Quantization）和同时量化激活有什么区别？.md)
- [为什么激活值的量化通常比权重量化更困难？](../notes/熟悉大语言模型推理优化-技术层次/为什么激活值的量化通常比权重量化更困难？.md)
- [Outlier特征如何影响量化效果？如何处理异常值？](../notes/熟悉大语言模型推理优化-技术层次/Outlier特征如何影响量化效果？如何处理异常值？.md)

**高级PTQ技术**
- [SmoothQuant如何解决激活值量化中的异常值问题？](../notes/熟悉大语言模型推理优化-技术层次/SmoothQuant如何解决激活值量化中的异常值问题？.md)
- [AWQ（Activation-aware Weight Quantization）的核心思想是什么？](../notes/熟悉大语言模型推理优化-技术层次/AWQ（Activation-aware%20Weight%20Quantization）的核心思想是什么？.md)
- [GPTQ、LLM.int8()等方法的原理和优缺点是什么？](../notes/熟悉大语言模型推理优化-技术层次/GPTQ、LLM.int8()等方法的原理和优缺点是什么？.md)

#### 1.2.3 量化感知训练（QAT）

**QAT原理**
- [量化感知训练与训练后量化的主要区别是什么？](../notes/熟悉大语言模型推理优化-技术层次/量化感知训练与训练后量化的主要区别是什么？.md)
- [QAT中的fake quantization是如何工作的？](../notes/熟悉大语言模型推理优化-技术层次/QAT中的fake_quantization是如何工作的？.md)
- [QAT能在多大程度上恢复量化导致的精度损失？](../notes/熟悉大语言模型推理优化-技术层次/QAT能在多大程度上恢复量化导致的精度损失？.md)

**QAT训练策略**
- [QAT需要从头训练还是可以在预训练模型上微调？](../notes/熟悉大语言模型推理优化-技术层次/QAT需要从头训练还是可以在预训练模型上微调？.md)
- [QAT的训练成本相比普通训练高多少？](../notes/熟悉大语言模型推理优化-技术层次/QAT的训练成本相比普通训练高多少？.md)
- [如何在QAT中处理BatchNorm等特殊层？](../notes/熟悉大语言模型推理优化-技术层次/如何在QAT中处理BatchNorm等特殊层？.md)

**极低比特量化**
- [1-bit、2-bit量化（如Binary/Ternary量化）的可行性如何？](../notes/熟悉大语言模型推理优化-技术层次/1-bit、2-bit量化（如Binary_Ternary量化）的可行性如何？.md)
- [极低比特量化对模型性能的影响有多大？适用于哪些场景？](../notes/熟悉大语言模型推理优化-技术层次/极低比特量化对模型性能的影响有多大？适用于哪些场景？.md)
- [BitNet等二值化网络在LLM中的应用前景如何？](../notes/熟悉大语言模型推理优化-技术层次/BitNet等二值化网络在LLM中的应用前景如何？.md)

### 1.3 知识蒸馏

#### 1.3.1 传统知识蒸馏

**师生模型框架**
- 知识蒸馏的基本原理是什么？教师模型和学生模型如何配合？
- 软标签（soft label）相比硬标签有什么优势？温度参数的作用是什么？
- 蒸馏损失函数如何设计？KL散度为什么常被使用？

**蒸馏目标选择**
- 可以蒸馏模型的哪些部分？输出logits、中间层特征、注意力权重？
- 白盒蒸馏和黑盒蒸馏的区别是什么？
- 如何在蒸馏中平衡学生模型的自主学习和对教师的模仿？

**蒸馏数据集**
- 蒸馏需要什么样的数据？是否需要标注数据？
- 数据增强在蒸馏中的作用是什么？
- 如何选择或生成高质量的蒸馏数据集？

#### 1.3.2 LLM特定蒸馏技术

**序列级蒸馏**
- 在自回归生成任务中如何进行蒸馏？
- Sequence-level KD和Token-level KD有什么区别？
- 如何处理教师模型和学生模型输出分布不匹配的问题？

**多任务蒸馏**
- 如何同时蒸馏多个任务的知识到一个学生模型？
- 任务间的知识迁移如何影响蒸馏效果？
- 多教师蒸馏（Multi-Teacher Distillation）如何工作？

**自蒸馏与在线蒸馏**
- 自蒸馏（Self-Distillation）的原理是什么？如何提升模型性能？
- 在线蒸馏如何在推理时动态调整？
- Speculative Decoding中的蒸馏思想是什么？

### 1.4 高效模型架构

#### 1.4.1 注意力机制优化

**线性注意力**
- 标准注意力的时间和空间复杂度是多少？为什么需要线性注意力？
- Linear Attention如何将O(n²)复杂度降低到O(n)？
- 线性注意力在长序列场景下的性能表现如何？有哪些局限性？

**稀疏注意力**
- 稀疏注意力的核心思想是什么？有哪些常见的稀疏模式？
- 局部窗口注意力、分块注意力、随机注意力各自的优缺点是什么？
- Longformer、BigBird等模型如何设计稀疏注意力模式？

**多查询注意力（MQA）与分组查询注意力（GQA）**
- MQA相比标准多头注意力（MHA）有什么改进？如何减少KV Cache？
- GQA如何在MQA和MHA之间取得平衡？
- MQA/GQA对推理速度和模型质量的影响分别是什么？

#### 1.4.2 混合专家模型（MoE）

**MoE基本架构**
- MoE模型的基本组成部分有哪些？路由器（Router）的作用是什么？
- 稀疏激活如何减少推理时的计算量？
- MoE模型的参数量和实际计算量之间的关系是什么？

**路由策略**
- Top-K路由的工作原理是什么？K值如何选择？
- 负载均衡（Load Balancing）在MoE中为什么重要？如何实现？
- 专家容量（Expert Capacity）如何影响MoE的性能？

**MoE推理优化**
- MoE模型在推理时的主要瓶颈是什么？
- 如何优化MoE的专家调度和数据传输？
- MoE模型的显存占用如何？如何进行显存优化？

#### 1.4.3 新型架构

**状态空间模型（SSM）**
- Mamba架构的核心创新是什么？相比Transformer有何优势？
- 选择性状态空间模型（Selective SSM）如何工作？
- SSM在长序列推理中的性能表现如何？

**混合架构**
- Transformer与SSM的混合架构有什么优势？
- 如何在一个模型中结合不同的架构组件？
- RWKV等新型架构的推理效率如何？

**架构搜索与优化**
- 如何为特定推理场景设计最优的模型架构？
- 神经架构搜索（NAS）在LLM中的应用有哪些？
- 架构优化与硬件适配如何协同？

### 1.5 解码策略优化

#### 1.5.1 投机解码（Speculative Decoding）

**投机解码原理**
- 投机解码的基本思想是什么？如何加速自回归生成？
- 小模型（draft model）和大模型（target model）如何协作？
- 投机解码能带来多大的加速比？什么情况下效果最好？

**验证与接受机制**
- 如何验证小模型生成的候选token？接受率如何计算？
- 接受率过低会导致什么问题？如何提高接受率？
- 验证过程是否会引入额外开销？如何优化？

**Draft模型选择**
- Draft模型应该具备什么特点？如何选择或训练？
- Draft模型的大小与加速效果的关系是什么？
- 可以使用非参数化方法（如n-gram）作为draft吗？

#### 1.5.2 并行解码

**Medusa/多头解码**
- Medusa如何实现并行生成多个token？
- 多个解码头如何训练？如何选择最优路径？
- 并行解码的加速效果如何？适用于什么场景？

**Lookahead Decoding**
- Lookahead Decoding的核心机制是什么？
- 如何通过提前计算未来token来加速？
- Lookahead与Speculative Decoding的区别和联系是什么？

**非自回归解码**
- 非自回归解码如何一次生成多个token？
- 非自回归解码的质量与自回归解码相比如何？
- CTC、Mask-Predict等方法在LLM中的应用前景如何？

#### 1.5.3 早停与缓存策略

**早停机制**
- 如何在保证质量的前提下提前终止生成？
- Beam Search中的早停策略有哪些？
- 动态调整生成长度如何影响推理效率？

**Prompt缓存**
- 如何缓存和复用常见的prompt的KV Cache？
- Prompt缓存在多轮对话中的作用是什么？
- 缓存失效和更新策略如何设计？

**输出复用**
- 如何识别和复用相似的生成结果？
- 检索增强生成（RAG）中的缓存策略是什么？
- 输出缓存对生成多样性的影响如何？

---

## 第二章：算子层优化

### 2.1 注意力机制优化

#### 2.1.1 FlashAttention

**FlashAttention原理**
- FlashAttention如何解决标准Attention的内存瓶颈？
- Tiling（分块）和Recomputation在FlashAttention中的作用是什么？
- FlashAttention相比标准实现能带来多大的加速和内存节省？

**IO感知优化**
- 什么是IO-aware算法？为什么GPU内存层次结构很重要？
- FlashAttention如何优化HBM与SRAM之间的数据传输？
- 分块大小如何影响性能？如何选择最优的tile size？

**FlashAttention-2/3改进**
- FlashAttention-2相比v1有哪些改进？
- 如何进一步减少非矩阵乘法运算（non-matmul FLOPs）？
- FlashAttention在不同硬件（A100/H100）上的性能表现如何？

#### 2.1.2 PagedAttention

**分页机制**
- PagedAttention的核心思想是什么？借鉴了操作系统的哪些概念？
- 如何将KV Cache组织为固定大小的页（page）？
- 分页机制如何减少内存碎片和浪费？

**动态内存管理**
- vLLM如何使用PagedAttention动态分配和回收内存？
- 页表（page table）在推理过程中如何维护？
- Copy-on-Write机制在Beam Search等场景中如何应用？

**性能优化**
- PagedAttention相比连续内存分配有什么优势？
- 页大小（page size）如何影响性能和内存利用率？
- PagedAttention与FlashAttention可以结合使用吗？

#### 2.1.3 其他注意力优化

**KV Cache压缩**
- H2O（Heavy Hitter Oracle）如何选择性保留重要的KV Cache？
- 如何量化或剪枝KV Cache以减少内存占用？
- KV Cache压缩对生成质量的影响有多大？

**Multi-Query/Grouped-Query Attention实现**
- MQA/GQA在算子层如何高效实现？
- 共享KV如何减少内存访问和计算？
- MQA/GQA的kernel实现与标准MHA有何不同？

**长序列注意力优化**
- Ring Attention如何处理超长序列？
- 如何在有限显存下支持百万级token的上下文？
- 长序列注意力的计算效率如何优化？

### 2.2 算子融合

#### 2.2.1 层级融合

**水平融合与垂直融合**
- 水平融合和垂直融合的区别是什么？各适用于什么场景？
- 如何将多个独立算子融合为一个kernel？
- 算子融合如何减少kernel launch开销和内存访问？

**Transformer层融合**
- 如何将LayerNorm、Linear、Activation融合为一个算子？
- QKV projection能否融合？融合后的性能提升如何？
- 残差连接在融合中如何处理？

**融合粒度选择**
- 细粒度融合和粗粒度融合的权衡是什么？
- 整个Transformer Block能融合成一个kernel吗？
- 融合程度如何影响寄存器和共享内存使用？

#### 2.2.2 Kernel融合技术

**元素级算子融合**
- Add、Mul、ReLU等元素级算子如何融合？
- 融合后的kernel如何避免中间结果的写回？
- GELU、SiLU等复杂激活函数的融合策略是什么？

**Reduce算子融合**
- Softmax如何与其他算子融合？
- LayerNorm/RMSNorm的融合实现要点是什么？
- 如何在融合中高效实现reduction操作？

**内存绑定算子优化**
- 什么是内存绑定（memory-bound）算子？如何识别？
- 融合如何提高内存绑定算子的性能？
- 如何平衡计算强度和内存访问？

#### 2.2.3 编译器自动融合

**计算图优化**
- 编译器如何自动识别可融合的算子模式？
- TVM、XLA等编译器的融合策略是什么？
- 自动融合相比手写融合kernel有什么优缺点？

**融合模式匹配**
- 如何定义和匹配常见的融合模式（pattern）？
- 模式匹配的搜索空间有多大？如何高效搜索？
- 如何处理复杂的控制流和数据依赖？

**代价模型**
- 如何评估融合后的性能收益？
- 代价模型需要考虑哪些因素（计算、内存、延迟）？
- 如何在编译时做出最优的融合决策？

### 2.3 矩阵运算优化

#### 2.3.1 GEMM优化

**GEMM基础**
- GEMM在LLM推理中占多大比例？为什么它如此重要？
- cuBLAS、cuDNN等库的GEMM实现原理是什么？
- 如何选择最优的GEMM算法（Winograd、Strassen等）？

**分块与流水线**
- GEMM的分块策略如何影响性能？
- 如何利用共享内存和寄存器进行分块？
- 双缓冲和流水线技术如何隐藏数据加载延迟？

**Tensor Core利用**
- Tensor Core是什么？它如何加速矩阵运算？
- 如何在GEMM中充分利用Tensor Core？
- 不同数据类型（FP16、INT8、FP8）在Tensor Core上的性能如何？

#### 2.3.2 稀疏矩阵运算

**稀疏格式**
- COO、CSR、CSC等稀疏矩阵格式的特点是什么？
- 2:4结构化稀疏是什么？为什么GPU硬件支持它？
- 如何选择适合LLM的稀疏格式？

**稀疏GEMM**
- 稀疏GEMM相比稠密GEMM的优势和挑战是什么？
- cuSPARSE等库如何实现高效的稀疏矩阵乘法？
- 稀疏度达到多少时稀疏GEMM才有优势？

**稀疏感知优化**
- 如何在推理时利用权重的稀疏性？
- 动态稀疏（激活稀疏）如何处理？
- 稀疏性与量化如何结合优化？

#### 2.3.3 低秩矩阵优化

**低秩GEMM**
- 如何高效计算分解后的低秩矩阵乘法？
- 低秩分解是否总能带来速度提升？
- 低秩矩阵的存储格式如何优化？

**在线分解**
- 是否可以在推理时动态分解矩阵？
- 在线分解的开销如何？何时值得使用？
- 如何缓存和复用分解结果？

**自适应秩选择**
- 如何根据硬件特性动态选择最优的秩？
- 不同层的最优秩是否相同？
- 如何在精度和性能之间自动平衡？

### 2.4 自定义算子

#### 2.4.1 CUDA编程基础

**CUDA内存层次**
- 全局内存、共享内存、寄存器、常量内存的特点和使用场景是什么？
- 如何优化全局内存访问模式以提高带宽利用率？
- 共享内存的bank conflict是什么？如何避免？

**线程组织与调度**
- Grid、Block、Thread的层次结构是什么？如何配置？
- Warp的概念是什么？Warp divergence如何影响性能？
- 如何选择合适的Block大小以最大化occupancy？

**同步与通信**
- __syncthreads()的作用是什么？何时需要同步？
- Warp内的通信（shuffle、vote）如何使用？
- 跨Block的同步有哪些方法？

#### 2.4.2 Triton编程

**Triton语言特性**
- Triton相比CUDA有什么优势？为什么更容易编写？
- Triton的编程模型是什么？如何表达并行计算？
- Triton的自动优化能力如何？生成的代码质量如何？

**内存管理与优化**
- Triton如何自动管理共享内存和寄存器？
- 如何在Triton中控制数据的加载和存储？
- Triton的性能是否接近手写CUDA？

**实际应用**
- 如何用Triton实现高效的GEMM kernel？
- Triton适合实现哪类算子？有哪些限制？
- FlashAttention的Triton实现与CUDA实现的性能对比如何？

#### 2.4.3 算子性能分析

**性能指标**
- 如何评估一个算子的性能？吞吐量、延迟、效率的定义是什么？
- 算术强度（Arithmetic Intensity）是什么？如何计算？
- Roofline模型如何用于分析算子性能瓶颈？

**Profiling工具**
- Nsight Compute、Nsight Systems的使用方法和区别是什么？
- 如何识别算子的性能瓶颈（计算绑定vs内存绑定）？
- 如何分析kernel的occupancy、IPC、内存吞吐等指标？

**优化策略**
- 针对计算绑定和内存绑定的算子，分别有哪些优化方法？
- 如何平衡并行度和内存使用？
- 如何进行迭代优化并验证性能提升？

---

## 第三章：系统层优化

### 3.1 内存管理

#### 3.1.1 KV Cache管理

**KV Cache基础**
- 什么是KV Cache？为什么它对LLM推理至关重要？
- KV Cache的大小如何计算？随序列长度如何增长？
- KV Cache占用显存的比例通常有多大？

**动态内存分配**
- 静态分配和动态分配KV Cache的优缺点是什么？
- 如何在生成过程中按需分配KV Cache？
- 预分配策略如何在性能和内存占用间平衡？

**KV Cache复用**
- Beam Search中如何共享KV Cache？
- 多个请求的公共前缀如何复用KV Cache？
- Copy-on-Write机制如何减少内存拷贝？

#### 3.1.2 显存优化

**显存分析**
- LLM推理的显存主要消耗在哪些地方？
- 如何估算一个模型在推理时需要的最小显存？
- Batch size与显存占用的关系是什么？

**显存复用技术**
- Activation Checkpointing在推理中是否适用？
- 如何复用中间激活值的内存？
- In-place操作如何减少显存开销？

**卸载（Offloading）**
- 模型参数或KV Cache能否卸载到CPU内存或磁盘？
- 卸载的开销有多大？何时值得使用？
- 如何设计高效的预取（prefetch）策略？

#### 3.1.3 内存池化

**内存池设计**
- 内存池的基本原理是什么？如何减少碎片？
- 如何为不同大小的内存请求设计分配策略？
- 内存池的回收和复用机制如何工作？

**伙伴系统**
- 伙伴系统（Buddy System）如何应用于GPU内存管理？
- 固定大小块分配和可变大小分配的权衡是什么？
- 内存对齐对性能的影响有多大？

**垃圾回收**
- 何时触发内存回收？如何避免频繁回收？
- LRU等替换策略在KV Cache管理中如何应用？
- 引用计数在内存管理中的作用是什么？

### 3.2 批处理策略

#### 3.2.1 静态批处理

**批处理基础**
- 批处理如何提高GPU利用率和吞吐量？
- Batch size如何影响延迟和吞吐量？
- 为什么LLM推理的最优batch size通常较小？

**Padding策略**
- 变长序列如何组batch？Padding的代价是什么？
- 如何减少Padding带来的计算浪费？
- Bucket策略如何对相似长度的序列分组？

**批处理限制**
- 静态批处理在实际服务中有什么局限性？
- 请求到达的随机性如何影响批处理效率？
- 不同请求的生成长度差异如何处理？

#### 3.2.2 动态批处理（Continuous Batching）

**Continuous Batching原理**
- Continuous Batching如何解决静态批处理的问题？
- 如何在生成过程中动态插入和移除请求？
- Continuous Batching能带来多大的吞吐量提升？

**请求调度**
- 如何决定何时接收新请求加入batch？
- 优先级调度在推理服务中如何实现？
- 如何平衡延迟和吞吐量？

**Iteration-level批处理**
- 每次迭代都可以改变batch组成吗？
- 动态batch如何管理KV Cache的内存分配？
- orca等调度算法的核心思想是什么？

#### 3.2.3 请求调度优化

**调度策略**
- FCFS、SJF、优先级调度等策略的优缺点是什么？
- 如何预测请求的生成长度以优化调度？
- 抢占式调度在LLM推理中可行吗？

**负载均衡**
- 多GPU推理时如何进行负载均衡？
- 如何处理长尾请求对系统性能的影响？
- 动态负载均衡的开销如何？

**QoS保证**
- 如何为不同用户提供服务质量保证？
- 延迟SLA如何与调度策略结合？
- 如何在保证QoS的同时最大化吞吐量？

### 3.3 推理框架

#### 3.3.1 vLLM

**vLLM架构**
- vLLM的核心组件有哪些？各自的作用是什么？
- PagedAttention在vLLM中如何集成？
- vLLM相比其他框架有什么独特优势？

**内存管理**
- vLLM的内存管理器如何工作？
- 如何动态分配和回收KV Cache？
- vLLM如何支持超长上下文的推理？

**性能优化**
- vLLM在吞吐量和延迟方面的表现如何？
- vLLM支持哪些优化技术（量化、张量并行等）？
- 如何配置vLLM以达到最佳性能？

#### 3.3.2 TensorRT-LLM

**TensorRT优化**
- TensorRT的核心优化技术有哪些？
- TensorRT-LLM如何针对LLM进行优化？
- 编译优化在TensorRT中的作用是什么？

**Plugin机制**
- TensorRT的Plugin系统如何工作？
- 如何为不支持的算子编写自定义Plugin？
- 常用的LLM Plugin有哪些（如In-flight Batching）？

**部署与性能**
- TensorRT-LLM的部署流程是什么？
- 相比其他框架，TensorRT-LLM的性能优势在哪里？
- TensorRT-LLM支持哪些模型和硬件平台？

#### 3.3.3 其他推理框架

**Text Generation Inference (TGI)**
- HuggingFace的TGI有什么特点？
- TGI的Continuous Batching实现如何？
- TGI与vLLM的性能对比如何？

**DeepSpeed-Inference**
- DeepSpeed-Inference的优化策略是什么？
- DeepSpeed的ZeRO-Inference技术如何工作？
- DeepSpeed在超大模型推理上有什么优势？

**框架选择**
- 如何根据场景选择合适的推理框架？
- 不同框架在易用性、性能、功能上的权衡是什么？
- 框架的生态和社区支持如何影响选择？

### 3.4 编译优化

#### 3.4.1 计算图优化

**图级变换**
- 常量折叠、死代码消除等优化如何工作？
- 如何识别和消除冗余计算？
- 计算图的重写规则如何定义？

**图融合**
- 如何在计算图层面进行算子融合？
- 融合候选的搜索空间有多大？如何枚举？
- 图融合与kernel融合的区别是什么？

**布局优化**
- NCHW与NHWC等数据布局对性能的影响是什么？
- 如何自动选择最优的数据布局？
- 布局转换的开销如何？何时值得转换？

#### 3.4.2 算子选择与调优

**算子库**
- 一个算子可能有多少种实现（cuBLAS、cuDNN、自定义）？
- 如何为给定的硬件和输入选择最优实现？
- 算子选择的代价模型如何构建？

**自动调优（Auto-tuning）**
- 什么是自动调优？为什么需要它？
- 如何在部署前进行离线调优？
- 在线自适应调优的可行性如何？

**TVM与AutoScheduler**
- TVM如何生成高效的算子实现？
- AutoScheduler的搜索策略是什么？
- 机器学习如何用于加速调优过程？

#### 3.4.3 编译框架

**XLA**
- XLA（Accelerated Linear Algebra）的优化流程是什么？
- XLA如何支持多种硬件后端？
- XLA在TensorFlow和JAX中的应用如何？

**MLIR**
- MLIR的多层IR设计有什么好处？
- 如何在MLIR中表达和优化机器学习模型？
- MLIR生态中有哪些方言（Dialect）用于LLM？

**Torch.compile**
- PyTorch 2.0的编译技术原理是什么？
- TorchDynamo、TorchInductor的作用分别是什么？
- torch.compile对LLM推理的加速效果如何？

### 3.5 分布式推理

#### 3.5.1 张量并行（Tensor Parallelism）

**张量并行原理**
- 张量并行如何将单个层的权重分布到多个设备？
- 列并行和行并行的区别是什么？
- 张量并行中的通信模式是什么（All-Reduce、All-Gather）？

**Megatron-LM实现**
- Megatron-LM如何实现Transformer的张量并行？
- QKV矩阵和FFN层分别如何切分？
- 张量并行的通信开销如何？能扩展到多少GPU？

**优化技术**
- 如何减少张量并行中的通信开销？
- 通信与计算如何重叠以隐藏延迟？
- 序列并行如何与张量并行结合？

#### 3.5.2 流水线并行（Pipeline Parallelism）

**流水线划分**
- 流水线并行如何将模型的不同层分配到不同设备？
- 如何划分流水线stage以平衡负载？
- 流水线深度如何影响性能？

**气泡（Bubble）问题**
- 流水线并行中的气泡是什么？为什么会降低效率？
- GPipe、PipeDream等方法如何减少气泡？
- Micro-batch如何提高流水线利用率？

**推理特定优化**
- 推理时的流水线并行与训练有何不同？
- 自回归生成如何影响流水线效率？
- 如何在流水线中处理动态batch？

#### 3.5.3 数据并行（Data Parallelism）

**数据并行基础**
- 数据并行如何在推理中应用？与训练的区别是什么？
- 多个模型副本如何分配请求？
- 数据并行的通信开销相比模型并行如何？

**负载均衡**
- 数据并行中如何实现负载均衡？
- 请求路由策略有哪些（轮询、最少连接、性能感知）？
- 如何处理异构硬件上的数据并行？

**副本一致性**
- 推理时是否需要保证副本一致性？
- 模型更新时如何同步所有副本？
- 灰度发布和A/B测试如何实现？

#### 3.5.4 序列并行（Sequence Parallelism）

**序列切分**
- 序列并行如何沿序列维度切分？
- 序列并行适用于哪些算子（LayerNorm、Dropout）？
- 序列并行如何与张量并行结合使用？

**通信模式**
- 序列并行需要哪些集合通信操作？
- All-Gather和Reduce-Scatter在序列并行中的作用是什么？
- 序列并行的通信开销如何？

**长序列优化**
- 超长序列推理如何受益于序列并行？
- Ring Attention中的序列并行如何实现？
- 序列并行对KV Cache管理的影响是什么？

#### 3.5.5 专家并行（Expert Parallelism）

**MoE并行策略**
- MoE模型中的专家如何分布到多个设备？
- All-to-All通信在专家并行中的作用是什么？
- 专家并行与其他并行策略如何组合？

**负载均衡**
- 专家之间的负载不均衡如何处理？
- 动态路由如何影响专家并行的性能？
- 专家容量限制如何设置？

**通信优化**
- 专家并行的通信瓶颈在哪里？
- 如何减少All-to-All的通信开销？
- 本地专家和远程专家的访问模式如何优化？

#### 3.5.6 混合并行策略

**3D并行**
- 3D并行（数据+张量+流水线）如何协同工作？
- 如何为给定的模型和硬件选择最优的并行组合？
- 不同并行维度的通信如何隔离？

**ZeRO优化**
- ZeRO的三个阶段分别优化什么？
- ZeRO-Infinity如何支持超大模型推理？
- ZeRO与其他并行策略的关系是什么？

**自动并行**
- 如何自动搜索最优的并行策略？
- Alpa等自动并行系统的原理是什么？
- 代价模型如何预测不同并行策略的性能？

---

## 第四章：硬件层优化

### 4.1 GPU架构与优化

#### 4.1.1 GPU架构基础

**计算单元**
- CUDA Core和Tensor Core的区别是什么？
- SM（Streaming Multiprocessor）的架构是什么？
- 不同GPU架构（Volta、Ampere、Hopper）的主要改进是什么？

**内存层次**
- GPU的内存层次结构是什么？各层的大小和延迟如何？
- L1/L2 Cache、共享内存、HBM的访问速度差异有多大？
- 内存带宽如何影响LLM推理性能？

**执行模型**
- GPU的SIMT执行模型是什么？
- Warp调度和延迟隐藏如何工作？
- Occupancy对性能的影响是什么？如何优化？

#### 4.1.2 Tensor Core优化

**Tensor Core架构**
- Tensor Core支持哪些矩阵大小和数据类型？
- Tensor Core相比CUDA Core能提供多大的性能提升？
- 如何在代码中显式使用Tensor Core（WMMA、MMA）？

**混合精度计算**
- Tensor Core的FP16累加到FP32是如何工作的？
- TF32格式是什么？它的优势是什么？
- FP8（E4M3、E5M2）在H100上如何使用？

**Tensor Core利用率**
- 如何确保矩阵大小对齐以充分利用Tensor Core？
- 为什么某些矩阵形状在Tensor Core上性能不佳？
- 如何profile Tensor Core的利用率？

#### 4.1.3 显存带宽优化

**带宽分析**
- 如何计算一个kernel的理论带宽需求？
- 实际带宽利用率通常能达到理论峰值的多少？
- 如何识别带宽瓶颈？

**访问模式优化**
- 合并访问（Coalesced Access）是什么？如何优化？
- 非对齐访问和跨步访问的性能代价是什么？
- 如何组织数据布局以提高访问效率？

**数据复用**
- 如何最大化数据复用以减少带宽需求？
- Blocking和Tiling策略如何提高数据局部性？
- 共享内存如何用于提高复用率？

#### 4.1.4 计算-访存比优化

**Roofline模型**
- Roofline模型如何分析性能上限？
- 算术强度如何决定性能瓶颈类型？
- 如何使用Roofline指导优化？

**提高计算强度**
- 算子融合如何提高计算访存比？
- 如何通过算法改进减少内存访问？
- 重计算与缓存的权衡是什么？

**平衡优化**
- LLM推理是计算绑定还是内存绑定？
- Prefill和Decode阶段的瓶颈分别是什么？
- 如何针对不同阶段进行针对性优化？

### 4.2 异构计算

#### 4.2.1 CPU-GPU协同

**任务划分**
- 哪些任务适合在CPU上执行？哪些适合GPU？
- CPU如何辅助GPU进行预处理和后处理？
- 动态调度如何在CPU和GPU间分配任务？

**数据传输**
- CPU-GPU数据传输的开销有多大？
- 如何使用Pinned Memory和Stream加速传输？
- 零拷贝（Zero-Copy）技术如何应用？

**流水线设计**
- 如何设计CPU-GPU流水线以隐藏传输延迟？
- 异步执行和多流如何提高利用率？
- Unified Memory在LLM推理中的应用如何？

#### 4.2.2 专用AI加速器

**TPU**
- Google TPU的架构特点是什么？
- TPU与GPU在LLM推理上的性能对比如何？
- TPU的脉动阵列（Systolic Array）如何工作？

**NPU/神经网络处理器**
- 华为昇腾、寒武纪等NPU的特点是什么？
- NPU在LLM推理中的优势和局限是什么？
- 如何将模型部署到NPU上？

**移动端AI芯片**
- 高通、联发科等移动AI芯片的能力如何？
- 边缘设备上如何高效运行LLM？
- 量化和压缩对移动端推理的重要性如何？

#### 4.2.3 FPGA与ASIC

**FPGA加速**
- FPGA在LLM推理中的应用场景是什么？
- FPGA的可编程性和性能如何权衡？
- 如何将LLM模型映射到FPGA上？

**ASIC设计**
- 专用推理芯片（如AWS Inferentia）的优势是什么？
- ASIC的开发周期和成本如何？
- 通用性和专用性如何平衡？

**硬件-软件协同设计**
- 如何为特定模型定制硬件？
- 算法与硬件的协同设计流程是什么？
- 软硬件协同能带来多大的性能提升？

### 4.3 硬件感知优化

#### 4.3.1 硬件特性利用

**指令集优化**
- 如何利用GPU的特殊指令（如fused multiply-add）？
- 向量化指令在CPU上如何加速计算？
- 硬件支持的原子操作如何使用？

**缓存优化**
- 如何优化数据访问以提高缓存命中率？
- 缓存亲和性（Cache Affinity）如何影响性能？
- 如何利用硬件预取器？

**并行度匹配**
- 如何选择线程数以匹配硬件并行度？
- 超线程（Hyper-threading）在推理中的效果如何？
- 如何避免资源竞争和同步开销？

#### 4.3.2 带宽与延迟优化

**带宽优化**
- 如何充分利用PCIe、NVLink等互连带宽？
- 多GPU间的通信带宽如何？
- 如何减少跨设备的数据传输？

**延迟隐藏**
- 指令级并行和流水线如何隐藏延迟？
- 异步执行和双缓冲如何应用？
- 预取策略如何设计？

**网络优化**
- InfiniBand、RoCE等高速网络的特点是什么？
- RDMA如何减少通信开销？
- 集合通信库（NCCL、Gloo）如何优化？

#### 4.3.3 能效优化

**功耗分析**
- LLM推理的主要功耗来源是什么？
- 计算和内存访问的能耗比是多少？
- 如何profile和分析能耗？

**能效优化策略**
- DVFS（动态电压频率调节）如何应用？
- 量化和剪枝如何降低能耗？
- 批处理大小如何影响能效？

**绿色AI**
- 如何在保证性能的前提下降低碳排放？
- 模型大小与能耗的关系是什么？
- 边缘推理如何优化能效？

### 4.4 专用推理加速硬件

#### 4.4.1 推理芯片架构

**架构特点**
- 推理专用芯片与训练芯片的主要区别是什么？
- 如何针对推理特点（固定模型、低精度）优化架构？
- 片上内存如何设计以减少外部访存？

**数据流架构**
- 数据流架构相比冯诺依曼架构有什么优势？
- 如何设计数据流以最大化数据复用？
- 静态调度和动态调度的权衡是什么？

**稀疏加速**
- 硬件如何原生支持稀疏计算？
- 稀疏张量的存储和计算单元如何设计？
- 稀疏加速的实际收益如何？

#### 4.4.2 商用推理芯片

**NVIDIA GPU（A100/H100）**
- A100和H100在推理性能上的对比如何？
- H100的Transformer Engine有什么特殊优化？
- 如何充分利用H100的FP8能力？

**AWS Inferentia/Trainium**
- Inferentia的架构和性能特点是什么？
- 如何将模型编译到Inferentia上运行？
- 成本和性能相比GPU如何？

**Google TPU**
- TPU v4/v5的推理能力如何？
- TPU在大规模部署中的优势是什么？
- JAX/XLA如何优化TPU推理？

#### 4.4.3 性能评估与选型

**Benchmark**
- 如何公平地对比不同硬件的推理性能？
- MLPerf等Benchmark的指标有哪些？
- 实际业务场景与Benchmark的差异如何？

**性价比分析**
- 如何综合考虑性能、成本、功耗选择硬件？
- TCO（总体拥有成本）包括哪些因素？
- 云端和本地部署的硬件选择有何不同？

**未来趋势**
- LLM推理硬件的发展趋势是什么？
- 存算一体、光计算等新技术的前景如何？
- 硬件专业化和通用性如何平衡？

---

## 附录：综合性问题

### A. 端到端优化

**如何为一个具体的LLM设计完整的推理优化方案？**
**模型层、算子层、系统层、硬件层的优化如何协同？**
**给定延迟和吞吐量目标，如何系统性地进行优化？**

### B. 实际部署案例

**如何部署一个百亿参数模型的在线推理服务？**
**如何在单卡、多卡、多机上分别进行优化？**
**如何监控和调试推理服务的性能问题？**

### C. 前沿技术

**当前LLM推理优化的研究前沿有哪些？**
**哪些优化技术最有潜力在未来大规模应用？**
**如何跟踪和学习最新的优化方法？**

---

**说明**：本大纲系统覆盖了大语言模型推理优化的四个技术层次，每个知识点下设置了2-4个具体问题点，既包含基础概念，也涵盖深入原理和实践应用。面试官可以根据候选人的背景和职位要求，灵活选择不同层次和难度的问题进行考核。
