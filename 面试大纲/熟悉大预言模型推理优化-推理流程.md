# 大语言模型推理优化面试大纲

> 主题：大语言模型推理优化（按推理流程）
> 侧重：原理与算法
> 结构：7个章节，覆盖从输入到输出的完整推理流程及系统优化

---

## 第1章：推理流程基础

### 1.1 推理基本概念

#### 1.1.1 推理与训练的区别
**计算模式差异**
- 推理与训练在计算图上有什么本质区别？
- 为什么推理不需要反向传播？这对内存占用有什么影响？
- 推理过程中哪些参数是固定的？哪些是动态的？

**资源需求差异**
- 推理和训练在显存占用上有什么不同？
- 为什么训练通常需要更大的batch size？
- 推理为什么可以使用混合精度甚至更低精度？

**优化目标差异**
- 训练的优化目标是什么？推理的优化目标是什么？
- 为什么训练侧重吞吐量，而推理需要兼顾延迟和吞吐量？
- Batch size对训练和推理的影响有何不同？

#### 1.1.2 推理的特点与挑战
**延迟敏感性**
- 什么是首token延迟（TTFT）？为什么它对用户体验至关重要？
- 在线推理服务的延迟要求通常是多少？
- 如何在保证延迟的同时提高吞吐量？

**资源利用率挑战**
- 为什么LLM推理的GPU利用率往往较低？
- 什么是计算密集型操作？什么是访存密集型操作？
- 如何提高推理过程中的GPU利用率？

**动态性与不确定性**
- 为什么推理的输出长度是不确定的？这带来什么挑战？
- 如何处理不同长度的输入和输出序列？
- 动态batch调度为什么重要？

#### 1.1.3 推理场景分类
**在线推理 vs 离线推理**
- 在线推理和离线推理的核心区别是什么？
- 在线推理对延迟的要求有多严格？
- 离线推理如何最大化吞吐量？

**单请求 vs 批处理**
- 单请求推理的性能瓶颈在哪里？
- 批处理如何提高硬件利用率？
- 批处理会增加延迟吗？如何权衡？

**边缘设备 vs 云端推理**
- 边缘设备推理面临哪些特殊挑战？
- 云端推理如何利用大规模并行？
- 两种场景下的优化策略有何不同？

### 1.2 完整推理流程概述

#### 1.2.1 端到端推理流程
**流程阶段划分**
- 一个完整的LLM推理请求包含哪些阶段？
- 从用户输入到最终输出，数据经过哪些转换？
- 每个阶段的输入和输出是什么？

**数据流与控制流**
- 推理过程中的数据如何在CPU和GPU之间传递？
- 哪些操作在CPU上执行？哪些在GPU上执行？
- 如何减少CPU-GPU数据传输开销？

**关键路径分析**
- 推理流程中的关键路径是什么？
- 哪个阶段通常是性能瓶颈？
- 如何识别和优化关键路径？

#### 1.2.2 输入处理阶段
**文本预处理**
- 输入文本需要经过哪些预处理步骤？
- Tokenization在整个流程中的位置和作用是什么？
- 输入处理阶段的性能开销大吗？

**输入编码**
- Token如何转换为模型可处理的数值表示？
- Position encoding在哪个阶段添加？
- Attention mask如何生成？

**输入阶段优化空间**
- 输入处理阶段有哪些优化机会？
- 批处理输入时如何处理不同长度？
- 预处理可以并行化吗？

#### 1.2.3 模型前向传播
**Transformer层迭代**
- Transformer模型的前向传播包含哪些层？
- 每一层的计算复杂度是多少？
- 各层之间如何传递数据？

**注意力计算**
- Self-attention的计算流程是什么？
- 注意力计算的时间复杂度和空间复杂度是多少？
- Multi-head attention如何并行化？

**前馈网络（FFN）**
- FFN的结构和作用是什么？
- FFN的计算特点是什么？
- FFN层的参数量和计算量占比多少？

#### 1.2.4 输出生成阶段
**自回归生成**
- 什么是自回归生成？
- 为什么需要逐个token生成？
- 自回归生成的串行性如何影响性能？

**采样与解码**
- 如何从logits生成下一个token？
- 常见的采样策略有哪些？
- 解码策略如何影响生成质量和性能？

**终止条件**
- 生成何时停止？
- 最大长度和EOS token的作用是什么？
- 如何处理生成过长的情况？

#### 1.2.5 各阶段的性能特征
**计算密集度分析**
- 哪些阶段是计算密集型的？
- 哪些阶段是访存密集型的？
- 如何量化计算密集度？

**内存访问模式**
- 不同阶段的内存访问模式有何不同？
- 哪些阶段对带宽要求高？
- 哪些阶段对延迟要求高？

**并行性分析**
- 哪些阶段容易并行化？
- 哪些阶段存在串行依赖？
- 如何提高各阶段的并行度？

### 1.3 Prefill vs Decode两阶段

#### 1.3.1 Prefill阶段（首token生成）
**阶段定义**
- Prefill阶段的输入和输出是什么？
- Prefill处理的是什么数据？
- Prefill在整个流程中的位置和作用是什么？

**计算特征**
- Prefill阶段的计算量如何计算？
- 为什么Prefill阶段可以高度并行化？
- Prefill的计算模式类似于什么？

**性能特点**
- Prefill阶段的主要性能瓶颈是什么？
- 如何衡量Prefill的性能？
- Prefill的延迟主要由什么决定？

#### 1.3.2 Decode阶段（自回归生成）
**阶段定义**
- Decode阶段的输入和输出是什么？
- Decode需要执行多少次？
- Decode的循环结构是怎样的？

**计算特征**
- Decode阶段每次迭代的计算量如何？
- 为什么Decode阶段的并行度有限？
- Decode的计算模式有什么特点？

**性能特点**
- Decode阶段的主要性能瓶颈是什么？
- 为什么Decode通常比Prefill慢得多？
- Decode的延迟如何影响用户体验？

#### 1.3.3 两阶段的计算模式差异
**批处理维度**
- Prefill如何在序列长度维度并行？
- Decode如何在batch维度并行？
- 两者的并行粒度有什么不同？

**矩阵运算特征**
- Prefill的矩阵乘法是什么形状？
- Decode的矩阵乘法是什么形状？
- 矩阵形状如何影响计算效率？

**硬件利用率**
- 为什么Prefill的GPU利用率通常更高？
- 为什么Decode难以充分利用GPU？
- 如何提高Decode阶段的硬件利用率？

#### 1.3.4 计算密集型 vs 访存密集型
**计算密集型特征（Prefill）**
- 什么是计算密集型操作？
- 如何判断一个操作是计算密集型的？
- 计算密集型操作的优化重点是什么？

**访存密集型特征（Decode）**
- 什么是访存密集型操作？
- 如何判断一个操作是访存密集型的？
- 访存密集型操作的优化重点是什么？

**Roofline模型分析**
- 什么是Roofline模型？
- 如何用Roofline分析Prefill和Decode？
- 算术强度如何影响性能优化策略？

#### 1.3.5 KV Cache的作用
**KV Cache基本概念**
- 什么是KV Cache？
- 为什么需要缓存Key和Value？
- KV Cache存储什么内容？

**避免重复计算**
- 没有KV Cache时每次Decode需要计算什么？
- KV Cache如何避免重复计算？
- 使用KV Cache可以节省多少计算量？

**空间换时间权衡**
- KV Cache需要多少显存？
- KV Cache的显存占用如何随序列长度变化？
- KV Cache是否总是值得的？

### 1.4 性能指标体系

#### 1.4.1 延迟指标
**TTFT（Time To First Token）**
- TTFT的定义是什么？
- TTFT主要由哪个阶段决定？
- TTFT对用户体验的影响是什么？

**TPOT（Time Per Output Token）**
- TPOT的定义是什么？
- TPOT主要由哪个阶段决定？
- TPOT如何影响生成速度？

**E2E Latency（端到端延迟）**
- E2E Latency包含哪些部分？
- 如何计算E2E Latency？
- E2E Latency = TTFT + TPOT × 输出长度，为什么？

**延迟的分解分析**
- 如何将延迟分解到各个组件？
- 哪些环节是可优化的？
- 如何识别延迟瓶颈？

#### 1.4.2 吞吐量指标
**Throughput（吞吐量）**
- 吞吐量的定义和单位是什么？
- 如何测量推理吞吐量？
- 吞吐量与什么因素相关？

**QPS（Queries Per Second）**
- QPS与吞吐量的区别是什么？
- QPS如何受到请求长度影响？
- 如何提高系统的QPS？

**Token Throughput**
- 每秒生成的token数如何计算？
- Token throughput与硬件性能的关系是什么？
- 如何提高token throughput？

#### 1.4.3 资源利用率
**GPU利用率**
- 如何定义和测量GPU利用率？
- 推理时GPU利用率通常是多少？
- 为什么推理的GPU利用率往往较低？

**显存占用**
- 推理时显存主要用于什么？
- 如何计算推理所需的显存？
- 显存占用如何随batch size和序列长度变化？

**模型FLOPs利用率（MFU）**
- 什么是MFU？
- 如何计算MFU？
- 推理的MFU与训练的MFU有何不同？

#### 1.4.4 性能权衡
**Latency vs Throughput**
- 延迟和吞吐量为什么存在权衡？
- 增大batch size如何影响延迟和吞吐量？
- 如何根据场景选择优化目标？

**质量 vs 性能**
- 生成质量和推理性能如何权衡？
- Beam search为什么慢但质量高？
- 采样策略如何影响性能？

**成本 vs 性能**
- 如何在硬件成本和性能间权衡？
- 模型压缩对成本和性能的影响是什么？
- TCO（总拥有成本）如何计算？

### 1.5 优化方向概览

#### 1.5.1 计算优化方向
**算子优化**
- 哪些算子是推理中的性能热点？
- 算子融合的基本思想是什么？
- Kernel优化的主要方法有哪些？

**数值精度优化**
- 为什么可以降低推理精度？
- FP16、INT8、INT4各有什么特点？
- 量化如何加速计算？

**稀疏性利用**
- 什么是模型稀疏性？
- 如何利用稀疏性加速推理？
- 稀疏矩阵运算的挑战是什么？

#### 1.5.2 访存优化方向
**减少内存访问**
- 为什么减少内存访问很重要？
- 算子融合如何减少内存访问？
- In-place操作的优势是什么？

**提高访存效率**
- 什么是内存合并访问？
- Cache locality如何影响性能？
- 如何优化内存访问模式？

**内存复用**
- 哪些中间结果可以复用？
- KV Cache是如何复用内存的？
- 内存池的作用是什么？

#### 1.5.3 调度优化方向
**批处理优化**
- Static batching的局限是什么？
- Continuous batching如何工作？
- 如何动态调整batch size？

**请求调度**
- 如何决定请求的执行顺序？
- Prefill和Decode可以分离调度吗？
- 优先级调度的策略是什么？

**资源调度**
- 如何在多请求间分配GPU资源？
- 抢占式调度的优缺点是什么？
- 如何避免资源碎片化？

#### 1.5.4 系统优化方向
**并行化**
- 有哪些维度可以并行？
- 张量并行、流水线并行、数据并行的区别是什么？
- 如何组合多种并行策略？

**分布式推理**
- 为什么需要分布式推理？
- 分布式推理的通信开销如何？
- 如何减少分布式推理的通信？

**硬件加速**
- GPU、TPU、专用AI芯片有何不同？
- 如何充分利用硬件特性？
- 硬件感知优化是什么？

#### 1.5.5 与后续章节的对应关系
**章节映射**
- 第2章（输入处理优化）对应哪些优化方向？
- 第3章（Prefill优化）主要关注什么？
- 第4章（Decode优化）的重点在哪里？
- 第5章（内存与调度）涉及哪些技术？
- 第6章（输出处理）包含什么内容？
- 第7章（系统级并行）覆盖什么范围？

**优化优先级**
- 对于Prefill阶段，哪些优化最有效？
- 对于Decode阶段，哪些优化最关键？
- 如何系统性地优化整个推理流程？

---

## 第2章：输入处理优化

### 2.1 Tokenization优化

#### 2.1.1 分词算法原理
**BPE（Byte Pair Encoding）**
- BPE算法的基本原理是什么？
- BPE如何构建词表？
- BPE的合并规则是什么？
- BPE的优缺点是什么？

**WordPiece**
- WordPiece与BPE有什么区别？
- WordPiece如何选择合并对？
- WordPiece为什么被BERT采用？

**SentencePiece**
- SentencePiece的特点是什么？
- SentencePiece如何处理空格？
- SentencePiece为什么适合多语言？

**分词算法性能对比**
- 不同分词算法的速度如何？
- 词表大小如何影响分词性能？
- 如何选择合适的分词算法？

#### 2.1.2 词表压缩与优化
**词表大小的影响**
- 词表大小如何影响模型性能？
- 词表大小如何影响推理速度？
- 词表大小如何影响显存占用？

**词表剪枝**
- 什么是词表剪枝？
- 如何识别不常用的token？
- 词表剪枝对模型质量的影响是什么？

**多语言词表优化**
- 多语言模型的词表有什么特点？
- 如何优化多语言词表的大小？
- 语言特定词表的优势是什么？

#### 2.1.3 动态padding策略
**Padding的必要性**
- 为什么batch推理需要padding？
- Padding如何影响计算效率？
- Padding的显存开销是多少？

**动态padding vs 固定padding**
- 动态padding的原理是什么？
- 动态padding如何减少浪费？
- 动态padding的实现复杂度如何？

**Bucket策略**
- 什么是bucket策略？
- 如何选择bucket的大小？
- Bucket策略的优缺点是什么？

### 2.2 Embedding层优化

#### 2.2.1 Embedding查找优化
**Embedding查找过程**
- Embedding查找的计算复杂度是多少？
- Embedding查找是计算密集还是访存密集？
- 如何优化Embedding查找的性能？

**Embedding表存储**
- Embedding表如何存储在显存中？
- 大词表的Embedding表有多大？
- 如何减少Embedding表的显存占用？

**查找并行化**
- Embedding查找如何并行化？
- 并行查找的访存模式是什么？
- 如何优化并行查找的带宽利用？

#### 2.2.2 位置编码优化
**绝对位置编码**
- 绝对位置编码的原理是什么？
- 绝对位置编码的计算开销如何？
- 绝对位置编码的长度限制是什么？

**相对位置编码（RoPE）**
- RoPE的基本原理是什么？
- RoPE如何避免长度限制？
- RoPE的计算开销相比绝对位置编码如何？
- RoPE可以预计算和缓存吗？

**ALiBi（Attention with Linear Biases）**
- ALiBi的核心思想是什么？
- ALiBi如何处理位置信息？
- ALiBi的计算开销是多少？
- ALiBi相比其他位置编码的优势是什么？

**位置编码的缓存策略**
- 哪些位置编码可以预计算？
- 如何缓存位置编码以提高性能？
- 缓存位置编码的显存开销如何？

#### 2.2.3 Embedding量化
**Embedding量化的必要性**
- Embedding层的参数量占比多少？
- Embedding量化能节省多少显存？
- Embedding量化对精度的影响大吗？

**量化方法**
- Embedding可以量化到什么精度？
- INT8量化Embedding的效果如何？
- 如何实现Embedding的量化查找？

**混合精度Embedding**
- 什么是混合精度Embedding？
- 如何选择哪些Embedding用低精度？
- 混合精度的实现复杂度如何？

---

## 第3章：Prefill阶段优化

### 3.1 Prefill阶段特性

#### 3.1.1 计算密集型特征
**计算量分析**
- Prefill阶段的总计算量如何计算？
- 计算量与输入长度的关系是什么？
- 计算量与模型参数的关系是什么？

**FLOPs计算**
- 如何计算Prefill的FLOPs？
- Attention和FFN各占多少FLOPs？
- 不同模型大小的FLOPs差异如何？

**计算密集度量化**
- 如何量化Prefill的计算密集度？
- Prefill的算术强度是多少？
- 为什么Prefill适合GPU加速？

#### 3.1.2 与Decode阶段的差异
**并行度差异**
- Prefill可以在哪些维度并行？
- Decode的并行度为什么受限？
- 并行度差异如何影响硬件利用率？

**矩阵形状差异**
- Prefill的GEMM是什么形状（M×N×K）？
- Decode的GEMM是什么形状？
- 矩阵形状如何影响计算效率？

**优化策略差异**
- Prefill的优化重点是什么？
- Decode的优化重点是什么？
- 两阶段能否用同一套优化？

### 3.2 全量注意力计算优化

#### 3.2.1 注意力计算复杂度分析
**标准Attention复杂度**
- 标准Self-Attention的时间复杂度是多少？
- 空间复杂度（显存需求）是多少？
- 复杂度的瓶颈在哪里？

**三步计算分解**
- QK^T计算的复杂度是多少？
- Softmax计算的复杂度是多少？
- Attention × V计算的复杂度是多少？

**长序列的挑战**
- 序列长度翻倍时，计算量和显存如何变化？
- 为什么长序列的Attention难以优化？
- 128K、1M token的上下文有多大挑战？

#### 3.2.2 Flash Attention原理
**I/O感知设计**
- Flash Attention解决什么问题？
- 什么是HBM（高带宽内存）和SRAM？
- Flash Attention如何减少HBM访问？

**Tiling策略**
- Flash Attention的分块策略是什么？
- 如何选择block大小？
- Tiling如何平衡计算和访存？

**Online Softmax**
- 什么是online softmax？
- 如何在不完整数据上计算softmax？
- Online softmax的数学原理是什么？

**Flash Attention性能提升**
- Flash Attention相比标准实现快多少？
- 显存节省有多少？
- Flash Attention的适用场景是什么？

**Flash Attention 2/3改进**
- Flash Attention 2的主要改进是什么？
- 并行度如何进一步提升？
- Flash Attention 3的新特性是什么？

#### 3.2.3 算子融合技术
**融合的必要性**
- 为什么需要算子融合？
- 未融合时的内存开销是多少？
- 算子融合如何减少内存访问？

**Attention融合模式**
- QKV计算可以融合吗？
- Softmax可以与其他操作融合吗？
- LayerNorm如何融合到Attention中？

**融合粒度权衡**
- 融合粒度越大越好吗？
- 过度融合的问题是什么？
- 如何选择最优融合策略？

### 3.3 矩阵运算优化

#### 3.3.1 GEMM优化
**GEMM在Transformer中的地位**
- GEMM占Transformer计算量的比例是多少？
- 哪些层使用GEMM？
- GEMM的性能如何影响整体性能？

**GEMM优化技术**
- Tiling/Blocking策略是什么？
- 如何优化GEMM的Cache利用？
- CUTLASS和cuBLAS的区别是什么？

**批量GEMM（Batched GEMM）**
- 什么是Batched GEMM？
- Multi-head attention如何使用Batched GEMM？
- Batched GEMM的性能特点是什么？

**低精度GEMM**
- INT8 GEMM相比FP16快多少？
- Tensor Core如何加速GEMM？
- 混合精度GEMM的实现方式是什么？

#### 3.3.2 FFN层优化
**FFN结构**
- FFN层的结构是什么？
- FFN的参数量和计算量占比多少？
- FFN为什么需要特别优化？

**激活函数融合**
- GELU/SiLU的计算开销如何？
- 激活函数如何融合到GEMM中？
- 融合后的性能提升是多少？

**专家混合（MoE）FFN**
- MoE FFN的结构是什么？
- 稀疏激活如何影响性能？
- MoE的路由开销如何优化？

#### 3.3.3 激活函数优化
**常见激活函数**
- ReLU、GELU、SiLU的计算复杂度如何？
- 不同激活函数对硬件的要求是什么？
- 激活函数的近似计算可行吗？

**激活函数的量化**
- 激活值可以量化吗？
- 动态量化vs静态量化的区别是什么？
- 激活量化的精度损失如何？

**激活函数的内存优化**
- 激活值需要保存吗（推理时）？
- 哪些激活值可以in-place计算？
- 激活值的显存占用如何？

### 3.4 KV Cache初始化

#### 3.4.1 KV Cache生成机制
**KV Cache的内容**
- 每一层的KV Cache包含什么？
- KV Cache的维度是多少？
- KV Cache如何在Prefill阶段生成？

**多头的KV Cache**
- Multi-head attention的KV Cache如何组织？
- KV Cache是分头存储还是合并存储？
- 存储方式如何影响访问效率？

**多层的KV Cache**
- 各层的KV Cache是独立的吗？
- 总的KV Cache大小如何计算？
- 层间KV Cache可以共享吗？

#### 3.4.2 预分配策略
**动态分配 vs 预分配**
- 动态分配KV Cache的问题是什么？
- 预分配的优势是什么？
- 如何确定预分配的大小？

**内存池管理**
- 什么是KV Cache内存池？
- 内存池如何减少分配开销？
- 内存池的碎片化问题如何解决？

**分页管理（PagedAttention预览）**
- 为什么需要分页管理KV Cache？
- 分页的基本单位是什么？
- 分页管理的优势是什么？

---

## 第4章：Decode阶段优化

### 4.1 Decode阶段特性

#### 4.1.1 访存密集型特征
**访存与计算比**
- Decode阶段的访存量与计算量比例如何？
- 为什么Decode是访存密集型的？
- 如何量化访存密集度？

**内存带宽瓶颈**
- Decode受限于什么带宽？
- HBM带宽对Decode性能的影响有多大？
- 如何测量带宽利用率？

**Roofline分析**
- Decode在Roofline模型中的位置是什么？
- Decode是否达到带宽上限？
- 如何将Decode从访存瓶颈移向计算瓶颈？

#### 4.1.2 自回归生成机制
**自回归依赖**
- 什么是自回归依赖？
- 为什么不能并行生成多个token？
- 自回归如何限制性能？

**串行性分析**
- Decode的串行深度是多少？
- 串行性如何影响延迟？
- 有方法打破串行性吗？

**生成长度的影响**
- 生成长度如何影响总延迟？
- 长文本生成的性能挑战是什么？
- 如何优化长序列生成？

#### 4.1.3 性能瓶颈分析
**瓶颈识别**
- 如何识别Decode的性能瓶颈？
- KV Cache访问是主要瓶颈吗？
- 模型参数加载的开销如何？

**Batch size的影响**
- Batch size如何影响Decode性能？
- 为什么增大batch可以提高吞吐？
- Batch size的上限是什么？

**硬件利用率**
- Decode的GPU利用率通常多低？
- 为什么Decode难以充分利用GPU？
- 如何提高Decode的硬件利用率？

### 4.2 增量解码原理

#### 4.2.1 增量注意力计算
**增量计算原理**
- 什么是增量注意力计算？
- 每次Decode需要计算什么？
- 增量计算如何避免重复？

**Query的生成**
- 当前步的Query如何计算？
- Query的维度是多少？
- Query计算的开销大吗？

**与历史Key/Value的交互**
- 如何使用缓存的KV计算attention？
- Attention score的计算复杂度是多少？
- Softmax的范围是多少？

#### 4.2.2 KV Cache复用机制
**KV Cache的更新**
- 每次Decode如何更新KV Cache？
- 新的KV如何追加到Cache中？
- Cache更新的开销是多少？

**Cache的读取模式**
- Decode时如何读取KV Cache？
- 读取的访存模式是什么？
- 如何优化Cache读取？

**Cache大小增长**
- KV Cache如何随生成长度增长？
- Cache增长如何影响性能？
- Cache大小的上限是什么？

#### 4.2.3 计算量分析
**单步Decode的FLOPs**
- 一次Decode迭代的FLOPs是多少？
- Attention和FFN各占多少？
- 与Prefill的计算量如何对比？

**计算量与序列长度的关系**
- Decode计算量如何随上下文长度变化？
- 长上下文对Decode的影响是什么？
- 如何降低长上下文的计算开销？

**计算效率分析**
- Decode的FLOPS（每秒浮点运算）是多少？
- 相比理论峰值，实际效率如何？
- 为什么Decode的计算效率低？

### 4.3 访存优化

#### 4.3.1 访存带宽优化
**带宽需求分析**
- Decode需要多少内存带宽？
- 哪些访存可以优化？
- 带宽需求如何随batch size变化？

**减少数据传输**
- 如何减少GPU与HBM间的数据传输？
- 算子融合如何帮助减少传输？
- 哪些数据可以保留在片上？

**提高带宽利用率**
- 如何提高内存带宽利用率？
- 内存合并访问的作用是什么？
- 预取（Prefetching）如何提高效率？

#### 4.3.2 Cache locality优化
**空间局部性**
- 什么是空间局部性？
- KV Cache的访问有空间局部性吗？
- 如何利用空间局部性？

**时间局部性**
- 什么是时间局部性？
- 模型参数的访问有时间局部性吗？
- 如何优化时间局部性？

**片上Cache利用**
- GPU的L1/L2 Cache如何利用？
- 数据如何保留在Cache中？
- Cache miss如何影响性能？

#### 4.3.3 内存合并访问
**合并访问原理**
- 什么是内存合并访问？
- 为什么合并访问更高效？
- 未合并访问的性能损失是多少？

**KV Cache访问模式**
- KV Cache的访问是合并的吗？
- 如何重组KV Cache以提高合并度？
- Batch维度的合并如何实现？

**数据布局优化**
- 数据布局如何影响访问模式？
- Row-major vs Column-major的影响是什么？
- 如何选择最优数据布局？

---

## 第5章：内存与调度优化

### 5.1 KV Cache管理

#### 5.1.1 Paged Attention原理
**传统KV Cache管理的问题**
- 连续内存分配有什么问题？
- 为什么会有内存碎片？
- 预留最大长度为什么浪费？

**分页思想**
- Paged Attention借鉴了什么思想？
- 虚拟内存分页的核心概念是什么？
- 如何将分页应用到KV Cache？

**Page的组织**
- Page的大小如何选择？
- Page如何映射到物理内存？
- Page table的作用是什么？

**Paged Attention的优势**
- Paged Attention如何减少内存浪费？
- 内存利用率提升多少？
- Paged Attention的开销是什么？

**实现细节**
- 如何实现分页的Attention计算？
- 跨Page的Attention如何处理？
- Page的分配和回收策略是什么？

#### 5.1.2 KV Cache压缩技术
**KV Cache压缩的必要性**
- KV Cache占用多少显存？
- 压缩能节省多少显存？
- 压缩对性能的影响是什么？

**基于重要性的压缩**
- 如何评估KV的重要性？
- H2O（Heavy-Hitter Oracle）的原理是什么？
- 如何选择保留哪些KV？

**基于相似性的压缩**
- 相邻token的KV有相似性吗？
- 如何利用相似性压缩？
- 聚类压缩的方法是什么？

**滑动窗口策略**
- 什么是滑动窗口KV Cache？
- 只保留最近的KV可行吗？
- 如何结合窗口和重要性？

#### 5.1.3 KV Cache量化
**量化的收益**
- KV Cache量化能节省多少显存？
- INT8/INT4 KV Cache的效果如何？
- 量化对生成质量的影响大吗？

**量化方法**
- Per-tensor vs Per-channel量化的区别是什么？
- 动态量化如何实现？
- 如何确定量化的scale和zero-point？

**反量化开销**
- Attention计算时如何反量化？
- 反量化的计算开销是多少？
- 混合精度KV Cache可行吗？

#### 5.1.4 多层KV Cache共享
**层间共享的可能性**
- 不同层的KV Cache可以共享吗？
- 哪些层的KV相似？
- 共享的理论依据是什么？

**Cross-layer Attention**
- 什么是跨层Attention？
- 如何实现层间KV共享？
- 共享对模型质量的影响如何？

**MQA/GQA的KV共享**
- Multi-Query Attention是什么？
- Grouped-Query Attention是什么？
- MQA/GQA如何减少KV Cache？

### 5.2 显存管理

#### 5.2.1 显存分配策略
**静态分配 vs 动态分配**
- 静态分配的优缺点是什么？
- 动态分配的开销有多大？
- 如何选择分配策略？

**预分配池化**
- 内存池的工作原理是什么？
- 内存池如何减少分配开销？
- 内存池的大小如何确定？

**按需分配**
- 按需分配的灵活性如何？
- 频繁分配的性能影响是什么？
- 如何平衡灵活性和性能？

#### 5.2.2 碎片化管理
**内存碎片的产生**
- 为什么会产生内存碎片？
- 外部碎片和内部碎片的区别是什么？
- 碎片化如何影响显存利用率？

**碎片整理**
- 显存可以整理碎片吗？
- 碎片整理的开销是什么？
- 何时需要碎片整理？

**避免碎片化**
- 如何从源头避免碎片？
- Buddy system的作用是什么？
- Slab分配器适合GPU吗？

#### 5.2.3 动态显存调整
**显存压力监控**
- 如何监控显存使用情况？
- 显存不足的预警机制是什么？
- OOM（Out of Memory）如何预防？

**动态Batch调整**
- 如何根据显存动态调整batch size？
- 显存紧张时的降级策略是什么？
- 动态调整对吞吐的影响如何？

**显存与性能的权衡**
- 如何在显存和性能间权衡？
- 显存换时间的策略有哪些？
- 时间换显存的方法有哪些？

### 5.3 Batch调度策略

#### 5.3.1 Static Batching vs Continuous Batching
**Static Batching**
- Static Batching的工作原理是什么？
- 所有请求同时开始和结束有什么问题？
- Static Batching的GPU利用率如何？

**Continuous Batching**
- Continuous Batching如何工作？
- 请求如何动态加入和离开batch？
- Continuous Batching的优势是什么？

**性能对比**
- Continuous Batching比Static快多少？
- 吞吐量提升有多大？
- 延迟的变化如何？

#### 5.3.2 动态批处理算法
**请求到达处理**
- 新请求如何加入正在执行的batch？
- 加入时机如何选择？
- 加入新请求的开销是多少？

**请求完成处理**
- 请求完成后如何从batch移除？
- 移除如何影响batch的执行？
- Batch大小变化如何处理？

**Batch重组**
- 何时需要重组batch？
- 重组的策略是什么？
- 重组的开销如何？

#### 5.3.3 请求级调度优化
**FCFS vs 优先级调度**
- FCFS（先来先服务）的问题是什么？
- 如何设计优先级调度？
- 优先级如何确定？

**SJF（Shortest Job First）**
- SJF在LLM推理中适用吗？
- 如何预测请求的执行时间？
- SJF的公平性问题如何解决？

**公平性与效率权衡**
- 如何平衡公平性和效率？
- 饥饿（Starvation）如何避免？
- 时间片轮转适合LLM吗？

### 5.4 多请求并发

#### 5.4.1 Prefill-Decode分离调度
**分离的必要性**
- 为什么要分离Prefill和Decode？
- Prefill和Decode混合执行有什么问题？
- 分离调度的优势是什么？

**Prefill优先 vs Decode优先**
- Prefill优先的策略是什么？
- Decode优先对延迟的影响如何？
- 如何平衡两种优先级？

**Chunked Prefill**
- 什么是Chunked Prefill？
- 如何将Prefill分块执行？
- Chunked Prefill如何改善调度？

#### 5.4.2 优先级调度
**优先级维度**
- 用户级优先级如何设置？
- SLA（服务等级协议）如何影响优先级？
- 请求类型如何影响优先级？

**动态优先级调整**
- 优先级可以动态调整吗？
- 等待时间如何影响优先级？
- 如何防止优先级反转？

**优先级实现**
- 优先级队列如何实现？
- 多级反馈队列适用吗？
- 优先级调度的开销如何？

#### 5.4.3 抢占与恢复机制
**抢占的必要性**
- 为什么需要抢占？
- 什么情况下应该抢占？
- 抢占的代价是什么？

**KV Cache的保存与恢复**
- 被抢占请求的KV Cache如何处理？
- KV Cache可以换出到CPU内存吗？
- 恢复时的开销有多大？

**抢占策略**
- 如何选择被抢占的请求？
- 抢占的粒度是什么（请求级？token级？）？
- 如何避免频繁抢占？

---

## 第6章：输出处理优化

### 6.1 采样策略

#### 6.1.1 Greedy Sampling
**基本原理**
- Greedy Sampling如何工作？
- 每次选择概率最大的token有什么问题？
- Greedy Sampling的确定性如何？

**性能特点**
- Greedy Sampling的计算开销是多少？
- 为什么Greedy是最快的采样方法？
- Greedy对生成质量的影响是什么？

**适用场景**
- Greedy Sampling适合什么场景？
- 何时应该避免使用Greedy？
- Greedy与其他方法的对比如何？

#### 6.1.2 Temperature Sampling
**Temperature参数**
- Temperature的作用是什么？
- Temperature如何影响概率分布？
- Temperature=1, >1, <1各是什么效果？

**实现细节**
- Temperature Sampling如何实现？
- 计算开销相比Greedy增加多少？
- Temperature可以动态调整吗？

**Temperature的选择**
- 如何选择合适的Temperature？
- 不同任务的最佳Temperature是多少？
- Temperature对生成质量和多样性的影响如何？

#### 6.1.3 Top-k Sampling
**Top-k原理**
- Top-k Sampling的算法流程是什么？
- 如何选择top-k个候选？
- k值如何影响生成？

**实现优化**
- 如何高效找到top-k？
- 完整排序vs部分排序的区别是什么？
- Top-k的计算复杂度是多少？

**k值的选择**
- k值如何设置？
- k值对质量和多样性的影响是什么？
- 动态k值可行吗？

#### 6.1.4 Top-p (Nucleus) Sampling
**Top-p原理**
- Top-p Sampling的核心思想是什么？
- 如何确定累积概率阈值？
- Top-p与Top-k的区别是什么？

**实现细节**
- Top-p如何高效实现？
- 需要排序吗？
- 计算开销相比Top-k如何？

**p值的选择**
- p值通常设为多少？
- p值对生成的影响是什么？
- 如何结合Top-k和Top-p？

#### 6.1.5 采样策略对性能的影响
**计算开销对比**
- 不同采样策略的计算开销如何排序？
- 采样开销占总推理时间的比例是多少？
- 批处理时采样如何并行？

**采样的优化**
- 采样计算可以优化吗？
- GPU采样vs CPU采样的选择是什么？
- 采样可以与模型前向重叠吗？

**质量与性能权衡**
- 如何在生成质量和性能间权衡？
- 快速采样对质量的影响有多大？
- 什么场景应该优先考虑性能？

### 6.2 Beam Search优化

#### 6.2.1 Beam Search原理
**算法流程**
- Beam Search的基本算法是什么？
- Beam的维护和更新如何进行？
- 如何选择保留的候选序列？

**Beam width的影响**
- Beam width如何影响生成质量？
- Beam width如何影响计算开销？
- 如何选择合适的beam width？

**与Greedy的对比**
- Beam Search相比Greedy的优势是什么？
- Beam Search为什么能找到更好的序列？
- Beam Search的质量提升有多大？

#### 6.2.2 Beam Search的计算开销
**计算量增加**
- Beam width=k时，计算量增加多少倍？
- 为什么Beam Search比采样慢很多？
- 计算开销如何随beam width扩展？

**显存需求**
- Beam Search需要多少显存？
- KV Cache如何为多个beam存储？
- 显存需求如何随beam width变化？

**延迟分析**
- Beam Search的延迟增加多少？
- TTFT和TPOT如何变化？
- 何时Beam Search的开销不可接受？

#### 6.2.3 Beam Search并行化
**Beam间并行**
- 不同beam可以并行计算吗？
- 如何在batch维度并行多个beam？
- 并行化能减少多少延迟？

**Beam内并行**
- 单个beam的计算可以并行吗？
- 如何利用数据并行加速？
- 并行粒度如何选择？

**优化技术**
- Length normalization的作用是什么？
- Early stopping如何减少计算？
- Diverse Beam Search是什么？

### 6.3 推测解码（Speculative Decoding）

#### 6.3.1 推测解码基本原理
**核心思想**
- 推测解码要解决什么问题？
- 如何打破自回归的串行性？
- Draft-then-Verify的流程是什么？

**Draft阶段**
- Draft模型如何生成候选token？
- 一次可以生成多少个候选？
- Draft的并行度如何？

**Verify阶段**
- 如何验证Draft的候选token？
- Target模型的作用是什么？
- 验证可以并行吗？

**接受与拒绝**
- 如何决定接受哪些token？
- 接受率如何影响加速效果？
- 拒绝后如何处理？

#### 6.3.2 Draft模型选择
**小模型作为Draft**
- 为什么小模型适合做Draft？
- 小模型应该多小？
- 小模型与目标模型的关系是什么？

**Draft模型的训练**
- Draft模型需要特殊训练吗？
- 蒸馏（Distillation）的作用是什么？
- 如何提高Draft的质量？

**Draft模型的部署**
- Draft模型如何与Target模型协同？
- 两个模型如何在同一GPU上运行？
- 显存分配如何平衡？

#### 6.3.3 验证机制
**并行验证**
- 如何并行验证多个候选token？
- 验证的计算量是多少？
- 验证可以复用KV Cache吗？

**概率比较**
- 如何比较Draft和Target的概率？
- 接受概率的计算公式是什么？
- 为什么需要概率而不是argmax？

**KV Cache一致性**
- 接受部分token后KV Cache如何更新？
- 拒绝时KV Cache如何回滚？
- 如何保证生成质量不变？

#### 6.3.4 加速比分析
**理论加速比**
- 推测解码的理论加速上限是多少？
- 加速比如何随Draft长度变化？
- 加速比如何随接受率变化？

**实际加速效果**
- 实际应用中加速比通常是多少？
- 哪些因素限制加速比？
- 加速比在不同任务上的差异如何？

**开销分析**
- Draft模型的开销是多少？
- 验证的开销是多少？
- 总开销何时超过收益？

### 6.4 其他生成策略

#### 6.4.1 Medusa多头预测
**Medusa架构**
- Medusa的多头结构是什么？
- 如何同时预测多个未来token？
- Medusa头如何训练？

**Tree attention**
- 什么是tree attention？
- 如何验证多个候选路径？
- Tree attention的计算复杂度如何？

**Medusa的优势与局限**
- Medusa相比Speculative Decoding的优势是什么？
- Medusa的接受率如何？
- Medusa的适用场景是什么？

#### 6.4.2 Lookahead Decoding
**Lookahead原理**
- Lookahead Decoding如何工作？
- 如何利用并行性生成多个token？
- Lookahead的验证机制是什么？

**N-gram缓存**
- N-gram缓存的作用是什么？
- 如何利用历史生成加速？
- 缓存的命中率如何？

**性能分析**
- Lookahead的加速效果如何？
- 适合什么类型的生成任务？
- 与Speculative Decoding的对比如何？

#### 6.4.3 Early Exit策略
**Early Exit原理**
- 什么是Early Exit？
- 如何决定何时退出？
- Early Exit如何减少计算？

**置信度估计**
- 如何估计输出的置信度？
- 哪一层的输出可以使用？
- 置信度阈值如何设置？

**质量保证**
- Early Exit对生成质量的影响是什么？
- 如何保证不影响关键token？
- 质量与性能的权衡如何？

---

## 第7章：系统级并行优化

### 7.1 张量并行（Tensor Parallelism）

#### 7.1.1 张量并行原理
**基本概念**
- 什么是张量并行？
- 张量并行与数据并行的区别是什么？
- 为什么需要张量并行？

**张量切分**
- 如何切分张量？
- 行切分和列切分的区别是什么？
- 切分维度如何选择？

**通信需求**
- 张量并行需要什么通信？
- All-Reduce和All-Gather的作用是什么？
- 通信频率是多少？

#### 7.1.2 Megatron-LM切分策略
**注意力层切分**
- QKV矩阵如何切分？
- Multi-head如何分配到多个GPU？
- 输出投影如何切分？

**FFN层切分**
- FFN的两层矩阵如何切分？
- 为什么第一层列切分、第二层行切分？
- 切分如何减少通信？

**f和g算子**
- Megatron的f和g算子是什么？
- 前向和反向如何处理通信？
- 为什么推理时也需要这些算子？

#### 7.1.3 通信开销分析
**通信量计算**
- 每层的通信量是多少？
- 通信量与模型大小的关系是什么？
- 如何减少通信量？

**通信延迟**
- 通信延迟如何影响性能？
- 带宽和延迟哪个更重要？
- NVLink vs PCIe的差异是什么？

**通信与计算重叠**
- 如何重叠通信和计算？
- 流水线通信的方法是什么？
- 重叠能减少多少开销？

#### 7.1.4 注意力层并行
**QKV并行**
- QKV计算如何并行？
- 每个GPU计算哪部分？
- 并行后的通信需求是什么？

**Attention计算并行**
- Attention score计算如何并行？
- Softmax如何在多个GPU上计算？
- 需要同步吗？

**输出聚合**
- 如何聚合多个GPU的输出？
- All-Reduce的位置在哪里？
- 聚合的开销是多少？

#### 7.1.5 FFN层并行
**第一层并行**
- 第一层GEMM如何切分？
- 列切分的优势是什么？
- 激活函数如何并行？

**第二层并行**
- 第二层GEMM如何切分？
- 行切分的原因是什么？
- All-Reduce在哪里？

**专家并行（MoE）**
- MoE的FFN如何并行？
- 专家如何分配到GPU？
- 专家并行的通信模式是什么？

### 7.2 流水线并行（Pipeline Parallelism）

#### 7.2.1 流水线并行原理
**基本思想**
- 流水线并行的核心思想是什么？
- 如何将模型切分到多个设备？
- 流水线如何提高吞吐量？

**层间切分**
- 如何决定层的分配？
- 均匀切分vs不均匀切分的选择是什么？
- 切分粒度如何影响性能？

**流水线阶段**
- 每个设备负责哪些层？
- 前向和反向如何流水？
- 推理时流水线如何工作？

#### 7.2.2 Micro-batch策略
**Micro-batch的必要性**
- 为什么需要Micro-batch？
- Micro-batch如何提高流水线效率？
- Micro-batch数量如何选择？

**Batch的切分**
- 如何将batch切分为micro-batch？
- Micro-batch大小如何影响性能？
- 切分对显存的影响是什么？

**Micro-batch调度**
- Micro-batch如何在流水线中调度？
- 调度顺序是什么？
- 如何最大化设备利用率？

#### 7.2.3 气泡时间优化
**气泡时间的产生**
- 什么是气泡时间（Bubble time）？
- 为什么会产生气泡？
- 气泡如何影响效率？

**气泡时间计算**
- 气泡时间占总时间的比例是多少？
- 如何量化气泡的影响？
- 气泡与流水线深度的关系是什么？

**减少气泡**
- 如何减少气泡时间？
- Interleaved策略是什么？
- Virtual pipeline的作用是什么？

#### 7.2.4 1F1B调度策略
**1F1B原理**
- 1F1B（One Forward One Backward）是什么？
- 为什么1F1B优于朴素流水线？
- 1F1B如何减少显存占用？

**调度顺序**
- 1F1B的具体调度顺序是什么？
- 何时开始反向传播？
- 稳定阶段的模式是什么？

**推理中的应用**
- 1F1B在推理中适用吗？
- 推理的流水线调度有何不同？
- 如何为推理优化流水线？

### 7.3 序列并行（Sequence Parallelism）

#### 7.3.1 序列并行原理
**基本思想**
- 序列并行的核心思想是什么？
- 为什么在序列维度并行？
- 序列并行与张量并行的区别是什么？

**序列切分**
- 如何切分序列？
- 每个设备处理哪部分序列？
- 切分对Attention的影响是什么？

**适用场景**
- 序列并行适合什么场景？
- 长上下文为什么需要序列并行？
- 序列并行的局限是什么？

#### 7.3.2 长序列切分策略
**Attention的序列并行**
- Self-Attention如何序列并行？
- Ring Attention的原理是什么？
- 如何处理跨设备的Attention？

**通信模式**
- 序列并行的通信模式是什么？
- All-to-All通信的作用是什么？
- 通信开销如何？

**显存节省**
- 序列并行如何节省显存？
- 长序列的显存瓶颈如何缓解？
- 显存节省与计算效率的权衡是什么？

#### 7.3.3 与其他并行的配合
**与张量并行结合**
- 序列并行和张量并行可以同时使用吗？
- 如何协调两种并行？
- 通信如何优化？

**与流水线并行结合**
- 序列并行和流水线并行可以结合吗？
- 三维并行的配置是什么？
- 如何选择并行维度的优先级？

**并行策略选择**
- 不同模型大小如何选择并行策略？
- 不同序列长度如何选择？
- 如何根据硬件拓扑选择？

### 7.4 专家并行（Expert Parallelism）

#### 7.4.1 MoE模型结构
**MoE基本概念**
- 什么是Mixture of Experts？
- MoE如何工作？
- MoE的优势是什么？

**稀疏激活**
- 什么是稀疏激活？
- Top-K路由是什么？
- 稀疏性如何影响计算？

**专家层结构**
- 专家层如何替代FFN？
- 专家数量如何选择？
- 专家的参数量是多少？

#### 7.4.2 专家路由机制
**路由算法**
- 如何决定token路由到哪个专家？
- 路由网络的结构是什么？
- 路由的计算开销是多少？

**Top-K选择**
- 如何选择Top-K个专家？
- K值如何影响性能和质量？
- Top-K的计算如何优化？

**路由策略**
- Token-choice vs Expert-choice路由是什么？
- 两种策略的优缺点是什么？
- 如何选择路由策略？

#### 7.4.3 负载均衡优化
**负载不均衡问题**
- 为什么会出现负载不均衡？
- 负载不均衡如何影响性能？
- 如何衡量负载均衡程度？

**负载均衡策略**
- Auxiliary loss的作用是什么？
- 如何鼓励均匀的专家选择？
- Capacity factor的作用是什么？

**动态负载调整**
- 如何动态调整专家负载？
- Switch Transformer的策略是什么？
- 负载均衡与质量的权衡如何？

### 7.5 混合并行策略

#### 7.5.1 多维并行组合
**3D并行**
- 什么是3D并行？
- 数据、张量、流水线并行如何组合？
- 3D并行的配置空间有多大？

**4D并行**
- 加入序列并行或专家并行后的配置是什么？
- 4D并行的适用场景是什么？
- 如何搜索最优配置？

**并行维度的选择**
- 不同并行维度的优先级如何？
- 如何根据模型大小选择？
- 如何根据硬件配置选择？

#### 7.5.2 通信优化
**通信拓扑**
- 不同并行策略的通信拓扑是什么？
- 如何匹配通信拓扑到物理拓扑？
- NVLink、IB、以太网如何利用？

**通信调度**
- 多种并行的通信如何调度？
- 通信优先级如何确定？
- 如何避免通信拥塞？

**通信压缩**
- 通信数据可以压缩吗？
- 梯度压缩在推理中适用吗？
- 压缩的收益和开销是什么？

#### 7.5.3 并行度选择策略
**自动并行策略搜索**
- 如何自动搜索最优并行策略？
- Alpa的自动并行原理是什么？
- 搜索空间如何剪枝？

**性能建模**
- 如何建模不同并行策略的性能？
- Roofline模型如何应用？
- 如何预测通信开销？

**实际部署考虑**
- 实际部署时如何选择并行策略？
- 调试和容错如何影响选择？
- 如何在性能和简单性间权衡？

---

## 附录：核心概念总结

### A. 关键性能指标
- **TTFT (Time To First Token)**: 首token延迟
- **TPOT (Time Per Output Token)**: 每token生成时间
- **Throughput**: 吞吐量（tokens/秒或请求/秒）
- **GPU Utilization**: GPU利用率
- **Memory Bandwidth Utilization**: 内存带宽利用率

### B. 关键优化技术
- **Flash Attention**: I/O感知的高效注意力计算
- **KV Cache**: 缓存Key/Value避免重复计算
- **Paged Attention**: 分页管理KV Cache
- **Continuous Batching**: 动态批处理
- **Speculative Decoding**: 推测解码加速生成

### C. 并行策略对比
| 并行类型   | 切分维度 | 通信模式   | 适用场景          |
| ---------- | -------- | ---------- | ----------------- |
| 数据并行   | Batch    | All-Reduce | 训练，大batch推理 |
| 张量并行   | 模型参数 | All-Reduce | 大模型推理        |
| 流水线并行 | 模型层   | P2P        | 超大模型          |
| 序列并行   | 序列长度 | All-to-All | 长上下文          |
| 专家并行   | 专家     | All-to-All | MoE模型           |

---

**大纲说明：**
本大纲按照大语言模型推理流程的7个关键阶段系统化组织，涵盖从输入处理到输出生成、从单机优化到分布式并行的完整知识体系。每个知识点都配有3-5个具体问题，侧重于原理理解、算法分析和性能优化，适合用于全面考核应届生对LLM推理优化的理解深度。
