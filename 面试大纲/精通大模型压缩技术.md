# 精通大模型压缩技术 - 面试大纲

## 一、基础理论与动机（20分钟）

### 1.1 大模型压缩的必要性
- **问题背景**：
  - [请阐述大型语言模型在实际部署中面临的主要挑战](../notes/精通大模型压缩技术/请阐述大型语言模型在实际部署中面临的主要挑战.md)
  - [从内存占用、计算成本、推理延迟三个角度分析压缩的必要性](../notes/精通大模型压缩技术/从内存占用、计算成本、推理延迟三个角度分析压缩的必要性.md)
  - [举例说明：一个70B参数的模型在FP32精度下需要多少存储空间？](../notes/精通大模型压缩技术/举例说明：一个70B参数的模型在FP32精度下需要多少存储空间？.md)
  
- **压缩目标**：
  - [如何权衡模型性能与资源消耗？](../notes/精通大模型压缩技术/如何权衡模型性能与资源消耗？.md)
  - [什么是压缩率（Compression Ratio）？如何评估压缩效果？](../notes/精通大模型压缩技术/什么是压缩率（Compression_Ratio）？如何评估压缩效果？.md)
  - [边缘设备部署与云端部署在压缩需求上有何不同？](../notes/精通大模型压缩技术/边缘设备部署与云端部署在压缩需求上有何不同？.md)

### 1.2 模型压缩的评估指标
- **性能指标**：
  - [困惑度（Perplexity）、准确率、F1分数等任务指标的变化](../notes/精通大模型压缩技术/困惑度（Perplexity）、准确率、F1分数等任务指标的变化.md)
  - [如何设定可接受的性能下降阈值？](../notes/精通大模型压缩技术/如何设定可接受的性能下降阈值？.md)
  
- **效率指标**：
  - [模型大小（参数量、存储空间）](../notes/精通大模型压缩技术/模型大小（参数量、存储空间）.md)
  - [推理速度（吞吐量、延迟）](../notes/精通大模型压缩技术/推理速度（吞吐量、延迟）.md)
  - [FLOPs（浮点运算次数）减少比例](../notes/精通大模型压缩技术/FLOPs（浮点运算次数）减少比例.md)
  - [内存带宽需求](../notes/精通大模型压缩技术/内存带宽需求.md)

---

## 二、剪枝技术（Pruning）（30分钟）

### 2.1 剪枝基础理论
- **核心概念**：
  - 什么是神经网络剪枝？基本原理是什么？
  - 为什么可以移除部分参数而保持模型性能？
  - 解释"过参数化"（Over-parameterization）概念及其与剪枝的关系
  
- **剪枝分类**：
  - 非结构化剪枝 vs 结构化剪枝：优缺点对比
  - 细粒度剪枝、通道剪枝、层剪枝的区别
  - 静态剪枝 vs 动态剪枝

### 2.2 剪枝策略与方法
- **重要性评估**：
  - 如何判断一个权重或神经元的重要性？
  - 常见的重要性度量方法：权重大小、梯度、Hessian矩阵等
  - L1/L2正则化在剪枝中的作用
  
- **剪枝流程**：
  - 一次性剪枝（One-shot Pruning）vs 迭代剪枝（Iterative Pruning）
  - 剪枝后的微调（Fine-tuning）策略
  - 如何确定剪枝率（Pruning Ratio）？

### 2.3 大模型剪枝的挑战与解决方案
- **大模型特定问题**：
  - LLM的Transformer架构对剪枝提出了哪些特殊要求？
  - 注意力机制（Attention）和前馈网络（FFN）哪个更适合剪枝？
  - 如何在不进行完整微调的情况下实现剪枝？（计算成本考虑）
  
- **前沿方法**（了解即可）：
  - SparseGPT：一次性大规模剪枝方法
  - Wanda：无需权重更新的剪枝技术
  - 结构化剪枝在Transformer中的应用

---

## 三、量化技术（Quantization）（30分钟）

### 3.1 量化基础
- **核心概念**：
  - [什么是模型量化？从FP32到INT8的转换过程](../notes/精通大模型压缩技术/什么是模型量化？从FP32到INT8的转换过程.md)
  - [量化如何减少模型大小和加速推理？](../notes/精通大模型压缩技术/量化如何减少模型大小和加速推理？.md)
  - [量化带来的精度损失来源分析](../notes/精通大模型压缩技术/量化带来的精度损失来源分析.md)
  
- **量化类型**：
  - [权重量化 vs 激活值量化](../notes/精通大模型压缩技术/权重量化vs激活值量化.md)
  - [对称量化 vs 非对称量化](../notes/精通大模型压缩技术/对称量化vs非对称量化.md)
  - [均匀量化 vs 非均匀量化](../notes/精通大模型压缩技术/均匀量化vs非均匀量化.md)

### 3.2 量化方法
- **训练后量化（Post-Training Quantization, PTQ）**：
  - 基本流程：校准（Calibration）→ 量化 → 验证
  - 如何选择校准数据集？
  - PTQ的优势与局限性
  
- **量化感知训练（Quantization-Aware Training, QAT）**：
  - QAT的训练流程与伪量化（Fake Quantization）
  - 如何在训练中模拟量化误差？
  - QAT vs PTQ：何时选择哪种方法？
  
- **混合精度量化**：
  - 为什么需要混合精度？
  - 如何为不同层选择不同的量化位宽？
  - 敏感层识别方法

### 3.3 大模型量化的关键技术
- **极低比特量化**：
  - INT4、甚至更低比特量化的挑战
  - 如何处理量化过程中的异常值（Outliers）？
  - 分组量化（Group Quantization）原理
  
- **主流大模型量化方案**（需了解）：
  - **GPTQ**：基于最优化的训练后量化
  - **AWQ**（Activation-aware Weight Quantization）：激活感知的权重量化
  - **QLoRA**：量化 + LoRA低秩适配的结合
  - **LLM.int8()**：8比特矩阵乘法分解
  
- **量化实现细节**：
  - 如何在GPU上高效执行低比特运算？
  - INT8 GEMM（矩阵乘法）加速原理
  - 量化模型的存储格式

---

## 四、知识蒸馏（Knowledge Distillation）（25分钟）

### 4.1 知识蒸馏原理
- **基本概念**：
  - 教师模型（Teacher）与学生模型（Student）的关系
  - 软标签（Soft Labels）与硬标签（Hard Labels）的区别
  - 为什么软标签包含更多信息？
  
- **蒸馏目标函数**：
  - KL散度在蒸馏中的作用
  - 温度参数（Temperature）的意义与调节
  - 蒸馏损失与任务损失的权衡

### 4.2 蒸馏策略
- **基于输出的蒸馏**：
  - 逻辑输出（Logits）蒸馏
  - 预测分布蒸馏
  - 适用场景与局限性
  
- **基于特征的蒸馏**：
  - 中间层特征匹配
  - 注意力图（Attention Map）蒸馏
  - 如何选择蒸馏的中间层？
  
- **基于关系的蒸馏**：
  - 样本间关系的迁移
  - 相似度矩阵蒸馏

### 4.3 大模型蒸馏的特殊性
- **LLM蒸馏挑战**：
  - 为什么直接蒸馏大模型效果不佳？
  - 如何处理教师模型和学生模型架构差异？
  - 序列到序列蒸馏的复杂性
  
- **数据选择**：
  - 蒸馏数据集的构建策略
  - 数据增强在蒸馏中的应用
  - 少样本蒸馏的可行性
  
- **成功案例分析**（需了解）：
  - DistilBERT：BERT的蒸馏版本
  - TinyBERT：更激进的压缩方案
  - 从GPT-4到GPT-3.5的能力迁移思路

---

## 五、其他压缩技术（15分钟）

### 5.1 低秩分解（Low-Rank Decomposition）
- **矩阵分解原理**：
  - SVD（奇异值分解）在模型压缩中的应用
  - 如何通过低秩近似减少参数量？
  - 权重矩阵的秩（Rank）与模型容量的关系
  
- **LoRA技术**：
  - LoRA的基本原理：\( W = W_0 + BA \)
  - 为什么低秩更新能保持性能？
  - LoRA在大模型微调中的应用

### 5.2 模块化分解（Modular Decomposition）
- **模块化压缩思想**：
  - 将多层Transformer合并为模块的动机
  - 模块化层的训练与组装
  - MoDeGPT等方法的核心思路（了解即可）

### 5.3 参数共享与神经架构搜索
- **参数共享**：
  - 跨层参数共享的可行性
  - ALBERT模型的参数共享策略
  
- **神经架构搜索（NAS）**：
  - 如何自动搜索压缩后的最优架构？
  - NAS在模型压缩中的应用前景与挑战

---

## 六、综合压缩方案（15分钟）

### 6.1 多技术组合
- **组合策略**：
  - 剪枝 + 量化：如何协同工作？
  - 蒸馏 + 量化（QLoRA）：训练与压缩一体化
  - 深度压缩（Deep Compression）流程：剪枝 → 量化 → 霍夫曼编码
  
- **压缩流水线设计**：
  - 如何为特定应用场景设计压缩方案？
  - 压缩技术应用顺序的影响
  - 压缩-性能帕累托前沿（Pareto Frontier）

### 6.2 硬件感知压缩
- **硬件特性考虑**：
  - 不同硬件（CPU、GPU、NPU）对压缩技术的友好度
  - 内存层次结构对压缩效果的影响
  - 稀疏矩阵加速的硬件支持
  
- **部署优化**：
  - 模型部署框架（ONNX、TensorRT、OpenVINO）对压缩的支持
  - 算子融合（Operator Fusion）与压缩的结合

---

## 七、实践应用能力（20分钟）

### 7.1 工具与框架
- **主流压缩工具**：
  - PyTorch的量化API（torch.quantization）
  - Hugging Face Optimum库
  - NVIDIA TensorRT-LLM
  - llama.cpp的量化方案
  
- **使用经验**：
  - 你是否有实际压缩大模型的经验？
  - 描述一次完整的模型压缩流程
  - 遇到过哪些实际问题？如何解决？

### 7.2 案例分析
- **给定场景设计压缩方案**：
  - 场景1：在边缘设备（如手机）上部署7B参数的LLM
  - 场景2：在云端优化推理成本，提升吞吐量
  - 场景3：保持高精度的前提下压缩模型
  
- **权衡分析**：
  - 如何在延迟、吞吐量、成本、精度之间做取舍？
  - 不同业务场景（聊天、翻译、代码生成）的压缩策略差异

### 7.3 实验与调优
- **实验设计**：
  - 如何设计对比实验验证压缩效果？
  - 哪些指标是必须监控的？
  
- **调优经验**：
  - 压缩后模型性能下降时的排查思路
  - 超参数调优：剪枝率、量化位宽、蒸馏温度等
  - A/B测试在模型压缩中的应用

---

## 八、前沿发展与研究（10分钟）

### 8.1 最新研究方向
- **极致压缩**：
  - 1-bit LLM的可行性探索
  - 二值化网络（Binary Neural Networks）
  
- **压缩友好的模型架构**：
  - 原生支持稀疏性的模型设计
  - Mixture of Experts (MoE) 与压缩的结合
  
- **训练即压缩**：
  - 在预训练阶段就考虑压缩的方法
  - 稀疏训练（Sparse Training）

### 8.2 开放性讨论
- **技术趋势判断**：
  - 你认为大模型压缩的未来方向是什么？
  - 量化与剪枝哪个更有前景？为什么？
  
- **伦理与应用**：
  - 模型压缩对AI民主化的意义
  - 压缩技术在降低碳排放中的作用

---

## 九、编码与算法题（选做，15分钟）

### 9.1 编码实现
- **任务1**：实现一个简单的权重剪枝函数
  - 输入：权重矩阵、剪枝率
  - 输出：剪枝后的稀疏矩阵
  
- **任务2**：实现INT8量化的scale和zero_point计算
  - 给定FP32权重范围，计算量化参数
  
- **任务3**：实现知识蒸馏的损失函数
  - 结合KL散度和交叉熵

### 9.2 算法理解
- **问题1**：为什么结构化剪枝比非结构化剪枝更适合实际部署？从计算图和硬件执行角度分析
  
- **问题2**：推导KL散度在知识蒸馏中的梯度形式
  
- **问题3**：分析量化误差在多层网络中的累积效应

---

## 十、综合评估维度

### 理论深度（30%）
- 对压缩技术原理的理解深度
- 能否从数学角度解释核心算法
- 对前沿研究的了解程度

### 实践能力（40%）
- 是否有实际项目经验
- 工具框架的熟练程度
- 问题解决与调优能力

### 系统思维（20%）
- 能否设计端到端的压缩方案
- 对不同技术的权衡分析能力
- 硬件-软件协同优化意识

### 创新潜力（10%）
- 对技术趋势的判断
- 提出新想法的能力
- 跨领域知识的迁移

---

## 附录：推荐学习资源

### 经典论文
1. **Deep Compression** (Han et al., 2016) - 综合压缩方法的奠基之作
2. **Distilling the Knowledge in a Neural Network** (Hinton et al., 2015) - 知识蒸馏开山之作
3. **GPTQ: Accurate Post-Training Quantization for GPT** (Frantar et al., 2022)
4. **LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale** (Dettmers et al., 2022)

### 开源工具
- Hugging Face Transformers & Optimum
- PyTorch Quantization
- NVIDIA TensorRT
- llama.cpp

### 博客与教程
- Hugging Face 量化指南
- PyTorch 官方量化文档
- 各大模型厂商的技术博客（OpenAI、Anthropic、Meta等）

---

**注**：本大纲面向对大模型压缩技术有深入理解需求的应届生或初级工程师，涵盖从理论到实践的全方位考核。面试时可根据候选人背景灵活调整各部分比重，重点考察其系统思维和实践能力。

